from sequre import sequre
from sequre.settings import DEBUG


class LinReg[T]:
    coef_: T

    def __init__(self, initial_weights: T):
        self.coef_ = initial_weights
    
    def fit(self, mpc, X: T, Y: T, step: float, epochs: int) -> LinReg[T]:
        self.coef_ = LinReg._fit(mpc, X, Y, self.coef_, step, epochs)
        return self
    
    def predict(self, mpc, X: T, error: float = 0.0) -> T:
        prediction = LinReg._predict(mpc, X, self.coef_)
        
        if error > 0.0:
            return prediction + error
        
        return prediction
    
    def loss(self, mpc, X: T, Y: T) -> T:
        return LinReg._loss(mpc, X, Y, self.coef_)

    def _fit(mpc, X: T, Y: T, initial_w: T, step: float, epochs: int, debug: Static[int] = DEBUG) -> T:
        # Adding bias
        X_tilde = X.hstack(T.ones((len(X), 1), mpc, X.modulus))

        # Gradient descent
        return LinReg._bgd(mpc, X_tilde, Y, initial_w, step, epochs, debug)
    
    @sequre
    def _bgd(mpc, X_tilde: T, Y: T, initial_w: T, step: float, epochs: int, debug: Static[int] = DEBUG) -> T:
        # Pre-computing invariants
        cov = X_tilde.T @ X_tilde  # n x n
        ref = X_tilde.T @ Y  # n x 1

        # Batched gradient descent
        w = initial_w  # n x 1
        for _ in range(epochs):
            w += (ref - cov @ w) * step
            if debug:
                print(f"CP{mpc.pid}:\t{_ + 1}/{epochs} loss:", LinReg._loss(mpc, X_tilde, Y, w).reveal(mpc))
        
        return w
    
    @sequre
    def _predict(mpc, X: T, w: T) -> T:
        return X @ w
    
    @sequre
    def _loss(mpc, X: T, Y: T, w: T) -> T:
        l = Y - X @ w
        return l.T @ l
