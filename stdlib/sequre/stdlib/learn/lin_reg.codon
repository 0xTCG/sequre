""" Linear regression module """

from numpy.ndarray import ndarray

from sequre.attributes import sequre
from sequre.settings import DEBUG
from utils import batch


class LinReg[T]:
    coef_: T
    optimizer: str

    def __init__(self, initial_weights: T, optimizer: str = "bgd"):
        self.coef_ = initial_weights.copy()
        self.optimizer = optimizer
    
    def fit(self, mpc, X: T, y: T, step: float, epochs: int, verbose: bool = True) -> LinReg[T]:
        self.coef_ = LinReg._fit(mpc, X, y, self.coef_, step, epochs, self.optimizer, verbose)
        return self
    
    def predict(self, mpc, X: T, error: float = 0.0) -> T:
        return LinReg._predict(mpc, X, self.coef_, error)
    
    def loss(self, mpc, X: T, y: T) -> T:
        return LinReg._loss(mpc, X, y, self.coef_)

    @staticmethod
    def estimate_step(train, test) -> float:
        cov_max = ndarray.max(train.T @ train)
        ref_max = ndarray.max(train.T @ test)
        return max(1 / (1 << 20), 1 / max(cov_max, ref_max))
    
    def _fit(mpc, X: T, y: T, initial_w: T, step: float, epochs: int, optimizer: str, verbose: bool, debug: Static[int] = DEBUG) -> T:
        # Adding bias
        X_tilde = X.pad_with_value(1, 1, 1, mpc)

        # Gradient descent
        if optimizer == "bgd":
            return LinReg._bgd(mpc, X_tilde, y, initial_w, step, epochs, verbose, debug)
        if optimizer == "mbgd":
            return LinReg._mbgd(mpc, X_tilde, y, initial_w, step, epochs, 10, verbose, debug)
        else:
            raise ValueError(f"LinReg: invalid optimizer passed: {optimizer}")
    
    @sequre
    def _bgd(mpc, X_tilde: T, y: T, initial_w: T, step: float, epochs: int, verbose: bool, debug: Static[int] = DEBUG) -> T:
        if debug:
            print(f"CP{mpc.pid}:\tLin. reg. BGD step size:", step)
        
        # Pre-compute invariants
        cov = X_tilde.T @ X_tilde  # n x n
        ref = X_tilde.T @ y  # n x 1

        if debug:
            print(f"CP{mpc.pid}:\tLin. reg. BGD cov avg:", ndarray.mean(cov.reveal(mpc)))
            print(f"CP{mpc.pid}:\tLin. reg. BGD ref avg:", ndarray.mean(ref.reveal(mpc)))
        
        # Batched gradient descent
        w = initial_w  # n x 1
        for _ in range(epochs):
            if verbose:
                print(f"CP{mpc.pid}:\tLin. reg. BGD epoch {_ + 1}/{epochs}")
            if debug:
                print(f"CP{mpc.pid}:\t\t weigts avg {ndarray.mean(w.reveal(mpc))} | loss: {LinReg._loss(mpc, X_tilde, y, w).reveal(mpc)[0, 0]}")
            
            w += (ref - cov @ w) * step
        
        return w
    
    @sequre
    def _mbgd(mpc, X_tilde: T, y: T, initial_w: T, step: float, epochs: int, batches: int, verbose: bool, debug: Static[int] = DEBUG) -> T:
        if debug:
            print(f"CP{mpc.pid}:\tLin. reg. BGD step size:", step)
        
        # Compute mini-batches
        X_mini_batches = batch(mpc, X_tilde, batch_count=batches)
        y_mini_batches = batch(mpc, y, batch_count=batches)
        
        # Pre-compute invariants
        covs = []
        refs = []
        for i in range(batches):
            X_mini_batch = X_mini_batches[i]
            y_mini_batch = y_mini_batches[i]
            
            cov = X_mini_batch.T @ X_mini_batch
            ref = X_mini_batch.T @ y_mini_batch
            if debug:
                print(f"CP{mpc.pid}:\tLin. reg. MBGD cov avg {i + 1}/{batches}:", ndarray.mean(cov.reveal(mpc)))
                print(f"CP{mpc.pid}:\tLin. reg. MBGD ref avg {i + 1}/{batches}:", ndarray.mean(ref.reveal(mpc)))
            
            covs.append(cov)
            refs.append(ref)
        
        # Mini-batched gradient descent
        w = initial_w
        for _ in range(epochs):
            for i in range(batches):
                if verbose:
                    print(f"CP{mpc.pid}:\tLin. reg. MBGD epoch {_ + 1}/{epochs} -- batch {i + 1}/{batches}")
                if debug:
                    print(f"CP{mpc.pid}:\t\t weigts avg {ndarray.mean(w.reveal(mpc))} | loss: {LinReg._loss(mpc, X_mini_batches[i], y_mini_batches[i], w).reveal(mpc)[0, 0]}")
                
                w += (refs[i] - covs[i] @ w) * step
        
        return w
    
    @sequre
    def _predict(mpc, X: T, w: T, error: float) -> T:
        prediction = X.pad_with_value(1, 1, 1, mpc) @ w
        
        if error != 0.0:
            return prediction + error
        
        return prediction
    
    @sequre
    def _loss(mpc, X: T, y: T, w: T) -> T:
        l = y - X @ w
        return l.T @ l
