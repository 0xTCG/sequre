""" Linear regression module """

from numpy.ndarray import ndarray

from sequre import sequre
from sequre.settings import DEBUG


class LinReg[T]:
    coef_: T

    def __init__(self, initial_weights: T):
        self.coef_ = initial_weights.copy()
    
    def fit(self, mpc, X: T, Y: T, step: float, epochs: int) -> LinReg[T]:
        self.coef_ = LinReg._fit(mpc, X, Y, self.coef_, step, epochs)
        return self
    
    def predict(self, mpc, X: T, error: float = 0.0) -> T:
        return LinReg._predict(mpc, X, self.coef_, error)
    
    def loss(self, mpc, X: T, Y: T) -> T:
        return LinReg._loss(mpc, X, Y, self.coef_)

    @staticmethod
    def estimate_step(train, test) -> float:
        cov_max = ndarray.max(train.T @ train)
        ref_max = ndarray.max(train.T @ test)
        return max(1 / (1 << 20), 1 / max(cov_max, ref_max))
    
    def _fit(mpc, X: T, Y: T, initial_w: T, step: float, epochs: int, debug: Static[int] = DEBUG) -> T:
        # Adding bias
        X_tilde = X.pad_with_value(1, 1, 1, mpc)

        # Gradient descent
        return LinReg._bgd(mpc, X_tilde, Y, initial_w, step, epochs, debug)
    
    @sequre
    def _bgd(mpc, X_tilde: T, Y: T, initial_w: T, step: float, epochs: int, debug: Static[int] = DEBUG) -> T:
        if debug:
            print(f"CP{mpc.pid}:\tLin. reg. BGD step size:", step)
        
        # Pre-computing invariants
        cov = X_tilde.T @ X_tilde  # n x n
        ref = X_tilde.T @ Y  # n x 1

        if debug:
            print(f"CP{mpc.pid}:\tLin. reg. BGD cov avg:", ndarray.mean(cov.reveal(mpc)))
            print(f"CP{mpc.pid}:\tLin. reg. BGD ref avg:", ndarray.mean(ref.reveal(mpc)))
        
        # Batched gradient descent
        w = initial_w  # n x 1
        for _ in range(epochs):
            if debug:
                print(f"CP{mpc.pid}:\tLin. reg. BGD epoch {_ + 1}/{epochs} | weigts avg {ndarray.mean(w.reveal(mpc))} | loss: {LinReg._loss(mpc, X_tilde, Y, w).reveal(mpc)[0, 0]}")
            w += (ref - cov @ w) * step
        
        return w
    
    @sequre
    def _predict(mpc, X: T, w: T, error: float) -> T:
        prediction = X.pad_with_value(1, 1, 1, mpc) @ w
        
        if error != 0.0:
            return prediction + error
        
        return prediction
    
    @sequre
    def _loss(mpc, X: T, Y: T, w: T) -> T:
        l = Y - X @ w
        return l.T @ l
