from sequre import sequre

from constants import SUPPORTED_ACTIVATIONS
from activations import activate, dactivate


class Input[ctype]:
    size: int
    output: ctype

    def __init__(self, size: int):
        self.size = size
    
    @property
    def input(self) -> ctype:
        return self.output
    
    @property
    def activation(self) -> str:
        return "None"

    def initialize(self, *args):
        return

    def evaluate(self, mpc, last_output: ctype):
        self.output = last_output


class Dense[ctype]:
    activation: str
    size: int
    kernel_initializer: str
    bias_initializer: str
    
    weights: ctype
    bias: ctype
    input: ctype
    output: ctype

    # Derivatives
    dw: ctype
    db: ctype

    # Update params
    vw: ctype
    vb: ctype

    def __init__(self, activation: str, size: int, kernel_initializer: str = "uniform", bias_initializer: str = "uniform"):
        assert activation in SUPPORTED_ACTIVATIONS, f"Dense neural net layer: {activation} activation is not supported. Supported activations: {SUPPORTED_ACTIVATIONS}"
        self.activation = activation
        self.size = size
        self.kernel_initializer = kernel_initializer
        self.bias_initializer = bias_initializer
    
    def initialize(self, mpc, prev_size: int, *args, **kwargs):
        w_shape = (prev_size, self.size)
        b_shape = (1, self.size)
        
        self.weights = ctype.rand(w_shape, self.kernel_initializer, mpc, *args, **kwargs)
        self.bias = ctype.rand(b_shape, self.bias_initializer, mpc, *args, **kwargs)
        
        self.dw = self.weights.zeros()
        self.db = self.bias.zeros()
        
        self.vw = self.weights.zeros()
        self.vb = self.bias.zeros()
    
    def is_evaluated(self):
        return not self.output.is_empty()
    
    def evaluate(self, mpc, last_output: ctype):
        Dense[ctype]._evaluate(mpc, self, last_output)
    
    def activate(self, mpc) -> ctype:
        return activate(mpc, self.input, self.activation)
    
    def derive(self, mpc, prev_output: ctype, dhidden: ctype) -> ctype:
        dact = dactivate(mpc, self.input, self.activation)
        return Dense[ctype]._derive(mpc, self, prev_output, dhidden, dact)
    
    def update(self, mpc, step: float, momentum: float):
        Dense[ctype]._nesterov_update(mpc, self, step, momentum)

    @sequre
    def _nesterov_update(mpc, layer: Dense, step: float, momentum: float):
        # Update the weights
        vw_prev = layer.vw.copy()
        layer.vw = layer.vw * momentum - layer.dw * step
        layer.weights += layer.vw * (momentum + 1) - vw_prev * momentum
        
        # Update the biases
        vb_prev = layer.vb.copy()
        layer.vb = layer.vb * momentum - layer.db * step
        layer.bias += layer.vb * (momentum + 1) - vb_prev * momentum
    
    @sequre
    def _derive(mpc, layer: Dense, prev_output: ctype, dhidden: ctype, dact: ctype) -> ctype:
        # Apply derivative of activation
        dhidden = dhidden * dact
        # Compute derivative of weights
        layer.dw = prev_output.T @ dhidden
        # Compute derivative of biases
        layer.db = dhidden.sum(axis=0).expand_dims(axis=0)
        # Compute backpropagated activations
        return dhidden @ layer.weights.T
    
    @sequre
    def _evaluate(mpc, layer: Dense, last_output: ctype):
        layer.input = last_output @ layer.weights + layer.bias
        layer.output = layer.activate(mpc)
