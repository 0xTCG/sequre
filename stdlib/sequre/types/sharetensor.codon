from numpy.ndarray import ndarray
from numpy.create import array, _extract_shape

from ..utils.param import *
from ..utils.utils import zeros_mat, zeros_vec, random_floats
from ..utils.constants import SMALL_CYCLES_INSTR_COST_ESTIMATE
from ..types.utils import num_to_bits, contains_type

from utils import double_to_fp


class Sharetensor[TP]:
    share: TP
    x_r: TP
    r: TP
    modulus: int_t  # TODO: #145 Add support for generic modulus type after migrating to new Codon
    sqrt: TP
    sqrt_inv: TP
    fp: bool
    public: bool
    diagonal: bool

    def __init__(self: Sharetensor[TP], other: int, modulus: int_t):
        self.share = TP(other)
        self.x_r = TP(0)
        self.r = TP(0)
        self.modulus = modulus
        self.sqrt = TP(0)
        self.sqrt_inv = TP(0)
        self.fp = False
        self.public = False
        self.diagonal = False
    
    def __init__(self: Sharetensor[TP], share: TP, x_r: TP, r: TP, modulus: int_t):
        self.share = share
        self.x_r = x_r
        self.r = r
        self.modulus = modulus
        self.sqrt = TP(0)
        self.sqrt_inv = TP(0)
        self.fp = False
        self.public = False
        self.diagonal = False
    
    def __init__(self: Sharetensor[TP],
                 share: TP, x_r: TP, r: TP,
                 modulus: int_t, sqrt: TP,
                 sqrt_inv: TP, fp: bool,
                 public: bool, diagonal: bool):
        self.share = share
        self.x_r = x_r
        self.r = r
        self.modulus = modulus
        self.sqrt = sqrt
        self.sqrt_inv = sqrt_inv
        self.fp = fp
        self.public = public
        self.diagonal = diagonal

    def __init__(self: Sharetensor[TP], other: TP, modulus: int_t):
        self.share = other
        self.x_r = TP(0)
        self.r = TP(0)
        self.modulus = modulus
        self.sqrt = TP(0)
        self.sqrt_inv = TP(0)
        self.fp = False
        self.public = False
        self.diagonal = False

    def __init__(self: Sharetensor[TP], other: Sharetensor[TP]):
        self.share = other.share.copy()
        self.x_r = other.x_r.copy()
        self.r = other.r.copy()
        self.modulus = other.modulus
        self.sqrt = other.sqrt.copy()
        self.sqrt_inv = other.sqrt_inv.copy()
        self.fp = other.fp
        self.public = other.public
        self.diagonal = other.diagonal
    
    def __iter__(self: Sharetensor[TP]):
        for i in range(len(self)): yield self[i]
    
    def __getitem__(self: Sharetensor[TP], index):
        item_share = self.share[index]
        return Sharetensor(
            share=item_share,
            x_r=self.x_r[index] if self.x_r else type(item_share)(0),
            r=self.r[index] if self.r else type(item_share)(0),
            modulus=self.modulus,
            sqrt=self.sqrt[index] if self.sqrt else type(item_share)(0),
            sqrt_inv=self.sqrt_inv[index] if self.sqrt_inv else type(item_share)(0),
            fp=self.fp,
            public=self.public,
            diagonal=False)
    
    def __setitem__(self: Sharetensor[TP], index, other):
        assert self.public == other.public, f"Sharetensor: cannot set public:{other.public} value into public:{self.public}"
        assert self.fp == other.fp, f"Sharetensor: cannot set fp:{other.fp} value into fp:{self.fp}"
        assert self.modulus == other.modulus, f"Sharetensor: cannot set: moduli missmatch"
        
        if isinstance(index, tuple[int, int]):
            ri, ci = index
            self.diagonal = self.diagonal and (ri == ci)
        else:
            self.diagonal = False
        
        self.share[index] = other.share
        if not other.x_r: self.x_r = TP(0)
        elif self.x_r: self.x_r[index] = other.x_r
        
        if not other.r: self.r = TP(0)
        elif self.r:self.r[index] = other.r

        if not other.sqrt: self.sqrt = TP(0)
        elif self.sqrt: self.sqrt[index] = other.sqrt
        
        if not other.sqrt_inv: self.sqrt_inv = TP(0)
        elif self.sqrt_inv: self.sqrt_inv[index] = other.sqrt_inv
        
    def __bool__(self: Sharetensor[TP]) -> bool:
        return bool(self.share)
    
    def __int__(self: Sharetensor[TP]) -> int:
        return int(self.share)
    
    def __neg__(self: Sharetensor[TP]) -> Sharetensor[TP]:
        return Sharetensor(
            share = self.share.neg_mod(self.modulus),
            x_r = self.x_r.neg_mod(self.modulus) if self.x_r else self.x_r,
            r = self.r.neg_mod(self.modulus) if self.r else self.r,
            modulus = self.modulus,
            sqrt = TP(0),
            sqrt_inv = TP(0),
            fp = self.fp,
            public = self.public,
            diagonal = self.diagonal)

    def __iadd__(self: Sharetensor[TP], other: Sharetensor[TP]) -> Sharetensor[TP]:
        if isinstance(TP, list[list[int_t]]):
            if other.diagonal:
                for i in range(min(other.shape)):
                    self.share[i][i] = self.share[i][i].add_mod(other.share[i][i], self.modulus)
            else: self.share = self.share.add_mod(other.share, self.modulus)
        else: self.share = self.share.add_mod(other.share, self.modulus)

        if self.x_r and other.x_r:
            if isinstance(TP, list[list[int_t]]):
                if other.diagonal:
                    for i in range(min(other.shape)):
                        self.x_r[i][i] = self.x_r[i][i].add_mod(other.x_r[i][i], self.modulus)
                else: self.x_r = self.x_r.add_mod(other.x_r, self.modulus)
            else: self.x_r = self.x_r.add_mod(other.x_r, self.modulus)
        else: self.x_r = TP(0)

        if self.r and other.r:
            if isinstance(TP, list[list[int_t]]):
                if other.diagonal:
                    for i in range(min(other.shape)):
                        self.r[i][i] = self.r[i][i].add_mod(other.r[i][i], self.modulus)
                else: self.r = self.r.add_mod(other.r, self.modulus)
            else: self.r = self.r.add_mod(other.r, self.modulus)
        else: self.r = TP(0)

        self.sqrt = TP(0)
        self.sqrt_inv = TP(0)
        
        return self
    
    def __iadd__(self: Sharetensor[TP], other: TP) -> Sharetensor[TP]:
        self.share = self.share.add_mod(other, self.modulus)
        if self.is_partitioned():
            self.x_r = self.x_r.add_mod(other, self.modulus)
            self.r = self.r.add_mod(other, self.modulus)
        self.sqrt = TP(0)
        self.sqrt_inv = TP(0)
        
        return self
    
    def __iadd__(self: Sharetensor[TP], other: int) -> Sharetensor[TP]:
        raise NotImplementedError("Sharetensors cannot be publicly added to without IR pass enabled.")
    
    def __iadd__(self: Sharetensor[TP], other: float) -> Sharetensor[TP]:
        raise NotImplementedError("Sharetensors cannot be publicly added to without IR pass enabled.")
    
    def __add__(self: Sharetensor[TP], other) -> Sharetensor[TP]:
        if isinstance(other, ByVal):
            raise NotImplementedError("Sharetensors cannot be publicly added to without IR pass enabled.")
        
        if isinstance(other, Sharetensor[TP]):
            new_number = Sharetensor(self if not self.diagonal else other)
            new_number += (other if not self.diagonal else self)
            return new_number
        else:
            new_number = Sharetensor(self)
            new_number += other
            return new_number
    
    def __isub__(self: Sharetensor[TP], other: Sharetensor[TP]) -> Sharetensor[TP]:
        if isinstance(self, list[list[int_t]]):
            if other.diagonal:
                for i in range(min(other.shape)):
                    self.share[i][i] = self.share[i][i].sub_mod(other.share[i][i], self.modulus)
            else: self.share = self.share.sub_mod(other.share, self.modulus)
        else: self.share = self.share.sub_mod(other.share, self.modulus)
        
        if self.x_r and other.x_r:
            if isinstance(self, list[list[int_t]]):
                if other.diagonal:
                    for i in range(min(other.shape)):
                        self.x_r[i][i] = self.x_r[i][i].sub_mod(other.x_r[i][i], self.modulus)
                else: self.x_r = self.x_r.sub_mod(other.x_r, self.modulus)
            else: self.x_r = self.x_r.sub_mod(other.x_r, self.modulus)
        else: self.x_r = TP(0)
        
        if self.r and other.r:
            if isinstance(self, list[list[int_t]]):
                if other.diagonal:
                    for i in range(len(self)):
                        self.r[i][i] = self.r[i][i].sub_mod(other.r[i][i], self.modulus)
                else: self.r = self.r.sub_mod(other.r, self.modulus)
            else: self.r = self.r.sub_mod(other.r, self.modulus)
        else: self.r = TP(0)
        
        self.sqrt = TP(0)
        self.sqrt_inv = TP(0)
        
        return self
    
    def __isub__(self: Sharetensor[TP], other: TP) -> Sharetensor[TP]:
        self.share = self.share.sub_mod(other, self.modulus)
        if self.is_partitioned():
            self.x_r = self.x_r.sub_mod(other, self.modulus)
            self.r = self.r.sub_mod(other, self.modulus)
        self.sqrt = TP(0)
        self.sqrt_inv = TP(0)
        
        return self
    
    def __isub__(self: Sharetensor[TP], other: int) -> Sharetensor[TP]:
        raise NotImplementedError("Sharetensors cannot be publicly subtracted to without IR pass enabled.")
    
    def __sub__(self: Sharetensor[TP], other) -> Sharetensor[TP]:
        if isinstance(other, Sharetensor[TP]):
            new_number = Sharetensor(self if not self.diagonal else other)
            new_number = new_number - (other if not self.diagonal else self)
            return new_number
        else:
            new_number = Sharetensor(self)
            new_number = new_number - other
            return new_number
    
    def __imul__(self: Sharetensor[TP], other) -> Sharetensor[TP]:        
        if isinstance(other, Sharetensor):
            raise NotImplementedError("Sharetensors cannot be multiplied without IR pass enabled.")
        elif isinstance(other, float):
            raise NotImplementedError("Sharetensors cannot be multiplied by float without IR pass enabled.")
        else:
            self.share = self.share.mul_mod(other, self.modulus)
            if self.is_partitioned():
                self.x_r = self.x_r.mul_mod(other, self.modulus)
                self.r = self.r.mul_mod(other, self.modulus)
            
            # TODO: Fix sqrt
            # other_sqrt = math.sqrt(other)
            # self.sqrt *= other_sqrt
            # self.sqrt_inv /= other_sqrt
            
            return self
    
    def __mul__(self: Sharetensor[TP], other) -> Sharetensor[TP]:
        new_number = Sharetensor(self)
        new_number *= other
        return new_number
    
    def __matmul__(self: Sharetensor[TP], other) -> Sharetensor[TP]:
        raise NotImplementedError("Sharetensors matrices cannot be multiplied without IR pass enabled.")

    def __truediv__(self: Sharetensor[TP], other) -> Sharetensor[TP]:
        raise NotImplementedError("Sharetensors cannot be divided without IR pass enabled.")
    
    def __ipow__(self: Sharetensor[TP], other) -> Sharetensor[TP]:
        raise NotImplementedError("You tried to compute a power of a Sharetensor by a non-int.")
    
    def __pow__(self: Sharetensor[TP], other) -> Sharetensor[TP]:
        new_number = Sharetensor(self)
        new_number **= other
        return new_number
    
    def __gt__(self: Sharetensor[TP], other) -> Sharetensor[TP]:
        raise NotImplementedError(
            "Sharetensors cannot be compared without IR pass enabled")
    
    def __lt__(self: Sharetensor[TP], other) -> Sharetensor[TP]:
        raise NotImplementedError(
            "Sharetensors cannot be compared without IR pass enabled")
    
    def __eq__(self: Sharetensor[TP], other) -> Sharetensor[TP]:
        raise NotImplementedError(
            "Sharetensors cannot be compared without IR pass enabled")
    
    def __len__(self: Sharetensor[TP]) -> int:
        return len(self.share)
    
    @staticmethod
    def enc(mpc, data, source_pid: int, modulus: int_t) -> Sharetensor:
        st = Sharetensor(
            other=mpc.comms.share_from(data.to_int_t(modulus), source_pid, modulus),
            modulus=modulus)
        
        if contains_type(data, float):
            st.fp = True
        
        return st
    
    def diag(self: Sharetensor[TP], other: Sharetensor[int_t]) -> Sharetensor[TP]:
        diagonal_sv = self.zeros()

        for i in range(len(diagonal_sv)):
            diagonal_sv[i, i] = other
        
        diagonal_sv.modulus = other.modulus
        diagonal_sv.fp = other.fp
        diagonal_sv.public = other.public
        diagonal_sv.diagonal = True

        return diagonal_sv

    @property
    def ndim(self) -> int:
        if isinstance(TP, ByVal):
            return 0
        return self.share.ndim
    
    @property
    def size(self) -> int:
        return ndarray._count(_extract_shape(self.share))
    
    @property
    def shape(self) -> list[int]:
        return self.share.shape
    
    @property
    def I(self: Sharetensor[TP]) -> Sharetensor[TP]:
        identity = self.share.get_identity()
        if self.fp:
            one = double_to_fp(1.0, self.modulus)
            for i in range(len(identity)):
                identity[i][i] = one
        
        sv = Sharetensor(identity, self.modulus)
        sv.modulus = self.modulus
        sv.fp = self.fp
        sv.public = True
        sv.diagonal = True

        return sv
    
    @property
    def T(self: Sharetensor[TP]) -> Sharetensor[TP]:
        # TODO: Consider caching T.
        return Sharetensor(
            share = self.share.transpose(),
            x_r = self.x_r.transpose() if self.x_r else self.x_r,
            r = self.r.transpose() if self.r else self.r,
            modulus = self.modulus,
            sqrt = self.sqrt.transpose() if self.sqrt else self.sqrt,
            sqrt_inv = self.sqrt_inv.transpose() if self.sqrt_inv else self.sqrt_inv,
            fp = self.fp,
            public = self.public,
            diagonal = self.diagonal)

    def diagonal_contig(self: Sharetensor[TP], antidiagonal: bool = False) -> Sharetensor[TP]:
        return Sharetensor(
            share = self.share.diagonal_contig(antidiagonal),
            x_r = self.x_r.diagonal_contig(antidiagonal) if self.x_r else self.x_r,
            r = self.r.diagonal_contig(antidiagonal) if self.r else self.r,
            modulus = self.modulus,
            sqrt = self.sqrt.diagonal_contig(antidiagonal) if self.sqrt else self.sqrt,
            sqrt_inv = self.sqrt_inv.diagonal_contig(antidiagonal) if self.sqrt_inv else self.sqrt_inv,
            fp = self.fp,
            public = self.public,
            diagonal = self.diagonal)

    def is_empty(self) -> bool:
        return self.size == 0
    
    def is_partitioned(self: Sharetensor[TP]) -> bool:
        return bool(self.r)
    
    def set_partitions(self: Sharetensor[TP], partitions):
        self.x_r, self.r = partitions
    
    def get_partitions(self: Sharetensor[TP]):
        return self.x_r, self.r
    
    def get_partitions(self: Sharetensor[TP], mpc, force = False):
        if not self.is_partitioned() or force:
            self.set_partitions(mpc.arithmetic.__beaver_partition(self.share, self.modulus))
        return self.get_partitions()
    
    def is_fp(self):
        return self.fp
    
    def is_public(self):
        return self.public
    
    def copy(self) -> Sharetensor[TP]:
        share = self.share.copy()
        x_r = self.x_r.copy()
        r = self.r.copy()
        modulus = self.modulus
        sqrt = self.sqrt.copy()
        sqrt_inv = self.sqrt_inv.copy()
        fp = self.fp
        public = self.public
        diagonal = self.diagonal
        
        return Sharetensor(
            share, x_r, r, modulus, sqrt,
            sqrt_inv, fp, public, diagonal)
    
    def expand_dims(self, axis: int = 0):
        assert 0 <= axis < self.ndim, "Sharetensor: axis out of range for expand dim"
        return Sharetensor(
            share = self.share.expand_dims(axis),
            x_r = self.x_r.expand_dims(axis) if self.x_r else list[TP](),
            r = self.r.expand_dims(axis) if self.r else list[TP](),
            modulus = self.modulus,
            sqrt = self.sqrt.expand_dims(axis) if self.sqrt else list[TP](),
            sqrt_inv = self.sqrt_inv.expand_dims(axis) if self.sqrt_inv else list[TP](),
            fp = self.fp,
            public = self.public,
            diagonal = self.diagonal)
    
    def expand_values(self, count: int):
        return Sharetensor(
            share = [self.share.copy() for _ in range(count)],
            x_r = [self.x_r.copy() for _ in range(count)] if self.x_r else list[TP](),
            r = [self.r.copy() for _ in range(count)] if self.r else list[TP](),
            modulus = self.modulus,
            sqrt = [self.sqrt.copy() for _ in range(count)] if self.sqrt else list[TP](),
            sqrt_inv = [self.sqrt_inv.copy() for _ in range(count)] if self.sqrt_inv else list[TP](),
            fp = self.fp,
            public = self.public,
            diagonal = self.diagonal)
    
    def range(l, h, modulus):
        return Sharetensor(
            share = [int_t(l + i) for i in range(h - l)],
            x_r = [int_t(l + i) for i in range(h - l)],
            r = zeros_vec(h - l, TP=int_t),
            modulus = modulus,
            sqrt = list[int_t](0),
            sqrt_inv = list[int_t](0),
            fp = False,
            public = True,
            diagonal = False)
    
    def zeros(self) -> Sharetensor[TP]:
        return Sharetensor(
            share = self.share.zeros(),
            x_r = self.share.zeros(),
            r = self.share.zeros(),
            modulus = self.modulus,
            sqrt = self.share.zeros(),
            sqrt_inv = self.share.zeros(),
            fp = self.fp,
            public = self.public,
            diagonal = self.diagonal)

    def zeros(rows: int, cols: int, modulus: int_t) -> Sharetensor[list[list[int_t]]]:
        return Sharetensor(
            share = zeros_mat(rows, cols, TP=int_t),
            x_r = zeros_mat(rows, cols, TP=int_t),
            r = zeros_mat(rows, cols, TP=int_t),
            modulus = modulus,
            sqrt = zeros_mat(rows, cols, TP=int_t),
            sqrt_inv = zeros_mat(rows, cols, TP=int_t),
            fp = False,
            public = False,
            diagonal = rows == cols)
    
    def zeros(rows: int, cols: int, fp: bool, modulus: int_t) -> Sharetensor[list[list[int_t]]]:
        return Sharetensor(
            share = zeros_mat(rows, cols, TP=int_t),
            x_r = zeros_mat(rows, cols, TP=int_t),
            r = zeros_mat(rows, cols, TP=int_t),
            modulus = modulus,
            sqrt = zeros_mat(rows, cols, TP=int_t),
            sqrt_inv = zeros_mat(rows, cols, TP=int_t),
            fp = fp,
            public = False,
            diagonal = rows == cols)
    
    def zeros(size: int, modulus: int_t) -> Sharetensor[list[int_t]]:
        return Sharetensor(
            share = zeros_vec(size, TP=int_t),
            x_r = zeros_vec(size, TP=int_t),
            r = zeros_vec(size, TP=int_t),
            modulus = modulus,
            sqrt = zeros_vec(size, TP=int_t),
            sqrt_inv = zeros_vec(size, TP=int_t),
            fp = False,
            public = False,
            diagonal = False)
    
    def zeros(shape: Tuple[int, int], modulus: int_t) -> Sharetensor[list[list[int_t]]]:
        return Sharetensor.zeros(shape[0], shape[1], modulus)
    
    def zeros(shape: Tuple[int, int], fp: bool, modulus: int_t) -> Sharetensor[list[list[int_t]]]:
        return Sharetensor.zeros(shape[0], shape[1], fp, modulus)
    
    def zeros(self, shape: Tuple[int, int]) -> Sharetensor[list[list[int_t]]]:
        return Sharetensor.zeros(shape, self.fp, self.modulus)

    @staticmethod
    def ones(shape: Tuple[int, int], mpc, modulus: int_t):
        rows, cols = shape

        val = double_to_fp(1.0, modulus)
        fp = True

        ones_matrix = [[val for _ in range(cols)] for _ in range(rows)]
        
        return Sharetensor(
            share = ones_matrix if mpc.pid == 1 else zeros_mat(rows, cols, TP=int_t),
            x_r = ones_matrix,
            r = zeros_mat(rows, cols, TP=int_t),
            modulus = modulus,
            sqrt = ones_matrix,
            sqrt_inv = ones_matrix,
            fp = fp,
            public = False,
            diagonal = False)

    def ones(self: Sharetensor[TP], mpc):
        return Sharetensor[TP].ones(_extract_shape(self.share), mpc, self.modulus)

    @staticmethod
    def rand(shape: Tuple[int, int], mpc, modulus: int_t):
        rows, cols = shape
        rand_matrix = double_to_fp(random_floats(shape, scale=1.0) + 1 / (1 << 10), modulus)
        
        return Sharetensor(
            share = rand_matrix if mpc.pid == 1 else zeros_mat(rows, cols, TP=int_t),
            x_r = rand_matrix,
            r = zeros_mat(rows, cols, TP=int_t),
            modulus = modulus,
            sqrt = TP(),
            sqrt_inv = TP(),
            fp = True,
            public = False,
            diagonal = False)

    def raw_zeros(self):
        return self.share.zeros()

    def raw_ones(self):
        return self.share.ones()
    
    def append(self, other):
        self.share.append(other.share)
        if self.x_r: self.x_r.append(other.x_r)
        if self.r: self.r.append(other.r)
        if self.sqrt: self.sqrt.append(other.sqrt)
        if self.sqrt_inv: self.sqrt_inv.append(other.sqrt_inv)
    
    def extend(self, other):
        share = self.share.__copy__()
        x_r = self.x_r.__copy__()
        r = self.r.__copy__()
        modulus = self.modulus
        sqrt = self.sqrt.__copy__()
        sqrt_inv = self.sqrt_inv.__copy__()
        fp = self.fp
        public = self.public
        diagonal = False

        share.extend(other.share)
        if x_r: x_r.extend(other.x_r)
        if r: r.extend(other.r)
        if sqrt: sqrt.extend(other.sqrt)
        if sqrt_inv: sqrt_inv.extend(other.sqrt_inv)

        return Sharetensor(
            share, x_r, r, modulus, sqrt,
            sqrt_inv, fp, public, diagonal)
    
    def hstack(self, other: Sharetensor) -> Sharetensor:
        assert self.modulus == other.modulus, f"Sharetensor: modulus missmatch on hstack: {self.modulus} != {other.modulus}"
        assert self.fp == other.fp, "Sharetensor: fp type missmatch on hstack"
        assert self.public == other.public, "Sharetensor: public type missmatch on hstack"
        
        share = self.share.hstack(other.share)
        modulus = self.modulus
        fp = self.fp
        public = self.public
        diagonal = False
        
        x_r = TP(0)
        r = TP(0)
        sqrt = TP(0)
        sqrt_inv = TP(0)
        
        if self.x_r and other.x_r:
            x_r = self.x_r.hstack(other.x_r)
        if self.r and other.r:
            r = self.r.hstack(other.r)
        if self.sqrt and other.sqrt:
            sqrt = self.sqrt.hstack(other.sqrt)
        if self.sqrt_inv and other.sqrt_inv:
            sqrt_inv = self.sqrt_inv.hstack(other.sqrt_inv)

        return Sharetensor(
            share, x_r, r, modulus, sqrt,
            sqrt_inv, fp, public, diagonal)
    
    def vstack(self, other: Sharetensor) -> Sharetensor:
        assert self.modulus == other.modulus, "Sharetensor: modulus type missmatch on vstack"
        assert self.fp == other.fp, "Sharetensor: fp type missmatch on vstack"
        assert self.public == other.public, "Sharetensor: public type missmatch on vstack"
        
        share = self.share.vstack(other.share)
        modulus = self.modulus
        fp = self.fp
        public = self.public
        diagonal = False
        
        x_r = TP(0)
        r = TP(0)
        sqrt = TP(0)
        sqrt_inv = TP(0)
        
        if self.x_r and other.x_r:
            x_r = self.x_r.vstack(other.x_r)
        if self.r and other.r:
            r = self.r.vstack(other.r)
        if self.sqrt and other.sqrt:
            sqrt = self.sqrt.vstack(other.sqrt)
        if self.sqrt_inv and other.sqrt_inv:
            sqrt_inv = self.sqrt_inv.vstack(other.sqrt_inv)

        return Sharetensor(
            share, x_r, r, modulus, sqrt,
            sqrt_inv, fp, public, diagonal)
    
    def pad(self, rows: int, cols: int):
        return Sharetensor(
            self.share.pad(rows, cols),
            self.x_r.pad(rows, cols),
            self.r.pad(rows, cols),
            self.modulus,
            self.sqrt.pad(rows, cols),
            self.sqrt_inv.pad(rows, cols),
            self.fp,
            self.public,
            self.diagonal and rows == cols)
    
    def pad_with_value(self, val, size: int, axis: int, mpc):
        if not isinstance(val, ByVal):
            compile_error("Sharetensor: value has to be passed by value")
        if isinstance(val, float):
            assert self.fp, "Sharetensor: cannot pad a non-fp sharetensor with a float"
        
        value = (float(val) if self.fp else val).to_int_t(self.modulus)
        shared_value = value if mpc.pid == 1 else int_t(0)
        
        return Sharetensor(
            self.share.pad_with_value(value if self.public else shared_value, size, axis),
            self.x_r.pad_with_value(value, size, axis) if self.x_r else TP(),
            self.r.pad_with_value(int_t(0), size, axis) if self.r else TP(),
            self.modulus, TP(), TP(), self.fp, self.public, False)

    def pad_right(self, size: int):
        if DEBUG: assert isinstance(TP, list[int_t]), 'Cannot pad anything other than vector'
        extension = [int_t(0) for _ in range(size)]
        
        share = self.share.__copy__()
        x_r = self.x_r.__copy__()
        r = self.r.__copy__()
        modulus = self.modulus
        sqrt = self.sqrt.__copy__()
        sqrt_inv = self.sqrt_inv.__copy__()
        fp = self.fp
        public = self.public
        diagonal = False

        share.extend(extension)
        if x_r: x_r.extend(extension)
        if r: r.extend(extension)
        if sqrt: sqrt.extend(extension)
        if sqrt_inv: sqrt_inv.extend(extension)

        return Sharetensor(
            share, x_r, r, modulus, sqrt,
            sqrt_inv, fp, public, diagonal)
    
    def pad_left(self, size: int):
        if DEBUG: assert isinstance(TP, list[int_t]), 'Cannot pad anything other than vector'
        extension = [int_t(0) for _ in range(size)]
        
        share = extension.__copy__()
        x_r = extension.__copy__() if self.x_r else self.x_r
        r = extension.__copy__() if self.r else self.r
        modulus = self.modulus
        sqrt = extension.__copy__() if self.sqrt else self.sqrt
        sqrt_inv = extension.__copy__() if self.sqrt_inv else self.sqrt_inv
        fp = self.fp
        public = self.public
        diagonal = False

        share.extend(self.share)
        if self.x_r: x_r.extend(self.x_r)
        if self.r: r.extend(self.r)
        if self.sqrt: sqrt.extend(self.sqrt)
        if self.sqrt_inv: sqrt_inv.extend(self.sqrt_inv)
        
        return Sharetensor(
            share, x_r, r, modulus, sqrt, sqrt_inv,
            fp, public, diagonal)
    
    def filter(self, mask):
        mask_len = len(mask)

        return Sharetensor(
            share = [self.share[i] for i in range(mask_len) if mask[i]],
            x_r = [self.x_r[i] for i in range(mask_len) if mask[i]] if self.x_r else TP(),
            r = [self.r[i] for i in range(mask_len) if mask[i]] if self.r else TP(),
            modulus = self.modulus,
            sqrt = [self.sqrt[i] for i in range(mask_len) if mask[i]] if self.sqrt else TP(),
            sqrt_inv = [self.sqrt_inv[i] for i in range(mask_len) if mask[i]] if self.sqrt_inv else TP(),
            fp = self.fp,
            public = self.public,
            diagonal = self.diagonal)
    
    def getitem_complement(self, target_col: int) -> Sharetensor:
        return Sharetensor(
            share = self.share.getitem_complement(target_col),
            x_r = self.x_r.getitem_complement(target_col) if self.x_r else TP(),
            r = self.r.getitem_complement(target_col) if self.r else TP(),
            modulus = self.modulus,
            sqrt = self.sqrt.getitem_complement(target_col) if self.sqrt else TP(),
            sqrt_inv = self.sqrt_inv.getitem_complement(target_col) if self.sqrt_inv else TP(),
            fp = self.fp,
            public = self.public,
            diagonal = False)

    def sum(self, axis: int = 0):
        assert 0 <= axis < self.ndim, "Sharetensor: axis out of range for sum"
        _self = self.T if axis == 1 else self

        cumsum = _self[0].zeros()
        for e in _self:
            cumsum += e

        return cumsum
    
    def to_fp(self):
        if self.fp: return self
        modulus = self.modulus
        
        fp_one = double_to_fp(1.0, modulus)
        
        return Sharetensor(
            share = self.share.mul_mod(fp_one, modulus),
            x_r = self.x_r.mul_mod(fp_one, modulus),
            r = self.r.mul_mod(fp_one, modulus),
            modulus = modulus,
            sqrt = self.sqrt,
            sqrt_inv = self.sqrt_inv,
            fp = True,
            public = self.public,
            diagonal = self.diagonal)
    
    def trunc(self, fp):
        return Sharetensor(
            share = fp.trunc(self.share, self.modulus),
            x_r = TP(0),  # TODO: #61 Resolve #61 and calculate x_r here
            r = TP(0),  # TODO: #61 Resolve #61 and calculate x_r here
            modulus = self.modulus,
            sqrt = fp.trunc(self.sqrt, self.modulus) if self.sqrt else self.sqrt,
            sqrt_inv = fp.trunc(self.sqrt_inv, self.modulus) if self.sqrt_inv else self.sqrt_inv,
            fp = self.fp,
            public = self.public,
            diagonal = self.diagonal)
    
    def beaver_reveal(self, mpc):
        revealed_float = self.share.zeros_float()
        if mpc.pid:
            revealed_float = mpc.comms.print_fp((self.x_r if mpc.pid == 1 else self.x_r.zeros()).add_mod(self.r, self.modulus))
        
        return revealed_float
    
    def validate_partitions(self, mpc, message = ""):
        if mpc.pid:
            partition_share = (self.x_r if mpc.pid == 1 else self.x_r.zeros()).add_mod(self.r, self.modulus)
            partition_reveal = mpc.comms.reveal(partition_share, self.modulus)
            share_reveal = mpc.comms.reveal(self.share, self.modulus)
            assert partition_reveal == share_reveal, f"{message}:\n\tPartition reveal: {partition_reveal}\n\tShare reveal: {share_reveal}"

    def reveal(self, mpc, debug = False):
        if mpc.pid == 0:
            revealed_value = self.share.zeros()
        else:
            revealed_value = mpc.comms.reveal(self.share, self.modulus)
        
        if debug and mpc.pid == 2:
            print(f'{mpc.pid}: Revealed {"fixed-point" if self.fp else "int"} value - {revealed_value}')
        
        if self.fp and mpc.pid:
            revealed_floats = mpc.comms.print_fp(self.share, self.modulus, debug=debug)
        else:
            revealed_floats = revealed_value.to_float()
        
        if isinstance(revealed_floats, ByVal):
            return revealed_floats
        else:
            return array(revealed_floats)
    
    def publish(self, mpc):
        self.share = mpc.comms.reveal(self.share, self.modulus)
        self.public = True
        return self
    
    def flatten(self):
        share = self.share.flatten()
        return Sharetensor(
            share = share,
            x_r = self.x_r.flatten(),
            r = self.r.flatten(),
            modulus = self.modulus,
            sqrt = self.sqrt.flatten(),
            sqrt_inv = self.sqrt_inv.flatten(),
            fp = self.fp,
            public = self.public,
            diagonal = False)
    
    def reverse(self):
        self.share.reverse()
        self.x_r.reverse()
        self.r.reverse()
        self.sqrt.reverse()
        self.sqrt_inv.reverse()

        return self

    def reshape(self, shape):
        share = self.share.reshape(shape)
        return Sharetensor(
            share = share,
            x_r = self.x_r.reshape(shape) if self.x_r else self.x_r,
            r = self.r.reshape(shape) if self.r else self.r,
            modulus = self.modulus,
            sqrt = self.sqrt.reshape(shape) if self.sqrt else self.sqrt,
            sqrt_inv = self.sqrt_inv.reshape(shape) if self.sqrt_inv else self.sqrt_inv,
            fp = self.fp,
            public = self.public,
            diagonal = False)
    
    def parallel_add(self: Sharetensor[list[list[int_t]]], other: Sharetensor[list[int_t]]):
        return Sharetensor(
            share = self.share.parallel_add(other.share),
            x_r = (self.x_r.parallel_add(other.x_r)) if self.x_r else self.x_r,
            r = (self.r.parallel_add(other.r)) if self.r else self.r,
            modulus = self.modulus,
            sqrt = TP(0),
            sqrt_inv = TP(0),
            fp = self.fp,
            public = self.public,
            diagonal = False)
    
    def broadcast_add[BT](self: Sharetensor[list[BT]], other: Sharetensor[BT]):
        return Sharetensor(
            share = self.share.broadcast_add_mod(other.share, self.modulus),
            x_r = self.x_r.broadcast_add_mod(other.x_r, self.modulus) if self.x_r else self.x_r,
            r = self.r.broadcast_add_mod(other.r, self.modulus) if self.r else self.r,
            modulus = self.modulus,
            sqrt = TP(0),
            sqrt_inv = TP(0),
            fp = self.fp,
            public = self.public,
            diagonal = False)
    
    def to_ring(self, mpc):
        if DEBUG: assert self.modulus.popcnt() != 1, "Shared tensor already on ring."

        self.share = mpc.arithmetic.field_to_ring(self.share)
        # TODO: #128 Calculate partitions instead of deleting them on modulus switch
        self.x_r = TP(0)
        self.r = TP(0)
        self.modulus = RING_SIZE
        self.sqrt = mpc.arithmetic.field_to_ring(self.sqrt) if self.sqrt else self.sqrt
        self.sqrt_inv = mpc.arithmetic.field_to_ring(self.sqrt_inv) if self.sqrt_inv else self.sqrt_inv

    def to_field(self, mpc):
        if DEBUG: assert self.modulus.popcnt() == 1, "Shared tensor already on field."

        self.share = mpc.arithmetic.ring_to_field(self.share)
        # TODO: #128 Calculate partitions instead of deleting them on modulus switch
        self.x_r = TP(0)
        self.r = TP(0)
        self.modulus = FIELD_SIZE
        self.sqrt = mpc.arithmetic.ring_to_field(self.sqrt) if self.sqrt else self.sqrt
        self.sqrt_inv = mpc.arithmetic.ring_to_field(self.sqrt_inv) if self.sqrt_inv else self.sqrt_inv
    
    def to_bits(self, mpc, bitlen, delimiter_prime):
        if self.public:
            bits = num_to_bits(self.share, bitlen, True).to_int_t(self.modulus)
            st = Sharetensor(bits, int_t(delimiter_prime))
            st.public = True
            return st
        
        bit_decomposition = mpc.boolean.bit_decomposition(
            self.share, bitlen, delimiter_prime, self.modulus).to_int_t(self.modulus)
        return Sharetensor(bit_decomposition, int_t(delimiter_prime))
    
    def generate_random_bits(self, mpc, k: int, padding: int, n: int, little_endian: bool, small_modulus: int):
        r, rbits = mpc.boolean.__share_random_bits(k, padding, n, little_endian, small_modulus, self.modulus)
        rbits_extended = rbits.to_int_t(self.modulus)

        return Sharetensor(r, self.modulus), Sharetensor(rbits_extended, int_t(small_modulus))
    
    def __no_trunc_mul(self, other: float):
        """ Temporary method. Until #117 is fixed."""
        other_fp = double_to_fp(other, self.modulus)
        sv = self * other_fp
        sv.fp = True
        return sv

    # Mock methods (to match the interface of ndarray)
    @property
    def _internal_type(self) -> float:
        # TODO: Add dtype to Sharetensor
        return float()
    
    def astype(self, t: type) -> Sharetensor[TP]:
        return self
    
    def get_matmul_cost(self: Sharetensor, other: Sharetensor) -> float:
        return self.shape[0] * self.shape[1] * other.shape[1] * SMALL_CYCLES_INSTR_COST_ESTIMATE
    
    @staticmethod
    def get_matmul_cost_for_shapes(shape_1: tuple[int, int], shape_2: tuple[int, int]) -> float:
        return shape_1[0] * shape_1[1] * shape_2[1] * SMALL_CYCLES_INSTR_COST_ESTIMATE


@extend
class int:
    def __add__[TP](self, other: Sharetensor[TP]) -> Sharetensor[TP]:
        return other + self
    
    def __sub__[TP](self, other: Sharetensor[TP]) -> Sharetensor[TP]:
        return -other + self
    
    def __mul__[TP](self, other: Sharetensor[TP]) -> Sharetensor[TP]:
        return other * self
    
    def __truediv__[TP](self, other: Sharetensor[TP]) -> Sharetensor[TP]:
        raise NotImplementedError("Cannot divide by secure value without IR passes enabled.")


@extend
class float:
    def __add__[TP](self, other: Sharetensor[TP]) -> Sharetensor[TP]:
        return other + self
    
    def __sub__[TP](self, other: Sharetensor[TP]) -> Sharetensor[TP]:
        return -other + self
    
    def __mul__[TP](self, other: Sharetensor[TP]) -> Sharetensor[TP]:
        return other * self
