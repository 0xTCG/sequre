import operator

from numpy.ndarray import ndarray
from numpy.create import array, zeros
from pickler import pickle, unpickle
from helpers import argmin

from sequre.lattiseq.ckks import Ciphertext, Plaintext
from sequre.utils.utils import one_hot_vector
from sequre.utils.constants import HE_MUL_COST_ESTIMATE, HE_ROT_COST_ESTIMATE, C_CONTIG, D_CONTIG
from sequre.settings import DEBUG


class CipherTensor[ctype]:
    _data: list[ctype]
    shape: list[int]
    slots: int
    _chunk_size: int
    _transposed: bool
    _diagonal_contiguous: bool
    _skinny: bool

    def __init__(
            self,
            _data: list[ctype],
            shape: list[int],
            slots: int,
            _transposed: bool,
            _diagonal_contiguous: bool,
            _skinny: bool):
        self.__init__(_data, shape, slots, _transposed, _diagonal_contiguous)
        self._skinny = _skinny
    
    def __init__(
            self,
            _data: list[ctype],
            shape: list[int],
            slots: int,
            _transposed: bool,
            _diagonal_contiguous: bool):
        self.__init__(_data, shape, slots, _transposed)
        self._diagonal_contiguous = _diagonal_contiguous
    
    def __init__(
            self,
            _data: list[ctype],
            shape: list[int],
            slots: int,
            _transposed: bool):
        self.__init__(_data, shape, slots)
        self._transposed = _transposed
    
    def __init__(
            self,
            _data: list[ctype],
            shape: list[int],
            slots: int):
        self.__init__(shape, slots)
        self._data = _data
    
    def __init__(
            self,
            shape: list[int],
            slots: int):
        self._data = []
        self.shape = shape
        self.slots = slots
        self._reset_chunk_size()
    
    def __bool__(self) -> bool:
        return bool(self._data)
    
    def __repr__(self) -> str:
        return f"""
            Ciphertensor:
            \tShape: {self.shape}
            \tSlots: {self.slots}
            \tTransposed: {self._transposed}
            \tDiagonal order: {self._diagonal_contiguous}
            \t(Ciphertensor elements cannot be printed in a bulk)
        """
    
    def __iter__(self):
        for i in range(self.shape[0]):
            yield self[i]

    def __pickle__(self, jar: Jar, pasteurized: bool):
        pickle(self._skinny, jar, pasteurized)
        if not pasteurized: jar += self._skinny._pickle_size()
        pickle(self._diagonal_contiguous, jar, pasteurized)
        if not pasteurized: jar += self._diagonal_contiguous._pickle_size()
        pickle(self._transposed, jar, pasteurized)
        if not pasteurized: jar += self._transposed._pickle_size()
        pickle(self.slots, jar, pasteurized)
        if not pasteurized: jar += self.slots._pickle_size()
        pickle(self.shape, jar, pasteurized)
        if not pasteurized: jar += self.shape._pickle_size()
        pickle(self._data, jar, pasteurized)
    
    def __unpickle__(jar: Jar, pasteurized: bool) -> CipherTensor[ctype]:
        _skinny = unpickle(jar, pasteurized, bool)
        if not pasteurized: jar += _skinny._pickle_size()
        _diagonal_contiguous = unpickle(jar, pasteurized, bool)
        if not pasteurized: jar += _diagonal_contiguous._pickle_size()
        _transposed = unpickle(jar, pasteurized, bool)
        if not pasteurized: jar += _transposed._pickle_size()
        slots = unpickle(jar, pasteurized, int)
        if not pasteurized: jar += slots._pickle_size()
        shape = unpickle(jar, pasteurized, list[int])
        if not pasteurized: jar += shape._pickle_size()
        _data = unpickle(jar, pasteurized, List[ctype])

        return CipherTensor[ctype](
            _data=_data,
            shape=shape,
            slots=slots,
            _transposed=_transposed,
            _diagonal_contiguous=_diagonal_contiguous,
            _skinny=_skinny)
    
    def _pickle_size(self) -> int:
        return (self._skinny._pickle_size() +
                self._diagonal_contiguous._pickle_size() +
                self._transposed._pickle_size() +
                self.slots._pickle_size() +
                self.shape._pickle_size() +
                self._data._pickle_size())
    
    def copy(self) -> CipherTensor[ctype]:
        return CipherTensor[ctype](
            _data=self._data.copy(),
            shape=self.shape.copy(),
            slots=self.slots,
            _chunk_size=self._chunk_size,
            _transposed=self._transposed,
            _diagonal_contiguous=self._diagonal_contiguous,
            _skinny=self._skinny)

    def destroy(self):
        self._data.clear()
        self.shape.clear()
        self.slots = 0
        self._chunk_size = 0
        self._transposed = False
        self._diagonal_contiguous = False
        self._skinny = False

    @property
    def actual_shape(self):
        cs = self.shape.copy()
        
        if self._diagonal_contiguous and self._skinny:
            cs = cs[::-1]
        if self._transposed:
            cs = cs[::-1]

        return cs
    
    @property
    def cipher_shape(self):
        cs = self.shape.copy()
        cs[-1] = (cs[-1] + self.slots - 1) // self.slots
        return cs
    
    @property
    def ndim(self):
        return len(self.shape)
    
    @property
    def T(self) -> CipherTensor[ctype]:
        return CipherTensor[ctype](
            _data=self._data.copy(),
            shape=self.shape.copy(),
            slots=self.slots,
            _chunk_size=self._chunk_size,
            _transposed=self._transposed ^ True,
            _diagonal_contiguous=self._diagonal_contiguous,
            _skinny=self._skinny)
    
    def __add__(self: CipherTensor[ctype], other) -> CipherTensor[ctype]:
        raise NotImplementedError("Ciphertensors cannot be added without IR pass enabled.")

    def __sub__(self: CipherTensor[ctype], other) -> CipherTensor[ctype]:
        raise NotImplementedError("Ciphertensors cannot be subtracted without IR pass enabled.")
    
    def __mul__(self: CipherTensor[ctype], other) -> CipherTensor[ctype]:
        raise NotImplementedError("Ciphertensors cannot be multiplied without IR pass enabled.")

    def __matmul__(self: CipherTensor[ctype], other) -> CipherTensor[ctype]:
        raise NotImplementedError("Ciphertensors matrices cannot be multiplied without IR pass enabled.")

    def __gt__(self: CipherTensor[ctype], other) -> CipherTensor[ctype]:
        raise NotImplementedError("Ciphertensors cannot be compared without IR pass enabled.")

    def __lt__(self: CipherTensor[ctype], other) -> CipherTensor[ctype]:
        raise NotImplementedError("Ciphertensors cannot be compared without IR pass enabled.")

    def __eq__(self, other: CipherTensor[ctype]) -> bool:
        if self.shape != other.shape or self.slots != other.slots: return False
        return self._data == other._data
    
    def __getitem__(self, i: int) -> CipherTensor[ctype]:
        assert self.ndim > 1, "Ciphertensor: Cannot getitem from one-dimensional ciphertensor."
        
        if DEBUG:
            if self._transposed:
                print("INFO: getitem on transposed ciphertensor fetches column instead of row")
            if self._diagonal_contiguous:
                print("INFO: getitem on diagonal-contiguous ciphertensor fetches diagonal instead of row")
        
        return CipherTensor[ctype](
            _data=self._data[i * self._chunk_size:(i + 1) * self._chunk_size],
            shape=self.shape[1:],
            slots=self.slots)

    def __getitem__(self, s: slice) -> CipherTensor[ctype]:
        if DEBUG:
            if self._transposed:
                print("INFO: getitem on transposed ciphertensor fetches column instead of row")
            if self._diagonal_contiguous:
                print("INFO: getitem on diagonal-contiguous ciphertensor fetches diagonal instead of row")
        
        shape = self.shape.copy()
        shape[0] = s.stop - s.start

        start = s.start * self._chunk_size
        end = s.stop * self._chunk_size
        
        return CipherTensor[ctype](
            _data=self._data[start:end],
            shape=shape,
            slots=self.slots)

    def getitemdup(self, mpc, i: int, new_size: int) -> CipherTensor[ctype]:
        assert self.ndim == 1, "Ciphertensor: getitemdup can be only from one-dimensional CipherTensor."
        _data = []
        
        target_cipher_idx = i // self.slots
        target_offset = i % self.slots
        mask = mpc.he.enc_vector(one_hot_vector(target_offset, self.slots, TP=float), T=Plaintext)
        dedup_single = mpc.he.mul([self._data[target_cipher_idx]], mask)[0]

        ciphers_count = self.cipher_shape[-1]
        if ciphers_count > 1:
            dedup_base = dedup_single.copy()
            mpc.he.crypto_params.evaluator.reduce_add(dedup_base, self.slots)
        
            for _ in range(ciphers_count - 1):
                _data.append(dedup_base.copy())

        new_size_offset = new_size % self.slots
        if new_size_offset:
            dedup_edge = dedup_single.copy()
            distance = target_offset - new_size_offset + 1
            initial_rotation = distance if distance > 0 else (self.slots + distance)
            mpc.he.irotate([dedup_edge], initial_rotation)
            mpc.he.crypto_params.evaluator.reduce_add(dedup_edge, new_size_offset)
            _data.append(dedup_edge)
        elif len(_data):
            _data.append(_data[-1].copy())
        elif new_size:
            dedup_base = dedup_single.copy()
            mpc.he.crypto_params.evaluator.reduce_add(dedup_base, self.slots)
            _data.append(dedup_base)
        
        return CipherTensor[ctype](
            _data=_data,
            shape=[new_size],
            slots=self.slots)
    
    def mask(self, mpc, i: int) -> CipherTensor[ctype]:
        assert self.ndim == 1, "Ciphertensor: Only one-dimensional CipherTensors can be masked."
        return self.mul(mpc, array(one_hot_vector(i, self.shape[0], TP=float)))

    @staticmethod
    def _count(shape):
        if len(shape) == 0:
            return 0
        total = 1
        for i in range(len(shape)):
            total *= shape[i]
        return total

    @staticmethod
    def _count_ciphers(shape, slots):
        if len(shape) == 0:
            return 0
        total = 1
        for i in range(len(shape) - 1):
            total *= shape[i]
        return (shape[-1] + slots - 1) // slots * total
    
    @staticmethod
    def enc(mpc, data, padding: int = 0, mode: str = C_CONTIG) -> CipherTensor[ctype]:
        if isinstance(data, ndarray):
            _data = data
        elif isinstance(data, list):
            _data = array(data)
        else:
            compile_error("Ciphertensor: Invalid input for ciphertensor encoding/encryption")
        
        if not isinstance(_data.S, Tuple[int, int]):
            assert mode == C_CONTIG, "Ciphertensor: 2-dimensional tensors can be encoded only in c-contiguous mode"
            return CipherTensor[ctype].enc_c_contig(mpc, _data, padding)
        
        if mode == C_CONTIG:
            return CipherTensor[ctype].enc_c_contig(mpc, _data, padding)
        elif mode == D_CONTIG:
            return CipherTensor[ctype].enc_d_contig(mpc, _data, padding)
        else:
            raise ValueError("Invalid contig mode")
    
    @staticmethod
    def enc_c_contig[S, dtype](mpc, data: ndarray[S, dtype], padding: int = 0) -> CipherTensor[ctype]:
        if padding:
            data = data.pad(padding, axis=(staticlen(S) - 1))
        
        slots = mpc.he.crypto_params.params.slots()
        vec_len = data.shape[-1]
        flat_data = data.flatten()
        _data = List[ctype](CipherTensor._count_ciphers(data.shape, slots))
        for i in range(0, flat_data.size, vec_len):
            _data.extend(mpc.he.enc_vector(flat_data[i:i + vec_len].tolist(), T=ctype))

        return CipherTensor[ctype](
            _data=_data,
            shape=list(data.shape),
            slots=slots)
    
    @staticmethod
    def enc_d_contig[S, dtype](mpc, other: ndarray[S, dtype], padding: int = 0) -> CipherTensor[ctype]:
        assert isinstance(S, Tuple[int, int]), "Ciphertensor: Only 2-dimensional matrices can be diagonal-conting encoded"
        diagonals = zeros((min(other.shape), max(other.shape)), dtype=dtype)
        
        for i in range(min(other.shape)):
            diagonals[i] = other.cyclic_diag(i)
        
        diag_contig_ctensor = CipherTensor[ctype].enc_c_contig(mpc, diagonals, padding)
        diag_contig_ctensor._diagonal_contiguous = True
        diag_contig_ctensor._skinny = other.shape[1] < other.shape[0]
        return diag_contig_ctensor
    
    @staticmethod
    def enc_patch_copy(mpc, value, shape: list[int]) -> CipherTensor[ctype]:
        if not (isinstance(value, int) or isinstance(value, float)):
            compile_error("Ciphertensor: Invalid value type to patch_copy")
        
        slots = mpc.he.crypto_params.params.slots()
        enc_row = mpc.he.enc_vector([value for _ in range(shape[-1])], T=ctype)
        new_tensor_data = []
        for _ in range(shape[:-1].reduce_mul()):
            new_tensor_data.extend(enc_row.copy())
        
        return CipherTensor[ctype](
            _data=new_tensor_data,
            shape=shape,
            slots=slots)

    @staticmethod
    def zeros(mpc, shape: List[int]) -> CipherTensor[Ciphertext]:
        slots = mpc.he.crypto_params.params.slots()
        number_of_elements = CipherTensor._count_ciphers(shape, slots)
        
        return CipherTensor[Ciphertext](
            _data=[mpc.he.zero_cipher() for _ in range(number_of_elements)],
            shape=shape.copy(),
            slots=slots)

    def level(self) -> int:
        assert len(self._data), "Ciphertensor: Cannot get level of an empty tensor"
        return self._data[0].level()
    
    def decrypt(self, mpc) -> CipherTensor[Plaintext]:
        assert isinstance(ctype, Ciphertext), "Ciphertensor: Data already decrypted"
        return CipherTensor[Plaintext](
            _data=mpc.he.collective_decrypt_vector(self._data, mpc.comms.hub_pid),
            shape=self.shape.copy(),
            slots=self.slots,
            _transposed=self._transposed,
            _diagonal_contiguous=self._diagonal_contiguous,
            _skinny=self._skinny)
    
    def decode[T](self, mpc) -> ndarray:
        assert isinstance(ctype, Plaintext), "Ciphertensor: Make sure to decrypt the ciphertensor before decoding it"
        assert 0 < self.ndim < 3, "Ciphertensor: Only 1-dim and 2-dim ciphertensors can be revealed at the moment"

        new_shape = self.shape[:-1]
        last_lane = (self.shape[-1] + self.slots - 1) // self.slots * self.slots
        new_shape.append(last_lane)
        decoded_vector = mpc.he.decode_vector(self._data, DTP=T)

        assert (decoded_vector.size() + self.slots - 1) // self.slots == len(self._data) == self.cipher_shape.reduce_mul(), f"Ciphertensor: Input data shapes do not match encryption/encoding shape. Input shape: {decoded_vector.shape}. Enc vector len: {len(self._data)}. Slots: {self.slots}. Cipher shape: {self.cipher_shape}"

        new_shape_tuple = (1, new_shape[0]) if self.ndim == 1 else (new_shape[0], new_shape[1])
        shape_tuple = (1, self.shape[0]) if self.ndim == 1 else (self.shape[0], self.shape[1])
        arr = array(decoded_vector, dtype=T).reshape(new_shape_tuple).resize(shape_tuple)
        arr = arr.diagonal_contig(antidiagonal=True) if self._diagonal_contiguous else arr
        return arr.T if self._transposed else arr
    
    def reveal[T](self, mpc) -> ndarray:
        plain = self.decrypt(mpc) if isinstance(ctype, Ciphertext) else self
        return plain.decode(mpc, T=T)
    
    def append(self, other: CipherTensor[ctype]):
        assert not self._diagonal_contiguous and not other._diagonal_contiguous, "Ciphertensor: Appending diagonal-contiguous ciphertensors is not implemented yet"
        assert not self._transposed and not other._transposed, "Ciphertensor: Appending transposed ciphertensors is not implemented yet"
        assert self.ndim > 1, "Ciphertensor: Cannot append to one-dimensional Ciphertensor"
        assert self.shape[1:] == other.shape, "Ciphertensor: Invalid shapes for append"
        self._data.extend(other._data)
        self.shape[0] += 1
    
    def pop(self):
        assert not self._diagonal_contiguous, "Ciphertensor: Popping diagonal-contiguous ciphertensors is not implemented yet"
        assert not self._transposed, "Ciphertensor: Popping transposed ciphertensor is not implemented yet"
        assert self.ndim > 1, "Ciphertensor: Cannot pop from one-dimensional CipherTensor"
        self._data.len -= self._chunk_size
        self.shape[0] -= 1

    def extend(self, mpc, other: CipherTensor[ctype]):
        assert not (self._diagonal_contiguous ^ other._diagonal_contiguous), "Not implemented yet: Extending diagonal-contiguous by non-diagonal-contiguous ciphertensors is not implemented yet"
        assert not (self._transposed ^ other._transposed), "Not implemented yet: Extending transposed by non-transposed ciphertensor is not implemented yet"
        
        self_actual_shape = self.actual_shape
        other_actual_shape = other.actual_shape
        
        if self._diagonal_contiguous:
            assert self_actual_shape[0] == other_actual_shape[0], "Ciphertensor: Shapes missmatch on extending diagonal-contiguous matrices"
            if self._transposed:
                assert not self._skinny, "Not implemented yet: Extending fat diagonal-contiguous matrix is not implemented yet"
            else:
                assert self._skinny or self_actual_shape[0] == self_actual_shape[1], "Not implemented yet: Extending fat diagonal-contiguous matrix is not implemented yet"

            self._iconcat_axis_1_raw(mpc, other)
            self.shape[1] += other.shape[1]
        else:
            assert self_actual_shape[1:] == other_actual_shape[1:], "Ciphertensor: Invalid shapes for extend"
            if self._transposed:
                self._iconcat_axis_1_raw(mpc, other)
                self.shape[1] += other.shape[1]
            else:
                self._data.extend(other._data)
                self.shape[0] += other.shape[0]
    
    def iconcat(self, mpc, other: CipherTensor[ctype], axis: int) -> CipherTensor[ctype]:
        assert not self._diagonal_contiguous and not other._diagonal_contiguous, "Not implemented yet: Concatenating non-diagonal-contiguous ciphertensors is not implemented yet"
        assert not (self._transposed ^ other._transposed), "Not implemented yet: Concatenating transposed by non-transposed ciphertensor is not implemented yet"
        assert axis <= 1 and self.ndim == 2, "Not implemented yet: Ciphertensor concat not yet enabled for arbitrary ndim tensor. Internal note: Switch to ndarray to enable this"  # TODO
        assert self.shape[axis ^ 1] == other.shape[axis ^ 1], "Ciphertensor: Incompatible shapes for concatenation along provided axis"
        assert self.slots == other.slots, "Ciphertensor: Incompatible slots for concatenation"
        
        if self._transposed:
            axis ^= 1
        
        if axis == 0:
            self._data.extend(other._data)
        else:
            self._iconcat_axis_1_raw(mpc, other)
        
        self.shape[axis] += other.shape[axis]
        return self
    
    def concat(self, mpc, other: CipherTensor[ctype], axis: int) -> CipherTensor[ctype]:
        return self.copy().iconcat(mpc, other, axis)
    
    def resize(self, mpc, shape: list[int]) -> CipherTensor[ctype]:
        assert self.ndim == 1 and len(shape) == 1, "Not implemented yet: Can resize only 1-dim to 1-dim ciphertensors"
        
        other_cipher_shape = (shape[-1] + self.slots - 1) // self.slots
        if self.shape == shape or (self.shape[-1] < shape[-1] and self.cipher_shape[-1] == other_cipher_shape):
            return CipherTensor[ctype](
                _data=self._data.copy(),
                shape=shape.copy(),
                slots=self.slots)
        elif self.shape[-1] < shape[-1]:
            return self.concat(mpc, CipherTensor[ctype].zeros(mpc, [shape[-1] - self.shape[-1]]), axis=0)
        else:
            mask = CipherTensor[Plaintext].enc(mpc, [(1.0 if i < shape[-1] else 0.0) for i in range(self.shape[-1])])
            resized = self.mul(mpc, mask)

            return CipherTensor[ctype](
                _data=resized._data[:other_cipher_shape],
                shape=shape.copy(),
                slots=self.slots)
    
    def iadd(self, mpc, other) -> CipherTensor[Ciphertext]:
        assert not isinstance(ctype, Plaintext), "Ciphertensor: Cannot add to plaintext inplace"
        other_cipher = self._check_other_operand_elem_wise(mpc, other)

        mpc.he.iadd(self._data, other_cipher._data)
        return self

    def isub(self, mpc, other) -> CipherTensor[Ciphertext]:
        assert not isinstance(ctype, Plaintext), "Ciphertensor: Cannot subtract from plaintext inplace"
        other_cipher = self._check_other_operand_elem_wise(mpc, other)

        mpc.he.isub(self._data, other_cipher._data)
        return self

    def imul(self, mpc, other) -> CipherTensor[Ciphertext]:
        assert not isinstance(ctype, Plaintext), "Ciphertensor: Cannot multiply plaintext inplace"
        other_cipher = self._check_other_operand_elem_wise(mpc, other)

        mpc.he.imul(self._data, other_cipher._data)
        return self
    
    def irotate(self, mpc, step: int) -> CipherTensor[ctype]:
        shape = self.actual_shape
        step = (shape[-1] + step) % shape[-1]

        if step == 0:
            return self

        # Diagonal-contiguous rotation is cheaper than rotating the c-continguous matrix
        if self._diagonal_contiguous:
            assert self.ndim == 2, "Ciphertensor: Cannot rotate. Diagonal-contiguous ciphertensor needs to be 2-dimensional"
            _data = []
            for i in range(self.shape[0]):
                _data.extend(self[(step + i) % self.shape[0]].irotate(mpc, step + i)._data)

            self._data = _data
        # Case c-contiguous 2-dimensional or higher tensor. Rotate only largest axis.
        elif self.ndim > 1:
            for row in self:
                row.irotate(mpc, step)
        # Best case scenario. Rotating by multiplicative of slots requires no homomorphic rotation.
        elif step % self.slots == 0 and self.shape[-1] % self.slots == 0:
            self._data.irotate(step // self.slots)
        # Second-best case scenario. If cipher fits in the number of slots, only one homomorphic rotation is needed.
        elif self.shape[-1] == self.slots:
            mpc.he.irotate(self._data, step % self.slots)
        # Worst case scenario. Either the rotation is done over multiple ciphertexts or encoded data size is less than the number of slots
        else:
            step_offset = step % self.slots
            mpc.he.irotate(self._data, step_offset)

            mask = [(0 if (i < self.slots - step_offset) else 1) for i in range(self.slots)]
            cipher_mask = mpc.he.enc_vector(mask, T=Plaintext)[0]
            cipher_mask_inv = mpc.he.enc_vector(mask ^ 1, T=Plaintext)[0]
            maks_enc = [cipher_mask for _ in range(len(self._data))]
            mask_inv_enc = [cipher_mask_inv for _ in range(len(self._data))]
            
            offset_tensor_data = mpc.he.mul(self._data, maks_enc).irotate(1)
            
            data_offset = self.shape[-1] % self.slots
            if data_offset:
                mpc.he.irotate([offset_tensor_data[-1]], self.slots - data_offset)

            mpc.he.imul(self._data, mask_inv_enc)
            mpc.he.iadd(self._data, offset_tensor_data)


            if self.slots < step < self.shape[-1]:
                self._data.irotate((step + self.slots - 1) // self.slots)

        return self

    def add(self, mpc, other):
        if isinstance(ctype, Plaintext) and isinstance(other, CipherTensor[Ciphertext]):
            return other.add(mpc, self)
        elif isinstance(ctype, Plaintext):
            return self._handle_plaintext_case(mpc, other, operator.add)
        
        return self.copy().iadd(mpc, other)

    def sub(self, mpc, other):
        assert not (isinstance(ctype, Plaintext) and isinstance(other, CipherTensor[Ciphertext])), "Ciphertensor: (Not implemented yet) Cannot subtract ciphertext from plaintext"
        if isinstance(ctype, Plaintext):
            return self._handle_plaintext_case(mpc, other, operator.sub)
        
        return self.copy().isub(mpc, other)
    
    def mul(self, mpc, other):
        if isinstance(ctype, Plaintext) and isinstance(other, CipherTensor[Ciphertext]):
            return other.mul(mpc, self)
        elif isinstance(ctype, Plaintext):
            return self._handle_plaintext_case(mpc, other, operator.mul)
        
        return self.copy().imul(mpc, other)
    
    def rotate(self, mpc, step: int) -> CipherTensor[ctype]:
        return self.copy().irotate(mpc, step)

    def shift(self, mpc, step: int) -> CipherTensor[ctype]:
        """
        Note: Shifting adds an extra ciphertext to the tensor and starts with (self.slots - step) number of zeros.
        Warning: This changes the shape of the ciphertensor. All zeros will be included in the rotated Ciphertensor.
        Example: [] - CipherTensor, () - Single ciphertext
            [(1, 2, 3, 4), (5, 6, 7, 8)].shift(2) -> [(0, 0, 1, 2), (3, 4, 5, 6), (7, 8, 0, 0)]
        """
        assert step < self.slots, "Ciphertensor: Shifting requires step to be less than the number of slots per each cipher"
        assert self.ndim == 1, "Ciphertensor: Only one-dimensional CipherTensors can be shifted."

        rotated_tensor_data = mpc.he.rotate(self._data, step)
        
        mask = [(0 if (i < self.slots - step) else 1) for i in range(self.slots)]
        cipher_mask = mpc.he.enc_vector(mask, T=Plaintext)[0]
        cipher_mask_inv = mpc.he.enc_vector(mask ^ 1, T=Plaintext)[0]
        maks_enc = [cipher_mask for _ in range(len(self._data))]
        mask_inv_enc = [cipher_mask_inv for _ in range(len(self._data))]
        
        offset_tensor_data = mpc.he.mul(rotated_tensor_data, maks_enc)
        offset_tensor_data.append(mpc.he.zero_cipher())
        mpc.he.imul(rotated_tensor_data, mask_inv_enc)
        rotated_tensor_data.insert(0, mpc.he.zero_cipher())
        mpc.he.iadd(rotated_tensor_data, offset_tensor_data)

        shape = self.shape.copy()
        shape[-1] = (self.cipher_shape[-1] + 1) * self.slots

        return CipherTensor[ctype](
            _data=rotated_tensor_data,
            shape=shape,
            slots=self.slots,
            _transposed=self._transposed)

    def patch_copy(self, mpc, new_size: int):
        """
        Examples: [] - CipherTensor, () - Single ciphertext
            [(1, 2, 3, 4), (5, 6, 0, 0)].patch_copy(7) -> [(1, 2, 3, 4), (5, 6, 1, 0)]
            [(1, 2, 3, 4), (5, 6, 0, 0)].patch_copy(8) -> [(1, 2, 3, 4), (5, 6, 1, 2)]
            [(1, 2, 3, 4), (5, 6, 0, 0)].patch_copy(9) -> [(1, 2, 3, 4), (5, 6, 1, 2), (3, 0, 0, 0)]
            [(1, 2, 3, 4), (5, 6, 0, 0)].patch_copy(10) -> [(1, 2, 3, 4), (5, 6, 1, 2), (3, 4, 0, 0)]
        """
        assert 0 < self.ndim < 3, "Ciphertensor: Patch-copying can be applied only to 1-dim and 2-dim tensors"
        assert self.shape[-1] < new_size, "Ciphertensor: Can only patch_copy to larger array. Use CipherTensor.resize() for trimming."

        if isinstance(ctype, Plaintext):
            # TODO: Read dtype (T) from self (add dtype static to CipherTensor class)
            return CipherTensor[Plaintext].enc(mpc, self.decode(mpc, T=float).patch_copy(new_size))

        patch_copy_tensor = self.copy()

        if self._diagonal_contiguous:
            raise NotImplementedError()
        elif self.ndim == 2:
            patch_copy_data = []

            for i in range(self.shape[0]):
                patch_copy_data.extend(patch_copy_tensor[i].patch_copy(mpc, new_size)._data)
            
            return CipherTensor[ctype](
                _data=patch_copy_data,
                shape=[self.shape[0], new_size],
                slots=self.slots,
                _transposed=self._transposed,
                _diagonal_contiguous=self._diagonal_contiguous)

        while patch_copy_tensor.shape[0] < new_size:
            offset = patch_copy_tensor.shape[0] % self.slots
            if offset:
                shifted_tensor_data = patch_copy_tensor.shift(mpc, self.slots - offset)._data

                if patch_copy_tensor.shape[0] <= self.slots - offset:
                    # Last cipher in shifted tensor is all zeros
                    shifted_tensor_data.pop()

                mpc.he.crypto_params.evaluator.add(
                    patch_copy_tensor._data[-1],
                    shifted_tensor_data[0],
                    patch_copy_tensor._data[-1])
                
                patch_copy_tensor._data.extend(shifted_tensor_data[1:])
            else:
                patch_copy_tensor._data.extend(patch_copy_tensor._data.copy())
            patch_copy_tensor.shape[0] <<= 1

        for _ in range(len(patch_copy_tensor._data) - (new_size + self.slots - 1) // self.slots):
            patch_copy_tensor._data.pop()

        offset = new_size % self.slots
        if offset and ((new_size >> (new_size.__cttz__())) != self.shape[0]):  # if offset or self.shape[0] != new_size // 2^k
            mask = mpc.he.enc_vector([(1.0 if i < offset else 0.0) for i in range(self.slots)], T=Plaintext)
            patch_copy_tensor._data[-1] = mpc.he.mul([patch_copy_tensor._data[-1]], mask)[0]

        patch_copy_tensor.shape = [new_size]

        return patch_copy_tensor
    
    def reduce_add(self, mpc) -> CipherTensor[ctype]:
        assert self.ndim == 1, "Ciphertensor: Addition reduction can be applied only to 1-dimensional tensors"
        
        if isinstance(ctype, Plaintext):
            # TODO: Read dtype (T) from other (add dtype static to CipherTensor class)
            _sum: float = self.decode(mpc, T=float).sum()
            return CipherTensor[Plaintext].enc(mpc, [_sum for _ in range(self.shape[-1])])

        assert not self._transposed, "Ciphertensor: Addition reduction of transposed ciphertensor is not implemented yet"
        return CipherTensor[ctype](
            _data=mpc.he.reduce_add(self._data, self.shape[-1]),
            shape=[self.shape[-1]],
            slots=self.slots)

    def reduce_add_tiled(self, mpc, tile_size: int) -> CipherTensor[ctype]:
        assert 0 < self.ndim < 3, "Ciphertensor: Tiled addition reduction can be applied only to 1-dim and 2-dim tensors"

        if self.ndim == 2:
            if self._diagonal_contiguous:
                assert not self._skinny, "Not implemented yet: Tiled addition reduction of skinny diagonal-contiguous matrices not implemented yet"
            
            reduced_tiled_data = []
            for i in range(self.shape[0]):
                reduced_tiled_data.extend(self[i].reduce_add_tiled(mpc, tile_size)._data)
            
            return CipherTensor[ctype](
                _data=reduced_tiled_data,
                shape=[self.shape[0], tile_size],
                slots=self.slots,
                _transposed=self._transposed,
                _diagonal_contiguous=self._diagonal_contiguous,
                _skinny=self._diagonal_contiguous and self.shape[0] < tile_size)
        
        size = self.shape[0]
        tail = size % tile_size
        cipher_offset = size % self.slots
        tile_offset = self.slots % tile_size
        if tile_size > self.slots or tail or tile_offset or (cipher_offset and len(self._data) > 1):
            raise NotImplementedError("(Internal TODO) CipherTensor: Tiled addition reduction case not implemented yet")
        
        cipher = self._data[0].copy()

        for i in range(1, len(self._data)):
            mpc.he.crypto_params.evaluator.add(cipher, self._data[i], cipher)

        rotation_step = tile_size
        while rotation_step < size:
            mpc.he.crypto_params.evaluator.add(
                cipher,
                mpc.he.crypto_params.evaluator.rotate_new(cipher, rotation_step),
                cipher)
            rotation_step <<= 1
        
        mask = mpc.he.enc_vector([(1.0 if i < tile_size else 0.0) for i in range(self.slots)], T=Plaintext)
        return CipherTensor[ctype](
            _data=mpc.he.mul([cipher], mask),
            shape=[tile_size],
            slots=self.slots)
    
    def matmul(self, mpc, other, debug=True) -> CipherTensor[ctype]:
        assert self.ndim == other.ndim == 2, f"Ciphertensor: At least one of the tensors is not a matrix. Self shape: {self.shape}, other shape: {other.shape}. Ciphertensor matmul supports only matrices at the moment."
        
        m, n = self.shape
        assert m.popcnt() == n.popcnt() == 1, "Ciphertensor: Temporary constraint. Matmul dimensions need to be power of 2 for now. This is soon to be fixed."

        if isinstance(other, CipherTensor):
            if self._diagonal_contiguous or other._diagonal_contiguous:
                return self._matmul_v3(mpc, other, debug)
            elif self._transposed and other._transposed:
                raise NotImplementedError()
            elif self._transposed:
                raise NotImplementedError()
            elif other._transposed:
                return self._matmul_v1(mpc, other, debug)
            else:
                return self._matmul_v2(mpc, other, debug)
        elif isinstance(other, ndarray):
            return self._switch_matmul_by_cost(mpc, other, debug)
        elif isinstance(other, list):
            return self._switch_matmul_by_cost(mpc, array(other), debug)
        else:
            compile_error("Invalid matmul operand type")

    @staticmethod
    def _get_matmul_v1_cost(first, other):
        slots = first.slots
        self_shape = first.shape
        other_shape = other.shape

        cost_per_cipher = (self_shape[1] + slots - 1) // slots
        output_size = self_shape[0] * other_shape[1]

        return ((cost_per_cipher + 1) * output_size * HE_MUL_COST_ESTIMATE +
                output_size * cost_per_cipher * (slots - 1).bitlen() * HE_ROT_COST_ESTIMATE)

    @staticmethod
    def _get_matmul_v2_cost(first, other):
        slots = first.slots
        self_shape = first.shape
        other_shape = other.shape

        cost_per_cipher = (self_shape[1] + slots - 1) // slots
        self_size = self_shape[0] * self_shape[1]

        return (cost_per_cipher * self_size * HE_MUL_COST_ESTIMATE +
                self_size * (min(other_shape[1], slots) - 1).bitlen() * HE_ROT_COST_ESTIMATE)
    
    @staticmethod
    def _get_matmul_v3_cost(first, other):
        slots = first.slots
        self_shape = first.shape
        other_shape = other.shape

        cost_per_cipher = (max(max(self_shape), max(other_shape)) + slots - 1) // slots
        iter_count = self_shape[0] * min(other_shape)

        return (cost_per_cipher * iter_count * (HE_MUL_COST_ESTIMATE + HE_ROT_COST_ESTIMATE) +
                (max(other_shape) - 1).bitlen() * HE_ROT_COST_ESTIMATE)
    
    def _get_matmul_tnt_cost(self, other):
        slots = self.slots
        self_actual_shape = self.actual_shape
        other_shape = other.shape

        cost_per_cipher = (max(self_actual_shape[0], max(other_shape)) + slots - 1) // slots
        iter_count = min(self_actual_shape[0], other_shape[1]) * other_shape[0]

        return cost_per_cipher * iter_count * (HE_MUL_COST_ESTIMATE + HE_ROT_COST_ESTIMATE)

    def _switch_matmul_by_cost[dtype](self, mpc, other: ndarray[Tuple[int, int], dtype], debug) -> CipherTensor[ctype]:
        if isinstance(ctype, Plaintext):
            return CipherTensor[Plaintext].enc(mpc, self.decode(mpc, T=dtype) @ other)
        
        if self._diagonal_contiguous:
            raise NotImplementedError()
        if self._transposed:
            return self._matmul_tnt(mpc, CipherTensor[Plaintext].enc(mpc, other), debug)
        
        costs = (CipherTensor._get_matmul_v1_cost(self, other),
                 CipherTensor._get_matmul_v2_cost(self, other),
                 CipherTensor._get_matmul_v3_cost(self, other))
        
        if debug:
            print(f"\nMatmul costs:\n\tM1: {costs[0]}\n\tM2: {costs[1]}\n\tM3: {costs[2]}\n")
                
        match argmin(costs):
            case 0:
                other_cipher = CipherTensor[Plaintext].enc(mpc, other.T)
                other_cipher._transposed = True
                return self._matmul_v1(mpc, other_cipher, debug)
            case 1:
                return self._matmul_v2(mpc, CipherTensor[Plaintext].enc(mpc, other), debug)
            case 2:
                return self._matmul_v3(mpc, CipherTensor[Plaintext].enc(mpc, other, mode=D_CONTIG), debug)
            case _:
                raise ValueError("Invalid cost index")
    
    def _matmul_v1(self, mpc, other: CipherTensor, debug=True):
        """
        M1 method from https://arxiv.org/pdf/2304.00129.pdf
        """
        self_shape = self.shape
        other_shape = other.shape
        if debug: print(f"Using M1 method for ciphertensor matrix multiplication for {self_shape} x {other_shape} operands")
        assert self_shape[1] == other_shape[1], f"Ciphertensor: Invalid matrix dimentions for M1 matmul {self_shape} x {other_shape}"
        assert other._transposed, "Ciphertensor: Ciphertensor should be lazily transposed prior to M1 matrix multiplication by it"
        assert not self._diagonal_contiguous and not other._diagonal_contiguous, "Ciphertensor: Cannot apply M1 matrix multiplication method to diagonal-contiguous ciphertensors"

        masks = [mpc.he.enc_vector(one_hot_vector(i % self.slots, self.slots, TP=float), T=Plaintext) for i in range(other_shape[0])]
        
        new_cipher_tensor = CipherTensor[Ciphertext](
                shape=[0, other_shape[0]],
                slots=self.slots)
        for i in range(self_shape[0]):
            new_row = CipherTensor[Ciphertext].zeros(mpc, [other_shape[0]])

            for j in range(other_shape[0]):
                reduction = self[i].mul(mpc, other[j]).reduce_add(mpc)

                if reduction.shape[0] < other_shape[0]:
                    reduction = reduction.patch_copy(mpc, other_shape[0])

                target_cipher = new_row._data[j // self.slots]
                mpc.he.crypto_params.evaluator.add(
                    target_cipher,
                    mpc.he.mul([reduction._data[0]], masks[j])[0],
                    target_cipher)
                if debug: print(f"Ciphertensor M1 matmul: {i + 1}/{self_shape[0]} -- {j + 1}/{other_shape[0]}")
            if debug: print("----------------------")
            
            new_cipher_tensor.append(new_row)
        
        return new_cipher_tensor

    def _matmul_v2(self, mpc, other: CipherTensor, debug=True):
        """
        M2 method from https://arxiv.org/pdf/2304.00129.pdf
        """
        self_shape = self.shape
        other_shape = other.shape
        if debug: print(f"Using M2 method for ciphertensor matrix multiplication for {self_shape} x {other_shape} operands")
        assert self_shape[1] == other_shape[0], f"Ciphertensor: Invalid matrix dimentions for M2 matmul {self_shape} x {other_shape}"
        assert not other._transposed, "Ciphertensor: Ciphertensor should not be lazily transposed prior to M2 matrix multiplication by it"
        assert not self._diagonal_contiguous and not other._diagonal_contiguous, "Ciphertensor: Cannot apply M1 matrix multiplication method to diagonal-contiguous ciphertensors"

        new_cipher_tensor = CipherTensor[ctype].zeros(mpc, [self_shape[0], other_shape[1]])

        for i in range(self_shape[0]):
            if debug: print(f"Ciphertensor M2 matmul: Computing row {i + 1}/{self_shape[0]} ...")
            new_row = new_cipher_tensor[i]
            self_row = self[i]
            for j in range(self_shape[1]):
                other_elem_duplicated = self_row.getitemdup(mpc, j, other_shape[1])
                new_row.iadd(mpc, other[j].mul(mpc, other_elem_duplicated))
        
        return new_cipher_tensor

    def _matmul_v3(self, mpc, other: CipherTensor, debug=True):
        if self._diagonal_contiguous and other._diagonal_contiguous:
            return self._matmul_v3_11(mpc, other, debug)
        elif self._diagonal_contiguous:
            return self._matmul_v3_10(mpc, other, debug)
        elif other._diagonal_contiguous:
            return self._matmul_v3_01(mpc, other, debug)
        else:
            raise ValueError("Ciphertensor: Cannot apply M3 matrix multiplication method if none of the operands is diagonal-contiguous")
    
    def _matmul_v3_01(self, mpc, other: CipherTensor, debug=True):
        """
        M3 method from https://arxiv.org/pdf/2304.00129.pdf
        Case self non-diagonal-contiguous and other is diagonal-contiguous.
        """
        self_actual_shape = self.actual_shape
        other_actual_shape = other.actual_shape
        
        if debug: print(f"Using M3 method for ciphertensor matrix multiplication for {self_actual_shape} x {other_actual_shape} operands")
        assert self_actual_shape[1] == other_actual_shape[0], f"Ciphertensor: Invalid matrix dimentions for M3 matmul {self_actual_shape} x {other_actual_shape}"
        assert other._diagonal_contiguous, "Ciphertensor: _matmul_v3_01 expects second operand to be diagonal-contiguous"
        assert not self._transposed and not other._transposed, "Not implemented yet: Transposed diagonal-contiguous matmul case is not added yet"

        if self.shape[1] < other.shape[1]: patch_copy_self = self.patch_copy(mpc, other.shape[1])
        else: patch_copy_self = self.copy()

        semi_slot = patch_copy_self.shape[1] < self.slots >> 1
        # No need to do actual rotation downstream if data size is less than slots // 2
        # Patch copy self to double size
        if semi_slot:
            patch_copy_self = patch_copy_self.patch_copy(mpc, patch_copy_self.shape[1] << 1)
            # Assume diagonals are trailed by zeros and pad them lazily by expanding shape
            other.shape[1] <<= 1
        
        new_data = []
        debug_counter = 0
        for row in patch_copy_self:
            if debug:
                debug_counter += 1
                print(f"Ciphertensor M3 matmul: Computing row {debug_counter}/{patch_copy_self.shape[0]} ...")
            
            new_row = row.mul(mpc, other[0])
            
            for i in range(1, min(other.shape)):
                if semi_slot: mpc.he.irotate(row._data, 1)
                else: row.irotate(mpc, 1)
                new_row.iadd(mpc, row.mul(mpc, other[i]))
            
            new_data.extend(new_row._data)
        
        new_cipher_tensor = CipherTensor[ctype](
                _data=new_data,
                shape=patch_copy_self.shape,
                slots=self.slots)

        if other_actual_shape[1] < patch_copy_self.shape[1]:
            return new_cipher_tensor.reduce_add_tiled(mpc, other_actual_shape[1])
        elif semi_slot:
            new_cipher_tensor.shape[1] >>= 1
            new_cipher_tensor._reset_chunk_size()
        
        return new_cipher_tensor

    def _matmul_v3_10(self, mpc, other: CipherTensor, debug=True):
        raise NotImplementedError("INTERNAL: _matmul_v3_10 case not implemented yet")
        return CipherTensor[ctype]()
    
    def _matmul_v3_11(self, mpc, other: CipherTensor, debug=True):
        raise NotImplementedError("INTERNAL: _matmul_v3_11 case not implemented yet")
        return CipherTensor[ctype]()
    
    def _matmul_tnt(self, mpc, other: CipherTensor, debug=True):
        """
        Case self is c-contiguous transposed and other is c-contiguous not transposed.
        """
        self_actual_shape = self.actual_shape
        
        if debug: print(f"Using c-contig transposed-non-transposed method for ciphertensor matrix multiplication for {self_actual_shape} x {other.shape} operands")
        assert self_actual_shape[1] == other.shape[0], f"Ciphertensor: Invalid matrix dimentions for M3 matmul {self_actual_shape} x {other.shape}"
        assert not self._diagonal_contiguous and not other._diagonal_contiguous, "Ciphertensor: _matmul_tnt expects operand to be c-contiguous"
        assert self._transposed and not other._transposed, "Ciphertensor: _matmul_tnt expects first operand to be transposed and second operand not to be transposed"

        if self.shape[1] < other.shape[1]: patch_copy_self = self.patch_copy(mpc, other.shape[1])
        else: patch_copy_self = self.copy()
        if self.shape[1] > other.shape[1]: other = other.patch_copy(mpc, self.shape[1])

        semi_slot = patch_copy_self.shape[1] < self.slots >> 1
        # No need to do actual rotation downstream if data size is less than slots // 2
        # Patch copy self to double size
        if semi_slot:
            patch_copy_self = patch_copy_self.patch_copy(mpc, patch_copy_self.shape[1] << 1)
            # Assume diagonals are trailed by zeros and pad them lazily by expanding shape
            other.shape[1] <<= 1
        
        new_data = []
        debug_counter = 0
        diagonals_count = min(self_actual_shape[0], other.shape[1])
        for d_idx in range(diagonals_count):
            if debug:
                debug_counter += 1
                print(f"Ciphertensor M3 matmul: Computing diagonal {debug_counter}/{diagonals_count} ...")
            
            if semi_slot and d_idx: mpc.he.irotate(patch_copy_self._data, 1)
            elif d_idx: patch_copy_self.irotate(mpc, 1)
            new_diagonal = patch_copy_self[0].mul(mpc, other[0])
            
            for i in range(1, other.shape[0]):
                new_diagonal.iadd(mpc, patch_copy_self[i].mul(mpc, other[i]))
                
            new_data.extend(new_diagonal._data)
        
        if semi_slot:
            patch_copy_self.shape[1] >>= 1
        
        return CipherTensor[ctype](
                _data=new_data,
                shape=[self_actual_shape[0], patch_copy_self.shape[1]],
                slots=self.slots,
                _transposed=False,
                _diagonal_contiguous=True,
                _skinny=self_actual_shape[0] < patch_copy_self.shape[1])

    def _handle_plaintext_case(self, mpc, other, op) -> CipherTensor[Plaintext]:
        # # TODO: Read dtype on decoding (T) from other (add dtype static to CipherTensor class)
        # if not isinstance(other, CipherTensor):
        #     return CipherTensor[Plaintext].enc(mpc, op(self.decode(mpc, T=float), other))
        # elif isinstance(other.ctype, Plaintext):
        #     return CipherTensor[Plaintext].enc(mpc, op(self.decode(mpc, T=float), other.decode(mpc, T=float)))
        # else:
        #     compile_error("Invalid type for plaintext case elem-wise operand")
        raise NotImplementedError("Plaintext-plaintext elem-wise operations not implemented yet. Decoding is too expensive.")
        return CipherTensor[Plaintext]()
    
    def _check_other_operand_elem_wise(self, mpc, other):
        enc_mode = D_CONTIG if self._diagonal_contiguous else C_CONTIG
        if isinstance(other, CipherTensor):
            assert not (self._transposed ^ other._transposed), "Ciphertensor: Elem-wise op on transposed to non-transposed ciphertensor is not implemented yet"
            assert not (self._diagonal_contiguous ^ other._diagonal_contiguous), "Ciphertensor: Elem-wise op on diagonal-contiguous to non-diagonal-contiguous ciphertensor is not implemented yet"
            pt = other
        elif isinstance(other, ndarray):
            operand = other.T if self._transposed else other
            pt = CipherTensor[Plaintext].enc(mpc, operand, mode=enc_mode)
        elif isinstance(other, List):
            operand = other.transpose() if self._transposed else other
            pt = CipherTensor[Plaintext].enc(mpc, array(operand), mode=enc_mode)
        elif isinstance(other, int) or isinstance(other, float):
            ptensor = CipherTensor[Plaintext].enc_patch_copy(mpc, other, self.shape)
            ptensor._transposed = self._transposed
            ptensor._diagonal_contiguous = self._diagonal_contiguous
            pt = ptensor
        else:
            compile_error("Invalid operand for CipherTensor")

        assert self.shape == pt.shape, f"Ciphertensor.elem_wise_op: Shapes mismatch: {self.shape} != {pt.shape}"
        return pt
    
    def _reset_chunk_size(self):
        self._chunk_size = 1 if len(self.shape) <= 1 else CipherTensor._count(self.cipher_shape[1:])

    def _iconcat_axis_1_raw(self, mpc, other):
        assert self.shape[0] == other.shape[0], "Ciphertensor: shapes missmatch while concatenating along axis 1"
        _data = []
        offset = self.shape[1] % self.slots
        
        for self_row, other_row in zip(self, other):
            _data.extend(self_row._data)
            if offset == 0:
                _data.extend(other_row._data)
            else:
                rotated_other = other_row.shift(mpc, self.slots - offset)
                mask = mpc.he.enc_vector([(1.0 if i < offset else 0.0) for i in range(self.slots)], T=Plaintext)
                mpc.he.imul([_data[-1]], mask)
                mpc.he.iadd([_data[-1]], [rotated_other._data[0]])
                _data.extend(rotated_other._data[1:])

                if other.shape[1] < self.slots - offset:
                    _data.pop()
        
        self._data = _data


@extend
class ndarray:
    def __matmul__(self, other):
        if isinstance(other, CipherTensor):
            raise NotImplementedError("Cannot matmul by secure value without IR passes enabled.")
            return type(other)()  # To avoid compiler error
        elif isinstance(other, ndarray):
            assert self.shape[1] == other.shape[0] and self.ndim == other.ndim == 2, "ndarray matmul: Invalid shapes"
            
            m, n = self.shape[0], other.shape[1]
            _data = ptr[T](m * n)
            _other_t = other.T

            for i in range(m):
                self_row = self[i]
                start = i * n
                for j in range(n):
                    _data[start + j] = (self_row * _other_t[j]).sum(axis=0)
            
            return ndarray[S, T]._new_contig((m, n), _data)
        else:
            compile_error("Invalid operand for ndarray matmul")
    
    def __add__(self, other: CipherTensor):
        raise NotImplementedError("Cannot add by secure value without IR passes enabled.")
    
    def __mul__(self, other: CipherTensor):
        raise NotImplementedError("Cannot multiply by secure value without IR passes enabled.")
    
    def add(self, mpc, other: CipherTensor):
        return other.add(mpc, self)
    
    def mul(self, mpc, other: CipherTensor):
        return other.mul(mpc, self)
    
    def matmul[ctype](self, mpc, other: CipherTensor[ctype], debug: bool = True) -> CipherTensor[ctype]:
        if isinstance(ctype, Plaintext):
            return CipherTensor[Plaintext].enc(mpc, self @ other.decode(mpc, T=T))
        
        return self._switch_matmul_by_cost(mpc, other, debug)
    
    def _switch_matmul_by_cost[ctype](self, mpc, other: CipherTensor[ctype], debug) -> CipherTensor[ctype]:
        # TODO: Implement cost calculation
        return self._matmul_v1(mpc, other, debug)
    
    def _matmul_v1[ctype](self, mpc, other: CipherTensor[ctype], debug: bool = True) -> CipherTensor[ctype]:
        if other._transposed:
            return CipherTensor[Plaintext].enc(mpc, self)._matmul_v1(mpc, other)

        assert self.ndim == other.ndim == 2, f"CipherTensor: At least one of the tensors is not a matrix. Self shape: {self.shape}, other shape: {other.shape}. Ciphertensor matmul supports only matrices at the moment"
        assert self.shape[1] == other.shape[0], f"CipherTensor: Invalid matrix dimentions for reversed matmul: {self.shape} x {other.shape}"

        new_cipher_tensor = CipherTensor[ctype].zeros(mpc, [self.shape[0], other.shape[1]])

        debug_counter = 1
        for self_column, other_row in zip(self.T, other):
            if debug: print(f"CipherTensor reversed matmul: Computing row {debug_counter}/{self.shape[1]} ...")
            for i, self_elem in enumerate(self_column):
                new_cipher_tensor[i].iadd(mpc, other_row.mul(mpc, self_elem))
            debug_counter += 1
        
        return new_cipher_tensor

    def _matmul_v2[ctype](self, mpc, other: CipherTensor[ctype], debug: bool = True) -> CipherTensor[ctype]:
        assert self.ndim == other.ndim == 2, f"Ciphertensor: At least one of the tensors is not a matrix. Self shape: {self.shape}, other shape: {other.shape}. Ciphertensor matmul supports only matrices at the moment"
        
        # A @ B = (B.T @ A.T).T
        return other.T.matmul(mpc, self.T, debug).T
