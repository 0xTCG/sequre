import prg, secure_operator

from numpy.ndarray import ndarray
from numpy.create import zeros

from helpers import argmin

from shared_tensor import SharedTensor
from cipher_tensor import CipherTensor
from multiparty_partition import MPP
from utils import double_to_fp

from ..stdlib.fp import fp_div, fp_sqrt
from ..utils.param import int_t, DEBUG


TP = int_t


@extend
class SharedTensor:
    def to_mhe(self, mpc) -> CipherTensor:
        if mpc.pid == 0:
            raise NotImplementedError("SharedTensor.to_mhe should not be called at trusted dealer")
        slots = mpc.he.crypto_params.params.slots()
        
        # Pad shape if ciphertexts are not fully utilized
        new_shape = self.shape.copy()
        new_shape[-1] = max(slots, new_shape[-1])
        share = self.share
        if new_shape[-1] != self.shape[-1]:
            share = self.share.resize(new_shape)

        ciphervector = mpc.he.additive_share_vector_to_ciphervector(share.flatten(), self.modulus)

        return CipherTensor(ciphervector, self.shape, slots)

    def to_mpp[S, dtype](self, mpc, ratios: list[int]) -> MPP[S, dtype]:
        if mpc.pid == 0:
            if staticlen(S) == 2:
                shape = (self.shape[0], self.shape[1])
            elif staticlen(S) == 1:
                shape = (self.shape[0],)
            else:
                compile_error("MPP supports only 1-dim and 2-dim arrays at the momemt")

            return MPP[S, dtype](
                modulus=self.modulus,
                _local_data=zeros(shape, dtype=dtype))
        
        return self.to_mhe(mpc).to_mpp(mpc, ratios, self.modulus, S=S, dtype=dtype)


@extend
class CipherTensor:
    def to_mpc[dtype](self, mpc, modulus: int_t) -> SharedTensor:
        if mpc.pid == 0:
            raise NotImplementedError("CipherTensor.to_mpc should not be called at trusted dealer")
        
        cipher_shape = self.cipher_shape
        cipher_shape[-1] *= self.slots
        share = mpc.he.ciphervector_to_additive_share_vector(
            self._data, self.cipher_shape.reduce_mul() * self.slots, modulus, dtype=dtype).reshape(cipher_shape).resize(self.shape)
        stensor = SharedTensor(share, modulus)
        if isinstance(dtype, float):
            stensor.fp = True
        return stensor
    
    def to_mpp[S, dtype](self, mpc, ratios: list[int], modulus: int_t) -> MPP[S, dtype]:
        if mpc.pid == 0:
            if staticlen(S) == 2:
                shape = (self.shape[0], self.shape[1])
            elif staticlen(S) == 1:
                shape = (self.shape[0],)
            else:
                compile_error("MPP supports only 1-dim and 2-dim arrays at the momemt")

            return MPP[S, dtype](
                modulus=modulus,
                _local_data=zeros(shape, dtype=dtype))
        
        if mpc.pid != mpc.comms.hub_pid:
            return MPP[S, dtype](
                modulus=modulus,
                _ratios=mpc.comms.receive_as_jar(mpc.comms.hub_pid, list[int]),
                _encryption_unified=mpc.comms.receive_as_jar(mpc.comms.hub_pid, CipherTensor[ctype]))
        
        n = self.shape[0]
        s = ratios.sum()
        assert n % s == 0, f"CipherTensor: Cannot distribute partitions by the given proportion. Shape: {self.shape}. Ratios: {ratios}"

        new_ratios = ratios * (n // s)
        
        for p in range(1, mpc.comms.number_of_parties):
            reference_pid = p - 1
            start = new_ratios[:reference_pid].sum()
            end = start + new_ratios[reference_pid]

            if p == mpc.comms.hub_pid:
                _mpp = MPP[S, dtype](
                    modulus=modulus,
                    _ratios=new_ratios,
                    _encryption_unified=self[start:end])
            else:
                mpc.comms.send_as_jar(new_ratios, p)
                mpc.comms.send_as_jar(self[start:end], p)
        
        return _mpp


@extend
class MPP:
    def to_mpc(self, mpc) -> SharedTensor:
        if mpc.pid == 0:
            return SharedTensor(zeros(self.shape, dtype=dtype).tolist().to_int_t(self.modulus), self.modulus)
        return self.collect(mpc).to_mpc(mpc, self.modulus, dtype=dtype)


class InternalMPC:
    def evalp(mpc, x_, coefs_, exps_):
        assert isinstance(x_[0], SharedTensor), "Secure polynomial evaluation is enabled only for SharedTensor"
        for x in x_:
            assert not x.fp, "Not implemented error: Polynomial evaluation is not implemented for fixed-point numbers yet"
        modulus = x_[0].modulus
        
        x_r = []
        r = []

        for sn in x_:
            x_r_, r_ = sn.get_partitions(mpc, force=False)
            x_r.append(x_r_)
            r.append(r_)

        coefs = [coefs_[i].to_int_t(modulus) for i in range(len(coefs_))]
        exps = [[exps_[i + j] for j in range(len(x_))]
                for i in range(0, len(exps_), len(x_))]
        
        result = mpc.polynomial._beaver_evaluate_poly(
            x_r, r, coefs, exps, modulus)
        sv = SharedTensor(result, modulus)
        sv.fp = x_[0].is_fp()
        
        # TODO: #23 Handle sqrts and partitions

        return sv

    def add(mpc, x, y):
        if isinstance(x, float):
            return InternalMPC.__add_public(mpc, x.to_fp(y.modulus), y, False)
        elif isinstance(y, float):
            return InternalMPC.__add_public(mpc, y.to_fp(x.modulus), x, False)
        elif isinstance(x, int):
            return InternalMPC.__add_public(mpc, x.to_fp(y.modulus) if y.fp else int_t(x), y, False)
        elif isinstance(y, int):
            return InternalMPC.__add_public(mpc, y.to_fp(x.modulus) if x.fp else int_t(y), x, False)
        elif isinstance(x, SharedTensor[list[list[int_t]]]) and isinstance(y, SharedTensor[list[int_t]]):
            return x.broadcast_add(y)
        elif isinstance(x, SharedTensor[list[int_t]]) and isinstance(y, SharedTensor[int_t]):
            return x.broadcast_add(y)
        else:
            if not x.is_public() and not y.is_public():
                return x + y
            elif x.is_public():
                return InternalMPC.__add_public(mpc, x.share, y, x.diagonal)
            elif y.is_public():
                return InternalMPC.__add_public(mpc, y.share, x, y.diagonal)

            raise ValueError("Invalid type of addends in secure add")

    def sub(mpc, x, y):
        if isinstance(x, float):
            return InternalMPC.__add_public(mpc, x.to_fp(y.modulus), -y, False)
        elif isinstance(y, float):
            return InternalMPC.__add_public(mpc, (-y).to_fp(x.modulus), x, False)
        elif isinstance(x, int):
            return InternalMPC.__add_public(mpc, x.to_fp(y.modulus) if y.fp else int_t(x), -y, False)
        elif isinstance(y, int):
            return InternalMPC.__add_public(mpc, (-y).to_fp(x.modulus) if x.fp else (x.modulus - y), x, False)
        elif isinstance(x, SharedTensor[list[list[int_t]]]) and isinstance(y, SharedTensor[list[int_t]]):
            return x.broadcast_add(-y)
        elif isinstance(x, SharedTensor[list[int_t]]) and isinstance(y, SharedTensor[int_t]):
            return x.broadcast_add(-y)
        else:
            if not x.is_public() and not y.is_public():
                return x + (-y)
            elif x.is_public():
                return InternalMPC.__add_public(mpc, x.share, -y, x.diagonal)
            elif y.is_public():
                return InternalMPC.__add_public(mpc, y.share.neg_mod(x.modulus), x, y.diagonal)

            raise ValueError("Invalid type of addends in sub")

    def mul(mpc, x, y):
        if isinstance(y, float):
            y_fp = double_to_fp(y, x.modulus)
            sv = x * y_fp
            # TODO: #117 Implement clever joint truncations pattern matcher
            if x.is_fp(): sv = sv.trunc(mpc.fp)
            sv.fp = True
            return sv
        else:
            if x.public or y.public:
                c_share = x.share.mul_mod(y.share, x.modulus if y.public else y.modulus)
                sv = SharedTensor(c_share, x.modulus if y.public else y.modulus)
                sv.fp = x.is_fp() or y.is_fp()
                return sv
            
            if DEBUG: assert x.modulus == y.modulus, f"Non-matching moduli for factors: {x.modulus} != {y.modulus}"
            modulus = x.modulus

            x_1_r, r_1 = x.get_partitions(mpc, force=False)
            x_2_r, r_2 = y.get_partitions(mpc, force=False)

            c = mpc.arithmetic.__beaver_mul(x_1_r, r_1, x_2_r, r_2, modulus)
            c = mpc.arithmetic.__beaver_reconstruct(c, modulus)

            if x.is_fp() and y.is_fp():
                c = mpc.fp.trunc(c, modulus)
            
            sv = SharedTensor(c, modulus)
            sv.fp = x.is_fp() or y.is_fp()

            # TODO: #23 Efficiently calculate beaver partitions of c here
            # Bellow is temp dirty solution for beaver partitioning which should be both:
            # - Computed with less network overhead
            # - Computed only if compiler figures out that partitions will be needed downstream
            # sv.set_partitions(mpc.arithmetic.__beaver_partition(sv.share, modulus))

            # TODO: #23 Check if there is a better way to do this
            # if x.sqrt and y.sqrt:
            #     sv.sqrt = mpc.arithmetic.multiply(x.sqrt, y.sqrt, modulus)
            #     sv.sqrt = mpc.fp.trunc(sv.sqrt, modulus)
            #     sv.sqrt_inv = mpc.arithmetic.multiply(x.sqrt_inv, y.sqrt_inv, modulus)
            #     sv.sqrt_inv = mpc.fp.trunc(sv.sqrt_inv, modulus)
            return sv

    def pow(mpc, x_, p):
        modulus = x_.modulus

        if x_.is_fp():
            # TODO: #58 Implement efficient pows calculation for FPs
            x_pow = x_
            for _ in range(p - 1): x_pow = InternalMPC.mul(mpc, x_pow, x_)
            return x_pow

        x_r, r = x_.get_partitions(mpc, force=False)
        c = mpc.polynomial.powers_cached(x_r, r, p, modulus)[p]
        
        sv = SharedTensor(c, modulus)
        sv.fp = x_.is_fp()

        # TODO: #23 Efficiently calculate beaver partitions of c here
        # TODO: #23 Check if there is a way to calculate cached sqrts efficiently
        return sv

    def div(mpc, x, y):
        # TODO: #26 Currently does not work for complex algebraic structures.
        # Resolve issue #26 and change code below in order to fix this.
        if isinstance(x, int) or isinstance(x, float):
            if isinstance(y.share, list):
                sv = SharedTensor([double_to_fp(float(x), y.modulus) for _ in range(len(y.share))], y.modulus)
                return fp_div(mpc, sv, y)
            else:
                sv = SharedTensor(double_to_fp(float(x), y.modulus), y.modulus)
                return fp_div(mpc, sv, y)
        elif isinstance(y, float) or isinstance(y, int):
            sv = x * double_to_fp(1.0 / y, x.modulus)
            if x.is_fp(): sv = sv.trunc(mpc.fp)
            return sv
        else:
            # TODO: Efficiently calculate beaver partitions of sv here
            return fp_div(mpc, x, y)
    
    def gt(mpc, x, y):
        # TODO: #26 Currently does not work for complex algebraic structures.
        # Resolve issue #26 and change code below in order to fix this.
        if isinstance(y, int) or isinstance(y, TP):
            modulus = x.modulus
            if y == 0:
                return SharedTensor(mpc.boolean.is_positive(x.share, modulus), modulus)

            return SharedTensor(mpc.boolean.greater_than_public(x.share, TP(y), modulus), modulus)
        elif isinstance(y, float):
            modulus = x.modulus
            return SharedTensor(mpc.boolean.greater_than_public(x.to_fp().share, double_to_fp(y, modulus), modulus), modulus)
        else:
            if DEBUG: assert x.modulus == y.modulus
            modulus = x.modulus
            return SharedTensor(mpc.boolean.greater_than(x.share, y.share, modulus), modulus)
        
        # TODO: Efficiently calculate beaver partitions of sv here

    def lt(mpc, x, y):
        # TODO: #26 Currently does not work for complex algebraic structures.
        # Resolve issue #26 and change code below in order to fix this.
        if isinstance(y, int) or isinstance(y, TP):
            modulus = x.modulus
            if y == 0:
                return SharedTensor(mpc.boolean.is_positive(x.share.neg_mod(modulus), modulus), modulus)
            
            return SharedTensor(mpc.boolean.less_than_public(x.share, TP(y), modulus), modulus)
        elif isinstance(y, float):
            modulus = x.modulus
            return SharedTensor(mpc.boolean.less_than_public(x.to_fp().share, double_to_fp(y, modulus), modulus), modulus)
        else:
            if DEBUG: assert x.modulus == y.modulus
            modulus = x.modulus
            return SharedTensor(mpc.boolean.less_than(x.share, y.share, modulus), modulus)
        
        # TODO: #23 Efficiently calculate beaver partitions of sv here
    
    def eq(mpc, x, y):
        if x.modulus.popcnt() == 1:
            # TODO: #158 Make comparisons stable on rings.
            l = InternalMPC.gt(mpc, InternalMPC.sub(mpc, x, y), 0)
            r = InternalMPC.gt(mpc, InternalMPC.sub(mpc, y, x), 0)
            return InternalMPC.sub(mpc, 1, InternalMPC.add(mpc, l, r))

        sub = InternalMPC.sub(mpc, x, y)
        is_pos = InternalMPC.gt(mpc, InternalMPC.mul(mpc, sub, sub), 0)
        return InternalMPC.sub(mpc, 1, is_pos)

    def sqrt_inv(mpc, x, y):
        # TODO: #26 Currently does not work for complex algebraic structures.
        # Resolve issue #26 and change code below in order to fix this.
        
        if not y.sqrt_inv:
            s, sinv = fp_sqrt(mpc, y)
            y.sqrt, y.sqrt_inv = s.share, sinv.share
        
        if isinstance(x, int):
            sv = SharedTensor(y.sqrt_inv, y.modulus)
            sv.fp = True
            # TODO: #23 Efficiently calculate beaver partitions of c here
            # Bellow is temp dirty solution for beaver partitioning which should be both:
            # - Computed with less network overhead
            # - Computed only if compiler figures out that partitions will be needed downstream
            # sv.set_partitions(mpc.arithmetic.__beaver_partition(sv.share, modulus))
            return sv * x
        else:
            if DEBUG: assert x.modulus == y.modulus
            modulus = x.modulus
            
            x_1_r, r_1 = x.get_partitions(mpc, force=False)
            x_2_r, r_2 = mpc.arithmetic.__beaver_partition(y.sqrt_inv, modulus)
            
            c = mpc.arithmetic.__beaver_mul(x_1_r, r_1, x_2_r, r_2, modulus)
            c = mpc.arithmetic.__beaver_reconstruct(c, modulus)
            if x.is_fp():
                c = mpc.fp.trunc(c, modulus)
            
            sv = SharedTensor(c, modulus)
            sv.fp = True
            # TODO: #23 Efficiently calculate beaver partitions of c here
            # Bellow is temp dirty solution for beaver partitioning which should be both:
            # - Computed with less network overhead
            # - Computed only if compiler figures out that partitions will be needed downstream
            # sv.set_partitions(mpc.arithmetic.__beaver_partition(sv.share, modulus))
            
            return sv

    def dot(mpc, x, y):
        if DEBUG: assert x.modulus == y.modulus
        modulus = x.modulus

        x_1_r, r_1 = x.get_partitions(mpc, force=False)
        x_2_r, r_2 = y.get_partitions(mpc, force=False)

        c = mpc.arithmetic.__beaver_dot_prod(x_1_r, r_1, x_2_r, r_2, modulus)
        c = mpc.arithmetic.__beaver_reconstruct(c, modulus)

        if x.is_fp() and y.is_fp():
            c = mpc.fp.trunc(c, modulus)
        
        sv = SharedTensor(c, modulus)
        sv.fp = x.is_fp() or y.is_fp()
        # TODO: #23 Efficiently calculate beaver partitions of sv here
        # Bellow is temp dirty solution for beaver partitioning which should be both:
        # - Computed with less network overhead
        # - Computed only if compiler figures out that partitions will be needed downstream
        # sv.set_partitions(mpc.arithmetic.__beaver_partition(sv.share, modulus))
        
        return sv

    def dot(mpc, x):
        return InternalMPC.dot(mpc, x, x)

    def matmul(mpc, x, y):
        if DEBUG: assert x.modulus == y.modulus
        modulus = x.modulus

        x_1_r, r_1 = x.get_partitions(mpc, force=False)
        x_2_r, r_2 = y.get_partitions(mpc, force=False)

        c = mpc.arithmetic.__beaver_matmul(x_1_r, r_1, x_2_r, r_2, modulus)
        c = mpc.arithmetic.__beaver_reconstruct(c, modulus)

        if x.is_fp() and y.is_fp():
            c = mpc.fp.trunc(c, modulus)
        
        sv = SharedTensor(c, modulus)
        sv.fp = x.is_fp() or y.is_fp()

        # TODO: #23 Efficiently calculate beaver partitions of sv here
        # Bellow is temp dirty solution for beaver partitioning which should be both:
        # - Computed with less network overhead
        # - Computed only if compiler figures out that partitions will be needed downstream
        # sv.set_partitions(mpc.arithmetic.__beaver_partition(sv.share, modulus))
        
        # TODO: #23 Check if there is a better way to do this
        # if x.sqrt and y.sqrt:
        #     sv.sqrt = mpc.arithmetic.multiply(x.sqrt, y.sqrt, modulus)
        #     sv.sqrt = mpc.fp.trunc(sv.sqrt, modulus)
        #     sv.sqrt_inv = mpc.arithmetic.multiply(x.sqrt_inv, y.sqrt_inv, modulus)
        #     sv.sqrt_inv = mpc.fp.trunc(sv.sqrt_inv, modulus)
        
        return sv

    def sqrt(mpc, x):
        # TODO: #26 Currently does not work for complex algebraic structures.
        # Resolve issue #26 and change code below in order to fix this.
        if not x.sqrt:
            s, sinv = fp_sqrt(mpc, x)
            x.sqrt, x.sqrt_inv = s.share, sinv.share
        
        sv = SharedTensor(x.sqrt, x.modulus)
        sv.fp = True
        # TODO: #23 Efficiently calculate beaver partitions of sv here
        return sv

    def reveal_to_all(mpc, value):
        value.share = mpc.comms.reveal_to_all(value.share, value.modulus)
        value.public = True
        return value

    def dist(mpc, shape, name, modulus, params):
        if name == 'normal':
            rows, cols = shape
            gaussian = [[double_to_fp(prg.gauss(*params), modulus) if mpc.pid == 0 else TP(0)
                        for _ in range(cols)] for _ in range(rows)]
            gaussian = mpc.comms.share_from_trusted_dealer(gaussian, modulus)

            stensor = SharedTensor(gaussian, modulus)
            stensor.fp = True

            return stensor

        raise NotImplementedError(f'Distribution {name} not implemented yet.')
    
    def __add_public(mpc, x_public, y, diagonal):
        share = y.share
        modulus = y.modulus

        if isinstance(share, list[list]) and isinstance(x_public, list[list]):
            if diagonal:
                for i in range(len(share)):
                    share[i][i] = mpc.arithmetic.add_public(share[i][i], x_public[i][i], modulus)
            else: share = mpc.arithmetic.add_public(share, x_public, modulus)
        else: share = mpc.arithmetic.add_public(share, x_public, modulus)
        
        sv = SharedTensor(share, modulus)
        sv.fp = y.fp

        if y.sqrt:
            sv.sqrt = type(share)(0)
            sv.sqrt_inv = type(share)(0)
        
        if not y.is_partitioned():
            return sv
        
        sv.x_r = y.x_r.add_mod(x_public, modulus)
        sv.r = y.r

        return sv        


class InternalMHE:
    def add(mpc, x, y):
        return x.add(mpc, y)

    def sub(mpc, x, y):
        return x.sub(mpc, y)
    
    def mul(mpc, x, y):
        return x.mul(mpc, y)

    def matmul(mpc, x, y):
        return x.matmul(mpc, y)
    
    def gt(mpc, x, y):
        if isinstance(x, CipherTensor): x_shared = x.to_mpc(mpc, mpc.base_modulus)
        else: x_shared = x
        
        if isinstance(y, CipherTensor): y_shared = y.to_mpc(mpc, mpc.base_modulus)
        else: y_shared = y
        
        return InternalMPC.gt(mpc, x_shared, y_shared).to_mhe(mpc)

    def lt(mpc, x, y):
        if isinstance(x, CipherTensor): x_shared = x.to_mpc(mpc, mpc.base_modulus)
        else: x_shared = x
        
        if isinstance(y, CipherTensor): y_shared = y.to_mpc(mpc, mpc.base_modulus)
        else: y_shared = y
        
        return InternalMPC.lt(mpc, x_shared, y_shared).to_mhe(mpc)

    def eq(mpc, x, y):
        return x == y


class InternalMPP:
    def add(mpc, x, y):
        return x._do_elem_wise_op(mpc, y, secure_operator.add)

    def sub(mpc, x, y):
        return x._do_elem_wise_op(mpc, y, secure_operator.sub)
    
    def mul(mpc, x, y):
        return x._do_elem_wise_op(mpc, y, secure_operator.mul)

    def matmul(mpc, x, y):
        mpc.stats.secure_matmul_complexity += (x.cohort_shape[0] * x.cohort_shape[1] * y.cohort_shape[1])
        return MPP.matmul(mpc, x, y)
    
    def gt(mpc, x, y):
        if isinstance(x, MPP):
            x_shared = x.to_mpc(mpc)
            y_shared = y
            ref_mpp = x
        elif isinstance(y, MPP):
            y_shared = y.to_mpc(mpc)
            x_shared = x
            ref_mpp = y
        else:
            compile_error("At least one operand should be MPP")
        
        return InternalMPC.gt(mpc, x_shared, y_shared).to_mpp(mpc, ratios=ref_mpp._ratios, S=ref_mpp.S, dtype=ref_mpp.dtype)

    def lt(mpc, x, y):
        if isinstance(x, MPP):
            x_shared = x.to_mpc(mpc)
            y_shared = y
            ref_mpp = x
        elif isinstance(y, MPP):
            y_shared = y.to_mpc(mpc)
            x_shared = x
            ref_mpp = y
        else:
            compile_error("At least one operand should be MPP")
        
        return InternalMPC.lt(mpc, x_shared, y_shared).to_mpp(mpc, ratios=ref_mpp._ratios, S=ref_mpp.S, dtype=ref_mpp.dtype)

    def eq(mpc, x, y):
        return x._do_elem_wise_op(mpc, y, MPP._elem_wise_eq)


class Internal:
    def secure_add(mpc, x, y):
        mpc.stats.secure_add_count += 1
        if isinstance(x, MPP) or isinstance(y, MPP):
            return InternalMPP.add(mpc, x, y)
        elif isinstance(x, SharedTensor) or isinstance(y, SharedTensor):
            return InternalMPC.add(mpc, x, y)
        elif isinstance(x, CipherTensor) or isinstance(y, CipherTensor):
            return InternalMHE.add(mpc, x, y)
        else: compile_error("Invalid secure operands")

    def secure_sub(mpc, x, y):
        mpc.stats.secure_sub_count += 1
        if isinstance(x, MPP) or isinstance(y, MPP):
            return InternalMPP.sub(mpc, x, y)
        elif isinstance(x, SharedTensor) or isinstance(y, SharedTensor):
            return InternalMPC.sub(mpc, x, y)
        elif isinstance(x, CipherTensor) or isinstance(y, CipherTensor):
            return InternalMHE.sub(mpc, x, y)
        else: compile_error("Invalid secure operands")

    def secure_mul(mpc, x, y):
        mpc.stats.secure_mul_count += 1
        if isinstance(x, MPP) or isinstance(y, MPP):
            return InternalMPP.mul(mpc, x, y)
        elif isinstance(x, SharedTensor) or isinstance(y, SharedTensor):
            return InternalMPC.mul(mpc, x, y)
        elif isinstance(x, CipherTensor) or isinstance(y, CipherTensor):
            return InternalMHE.mul(mpc, x, y)
        else: compile_error("Invalid secure operands")
    
    def secure_matmul(mpc, x, y):
        mpc.stats.secure_matmul_count += 1
        return Internal.matmul(mpc, x, y)

    def secure_pow(mpc, x_, p):
        if isinstance(x_, SharedTensor):
            return InternalMPC.pow(mpc, x_, p)
        elif isinstance(x_, CipherTensor):
            return InternalMHE.pow(mpc, x_, p)
        else: compile_error("Invalid secure operands")

    def secure_div(mpc, x, y):
        if isinstance(x, SharedTensor) or isinstance(y, SharedTensor):
            return InternalMPC.div(mpc, x, y)
        elif isinstance(x, CipherTensor) or isinstance(y, CipherTensor):
            return InternalMHE.div(mpc, x, y)
        else: compile_error("Invalid secure operands")
    
    def secure_gt(mpc, x, y):
        if isinstance(x, MPP) or isinstance(y, MPP):
            return InternalMPP.gt(mpc, x, y)
        elif isinstance(x, SharedTensor) or isinstance(y, SharedTensor):
            return InternalMPC.gt(mpc, x, y)
        elif isinstance(x, CipherTensor) or isinstance(y, CipherTensor):
            return InternalMHE.gt(mpc, x, y)
        else: compile_error("Invalid secure operands")

    def secure_lt(mpc, x, y):
        if isinstance(x, MPP) or isinstance(y, MPP):
            return InternalMPP.lt(mpc, x, y)
        elif isinstance(x, SharedTensor) or isinstance(y, SharedTensor):
            return InternalMPC.lt(mpc, x, y)
        elif isinstance(x, CipherTensor) or isinstance(y, CipherTensor):
            return InternalMHE.lt(mpc, x, y)
        else: compile_error("Invalid secure operands")
    
    def secure_eq(mpc, x, y):
        if isinstance(x, MPP) or isinstance(y, MPP):
            return InternalMPP.eq(mpc, x, y)
        elif isinstance(x, SharedTensor) or isinstance(y, SharedTensor):
            return InternalMPC.eq(mpc, x, y)
        elif isinstance(x, CipherTensor) or isinstance(y, CipherTensor):
            return InternalMHE.eq(mpc, x, y)
        else: compile_error("Invalid secure operands")

    def secure_sqrt_inv(mpc, x, y):
        assert isinstance(x, SharedTensor) or isinstance(y, SharedTensor), "Secure sqrt inverse computing is enabled only for MPC"
        return InternalMPC.sqrt_inv(mpc, x, y)
    
    def secure_evalp(mpc, x_, coefs_, exps_):
        return InternalMPC.evalp(mpc, x_, coefs_, exps_)

    def dot(mpc, x, y):
        if isinstance(x, SharedTensor) or isinstance(y, SharedTensor):
            return InternalMPC.dot(mpc, x, y)
        elif isinstance(x, CipherTensor) or isinstance(y, CipherTensor):
            return InternalMHE.dot(mpc, x, y)
        else: compile_error("Invalid secure operands")

    def dot(mpc, x):
        if isinstance(x, SharedTensor):
            return InternalMPC.dot(mpc, x)
        elif isinstance(x, CipherTensor):
            return InternalMHE.dot(mpc, x)
        else: compile_error("Invalid secure operands")

    def matmul(mpc, x, y):
        if isinstance(x, MPP) or isinstance(y, MPP):
            return InternalMPP.matmul(mpc, x, y)
        elif isinstance(x, SharedTensor) or isinstance(y, SharedTensor):
            return InternalMPC.matmul(mpc, x, y)
        elif isinstance(x, CipherTensor) or isinstance(y, CipherTensor):
            return InternalMHE.matmul(mpc, x, y)
        else: compile_error("Invalid secure operands")

    def sqrt(mpc, x):
        if isinstance(x, SharedTensor):
            return InternalMPC.sqrt(mpc, x)
        elif isinstance(x, CipherTensor):
            return InternalMHE.sqrt(mpc, x)
        else: compile_error("Invalid secure operands")

    def reveal_to_all(mpc, value):
        if isinstance(value, SharedTensor):
            return InternalMPC.reveal_to_all(mpc, value)
        elif isinstance(value, CipherTensor):
            return InternalMHE.reveal_to_all(mpc, value)
        else: compile_error("Invalid secure operands")

    def min_cost_router(new_vars, cost_vars):
        return new_vars[argmin(cost_vars)]
