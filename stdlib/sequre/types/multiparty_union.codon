import operator

from numpy.ndarray import ndarray
from numpy.create import zeros

from sequre.attributes import sequre
from sequre.lattiseq.ckks import Ciphertext
from sequre.types.ciphertensor import Ciphertensor
from sequre.types.multiparty_partition import MPP
from sequre.types.multiparty_aggregate import MPA


# Multiparty union
class MPU[S, dtype]:
    _mpp: MPP[S, dtype]
    _mpa: MPA[S, dtype]

    def __init__(self: MPU[S, dtype], mpc, plain: ndarray[S, dtype], collective_type: Static[str]):
        if collective_type == "partition":
            self._mpp = MPP[S, dtype](mpc, plain)
            self._mpa = MPA[S, dtype](mpc)
        elif collective_type == "aggregate":
            self._mpp = MPP[S, dtype](mpc)
            self._mpa = MPA[S, dtype](mpc, plain)
        else:
            compile_error("MPU: invalid collective type. Should be either partition or aggregate.")

    def __init__(self: MPU[S, dtype], mpc, encryption: Ciphertensor[Ciphertext], collective_type: Static[str]):
        if collective_type == "partition":
            self._mpp = MPP[S, dtype](mpc, encryption)
            self._mpa = MPA[S, dtype](mpc)
        elif collective_type == "aggregate":
            self._mpp = MPP[S, dtype](mpc)
            self._mpa = MPA[S, dtype](mpc, encryption)
        else:
            compile_error("MPU: invalid collective type. Should be either partition or aggregate.")

    def __len__(self) -> int:
        self.check_valid()

        if self.is_mpa():
            return len(self._mpa)
        return len(self._mpp)
    
    def __getitem__(self, index) -> MPU:
        self.check_valid()
        
        if self.is_mpa():
            item = self._mpa[index]
            return MPU[item.S, item.dtype](
                _mpp=MPP[item.S, item.dtype](item._mpc),
                _mpa=item)
        
        item = self._mpp[index]
        return MPU[item.S, item.dtype](
            _mpp=item,
            _mpa=MPA[item.S, item.dtype](item._mpc))
    
    def __setitem__(self, index, val: MPU):
        self.check_valid()
        
        if self.is_mpa():
            assert val.is_mpa(), "Not implemented yet: cannot set MPA value into MPP-typed MPU"
            self._mpa[index] = val._mpa
        else:
            assert val.is_mpp(), "Not implemented yet: cannot set MPP value into MPA-typed MPU"
            self._mpp[index] = val._mpp
    
    def __gt__(self, other) -> MPU[S, dtype]:
        if not isinstance(other, ByVal):
            compile_error("Not implemented yet: Comparison between MPUs")
        
        self.check_valid()
        
        if self.is_mpa():
            return MPU[S, dtype](
                _mpp=MPP[S, dtype](self._mpa._mpc),
                _mpa=self._mpa > other)
        
        return MPU[S, dtype](
            _mpp=self._mpp > other,
            _mpa=MPA[S, dtype](self._mpp._mpc))
    
    def __lt__(self, other) -> MPU[S, dtype]:
        if not isinstance(other, ByVal):
            compile_error("Not implemented yet: Comparison between MPUs")
        
        self.check_valid()
        
        if self.is_mpa():
            return MPU[S, dtype](
                _mpp=MPP[S, dtype](self._mpa._mpc),
                _mpa=self._mpa < other)
        
        return MPU[S, dtype](
            _mpp=self._mpp < other,
            _mpa=MPA[S, dtype](self._mpp._mpc))
    
    def __neg__(self) -> MPU[S, dtype]:
        self.check_valid()
        
        if self.is_mpa():
            return MPU[S, dtype](
                _mpp=MPP[S, dtype](self._mpa._mpc),
                _mpa=-self._mpa)
        
        return MPU[S, dtype](
            _mpp=-self._mpp,
            _mpa=MPA[S, dtype](self._mpp._mpc))
    
    def __add__(self, other) -> MPU[S, dtype]:
        self.check_valid()
        if isinstance(other, MPU):
            other.check_valid()
        return self._add(other)
    
    def __sub__(self, other) -> MPU[S, dtype]:
        self.check_valid()
        if isinstance(other, MPU):
            other.check_valid()
        return self._sub(other)
    
    def __mul__(self, other) -> MPU[S, dtype]:
        self.check_valid()
        if isinstance(other, MPU):
            other.check_valid()
        return self._mul(other)
    
    def __matmul__(self, other) -> MPU[S, dtype]:
        self.check_valid()
        if isinstance(other, MPU):
            other.check_valid()
        return self._matmul(other)
    
    def __truediv__(self, other) -> MPU[S, dtype]:
        self.check_valid()
        if isinstance(other, MPU):
            other.check_valid()
        return self._truediv(other)
    
    @property
    def ndim(self) -> int:
        return staticlen(S)

    @property
    def shape(self):
        self.check_valid()

        if self.is_mpa():
            return self._mpa.shape
        return self._mpp.cohort_shape

    @property
    def mpc(self):
        return self._mpp._mpc
    
    @property
    def T(self) -> MPU[S, dtype]:
        self.check_valid()
        
        if self.is_mpa():
            return MPU[S, dtype](
                _mpp=MPP[S, dtype](self._mpa._mpc),
                _mpa=self._mpa.T)
        
        return MPU[S, dtype](
            _mpp=self._mpp.T,
            _mpa=MPA[S, dtype](self._mpp._mpc))
    
    @property
    def I(self) -> MPU[S, dtype]:
        self.check_valid()
        
        if self.is_mpa():
            return MPU[S, dtype](
                _mpp=MPP[S, dtype](self._mpa._mpc),
                _mpa=self._mpa.I)
        
        return MPU[S, dtype](
            _mpp=self._mpp.I,
            _mpa=MPA[S, dtype](self._mpp._mpc))
    
    def copy(self) -> MPU[S, dtype]:
        self.check_valid()
        return MPU[S, dtype](
            _mpp=self._mpp.copy(),
            _mpa=self._mpa.copy())
    
    def is_empty(self) -> bool:
        self.check_valid()

        if self.is_mpa():
            return self._mpa.is_empty()
        return self._mpp.is_empty()
    
    def is_mpp(self) -> bool:
        return bool(self._mpp)
    
    def is_mpa(self) -> bool:
        return bool(self._mpa)
    
    def is_valid(self) -> bool:
        return self.is_mpp() ^ self.is_mpa()
    
    def check_valid(self):
        assert self.is_valid(), f"MPU: invalid MPU---should contain either MPP or MPA exclusively. MPP found: {self.is_mpp()}. MPA found: {self.is_mpa()}."
    
    def encrypt(self) -> MPU[S, dtype]:
        self.check_valid()
        
        if self.is_mpa():
            if self._mpa.is_plain():
                self._mpa.encrypt()
        else:
            self._mpp.encrypt()

        return self
    
    def reveal(self, *args) -> ndarray[S, dtype]:
        self.check_valid()
        
        if self.is_mpa():
            return self._mpa.reveal()
        return self._mpp.reveal()
    
    def reveal_local(self, *args) -> ndarray[S, dtype]:
        self.check_valid()
        
        if self.is_mpa():
            return self._mpa.reveal_local()
        return self._mpp.reveal_local()
    
    def expand_dims(self, axis: int = 0) -> MPU:
        self.check_valid()
        
        if self.is_mpa():
            expanded = self._mpa.expand_dims(axis)
            return MPU[expanded.S, dtype](
                _mpp=MPP[expanded.S, dtype](self._mpa._mpc),
                _mpa=expanded)
        
        expanded = self._mpp.expand_dims(axis)
        return MPU[expanded.S, dtype](
            _mpp=expanded,
            _mpa=MPA[expanded.S, dtype](self._mpp._mpc))
    
    def extend(self, other: MPU[S, dtype]) -> MPU[S, dtype]:
        self.check_valid()
        other.check_valid()
        
        if self.is_mpa() and other.is_mpa():
            self._mpa.extend(other._mpa)
            return self
        
        if self.is_mpp() and other.is_mpp():
            self._mpp.extend(other._mpp)
            return self
        
        raise NotImplementedError("MPU: extending MPA by MPP and vice-versa")

    def sum(self, axis: int) -> MPU[Tuple[int], dtype]:
        self.check_valid()
        
        if self.is_mpa():
            return MPU[Tuple[int], dtype](
                _mpp=MPP[Tuple[int], dtype](self._mpa._mpc),
                _mpa=self._mpa.sum(axis))
        
        return self._sum_mpp(axis)
    
    def dot(self, axis: int) -> MPU[Tuple[int], dtype]:
        self.check_valid()
        
        if self.is_mpa():
            return MPU[Tuple[int], dtype](
                _mpp=MPP[Tuple[int], dtype](self._mpa._mpc),
                _mpa=self._mpa.dot(axis))
        
        return MPU[Tuple[int], dtype](
            _mpp=self._mpp.dot(axis),
            _mpa=MPA[Tuple[int], dtype](self._mpp._mpc))
    
    def via_mpc(self, foo, *args) -> MPU[S, dtype]:
        self.check_valid()
        
        if self.is_mpa():
            return MPU[S, dtype](
                _mpp=MPP[S, dtype](self._mpa._mpc),
                _mpa=self._mpa.via_mpc(foo, *args))
        
        return MPU[S, dtype](
            _mpp=self._mpp.via_mpc(foo, *args),
            _mpa=MPA[S, dtype](self._mpp._mpc))

    def astype(self, t: type) -> MPU[S, t]:
        self.check_valid()
        
        if self.is_mpa():
            return MPU[S, t](
                _mpp=MPP[S, t](self._mpa._mpc),
                _mpa=self._mpa.astype(t))
        
        return MPU[S, t](
            _mpp=self._mpp.astype(t),
            _mpa=MPA[S, t](self._mpp._mpc))
    
    def to_fp(self) -> MPU[S, float]:
        self.check_valid()
        
        if self.is_mpa():
            return MPU[S, float](
                _mpp=MPP[S, float](self._mpa._mpc),
                _mpa=self._mpa.to_fp())
        
        return MPU[S, float](
            _mpp=self._mpp.to_fp(),
            _mpa=MPA[S, float](self._mpp._mpc))
    
    def zeros(self, *args) -> MPU[S, dtype]:
        self.check_valid()
        
        if self.is_mpa():
            return MPU[S, dtype](
                _mpp=MPP[S, dtype](self._mpa._mpc),
                _mpa=self._mpa.zeros(*args))
        
        return MPU[S, dtype](
            _mpp=self._mpp.zeros(*args),
            _mpa=MPA[S, dtype](self._mpp._mpc))
    
    def ones(self, *args) -> MPU[S, dtype]:
        self.check_valid()
        
        if self.is_mpa():
            return MPU[S, dtype](
                _mpp=MPP[S, dtype](self._mpa._mpc),
                _mpa=self._mpa.ones(*args))
        
        return MPU[S, dtype](
            _mpp=self._mpp.ones(*args),
            _mpa=MPA[S, dtype](self._mpp._mpc))
    
    def pad_with_value(self, val, size: int, axis: int, *args) -> MPU[S, dtype]:
        self.check_valid()
        
        if self.is_mpa():
            return MPU[S, dtype](
                _mpp=MPP[S, dtype](self._mpa._mpc),
                _mpa=self._mpa.pad_with_value(val, size, axis, *args))
        
        return MPU[S, dtype](
            _mpp=self._mpp.pad_with_value(val, size, axis, *args),
            _mpa=MPA[S, dtype](self._mpp._mpc))

    @staticmethod
    def rand(shape: S, mpc) -> MPU[S, dtype]:
        return MPU[S, dtype](
            _mpp=MPP[S, dtype](mpc),
            _mpa=MPA[S, dtype].rand(shape, mpc))

    def _sum_mpp(self, axis: int) -> MPU[Tuple[int], dtype]:
        assert self.is_mpp(), "MPU: cannot sum empty MPP"
        mpp = self._mpp
        mpc = mpp._mpc

        assert 0 <= axis < mpp.ndim, "MPU: axis out of range for sum"
        if mpp._transposed:
            axis ^= 1
        
        if axis == 1:
            return MPU[Tuple[int], dtype](
                _mpp=mpp.sum(axis),
                _mpa=MPA[Tuple[int], dtype](mpc))
        
        if mpc.pid == 0:
            return MPU[Tuple[int], dtype](
                _mpp=MPP[Tuple[int], dtype](mpc),
                _mpa=MPA[Tuple[int], dtype](mpc, mpp._local_data.sum_axis(axis)))

        _local_data = ndarray[Tuple[int], dtype]()
        _encryption_unified = Ciphertensor[Ciphertext]()
        
        if mpp.is_local():
            _local_data = mpp._local_data.sum_axis(axis)
        else:
            mpp.unify_encryption()
            _encryption_unified = mpp._encryption_unified.sum(mpc, axis)
        
        mpa = MPA[Tuple[int], dtype](
            _mpc=mpp._mpc,
            _plain=_local_data,
            _encryption=_encryption_unified,
            _aggregate=Ciphertensor[Ciphertext]())
        
        return MPU[Tuple[int], dtype](
                _mpp=MPP[Tuple[int], dtype](mpc),
                _mpa=mpa)

    def _add(self, other) -> MPU[S, dtype]:
        if isinstance(other, ndarray) or isinstance(other, ByVal):
            return self._public_op(other, operator.add)

        if self.is_mpa() and other.is_mpa():
            return MPU[S, dtype](
                _mpp=MPP[S, dtype](self._mpa._mpc),
                _mpa=self._mpa + other._mpa)
        
        if self.is_mpp() and other.is_mpp():
            return MPU[S, dtype](
                _mpp=self._mpp + other._mpp,
                _mpa=MPA[S, dtype](self._mpp._mpc))
        
        if self.is_mpa():
            return MPU[S, dtype](
                _mpp=MPP[S, dtype](self._mpa._mpc),
                _mpa=MPU[S, dtype]._add_mpa_mpp(self._mpa, other._mpp))
        
        # Case self is MPP and other is MPA
        if self.shape == other.shape:
            return MPU[S, dtype](
                _mpp=MPP[S, dtype](self._mpa._mpc),
                _mpa=MPU[S, dtype]._add_mpa_mpp(other._mpa, self._mpp))
        
        if staticlen(S) == 2:  # Broadcasting non-2-dimensional MPUs is not implemented yet
            # Broadcast cases
            if 1 == other.shape[0] < self.shape[0] and self.shape[1:] == other.shape[1:]:
                return MPU[S, dtype](
                    _mpp=self._mpp + MPU[S, dtype]._broadcast_mpa_to_mpp(other._mpa, self._mpp),
                    _mpa=MPA[S, dtype](self._mpp._mpc))

        raise ValueError(f"MPU: Cannot sum: shapes missmatch: {self.shape} != {other.shape}")
        
    def _sub(self, other) -> MPU[S, dtype]:
        if isinstance(other, ndarray) or isinstance(other, ByVal):
            return self._public_op(other, operator.sub)

        if self.is_mpa() and other.is_mpa():
            return MPU[S, dtype](
                _mpp=MPP[S, dtype](self._mpa._mpc),
                _mpa=self._mpa - other._mpa)
        
        if self.is_mpp() and other.is_mpp():
            return MPU[S, dtype](
                _mpp=self._mpp - other._mpp,
                _mpa=MPA[S, dtype](self._mpp._mpc))
        
        if self.is_mpa():
            return MPU[S, dtype](
                _mpp=MPP[S, dtype](self._mpa._mpc),
                _mpa=MPU[S, dtype]._sub_mpa_mpp(self._mpa, other._mpp))
        
        # Case self is MPP and other is MPA
        if self.shape == other.shape:
            return MPU[S, dtype](
                _mpp=MPP[S, dtype](self._mpa._mpc),
                _mpa=MPU[S, dtype]._add_mpa_mpp(-other._mpa, self._mpp))
        
        if staticlen(S) == 2:  # Broadcasting non-2-dimensional MPUs is not implemented yet
            # Broadcast cases
            if 1 == other.shape[0] < self.shape[0] and self.shape[1:] == other.shape[1:]:
                return MPU[S, dtype](
                    _mpp=self._mpp - MPU[S, dtype]._broadcast_mpa_to_mpp(other._mpa, self._mpp),
                    _mpa=MPA[S, dtype](self._mpp._mpc))

        raise ValueError(f"MPU: Cannot sum: shapes missmatch: {self.shape} != {other.shape}")
    
    def _mul(self, other) -> MPU[S, dtype]:
        if isinstance(other, ndarray) or isinstance(other, ByVal):
            return self._public_op(other, operator.mul)

        if self.is_mpa() and other.is_mpa():
            return MPU[S, dtype](
                _mpp=MPP[S, dtype](self._mpa._mpc),
                _mpa=self._mpa * other._mpa)
        
        if self.is_mpp() and other.is_mpp():
            return MPU[S, dtype](
                _mpp=self._mpp * other._mpp,
                _mpa=MPA[S, dtype](self._mpp._mpc))
        
        if self.is_mpa():
            return MPU[S, dtype](
                _mpp=MPP[S, dtype](self._mpa._mpc),
                _mpa=MPU[S, dtype]._mul_mpa_mpp(self._mpa, other._mpp))
        
        return MPU[S, dtype](
            _mpp=MPP[S, dtype](self._mpa._mpc),
            _mpa=MPU[S, dtype]._mul_mpa_mpp(other._mpa, self._mpp))

    def _matmul(self, other) -> MPU[S, dtype]:
        if isinstance(other, ndarray):
            return self._matmul_public(other)

        if self.is_mpa() and other.is_mpa():
            return MPU[S, dtype](
                _mpp=MPP[S, dtype](self._mpa._mpc),
                _mpa=self._mpa @ other._mpa)
        
        if self.is_mpp() and other.is_mpp():
            if self._mpp._transposed:
                return MPU[S, dtype](
                    _mpp=MPP[S, dtype](self._mpp._mpc),
                    _mpa=MPU[S, dtype]._matmul_mpp_t_mpp(self._mpp._mpc, self._mpp, other._mpp))

            return MPU[S, dtype](
                _mpp=self._mpp @ other._mpp,
                _mpa=MPA[S, dtype](self._mpp._mpc))
        
        if self.is_mpa():
            return MPU[S, dtype]._matmul_mpa_mpp(self._mpa._mpc, self._mpa, other._mpp)
        
        return MPU[S, dtype]._matmul_mpp_mpa(self._mpp._mpc, self._mpp, other._mpa)
    
    def _truediv(self, other) -> MPU[S, dtype]:
        if isinstance(other, ByVal):
            return self._public_op(other, operator.truediv)
        raise NotImplementedError("MPUs cannot be divided by secure value yet")
    
    def _public_op(self, other, op) -> MPU[S, dtype]:
        if self.is_mpa():
            return MPU[S, dtype](
                _mpp=MPP[S, dtype](self._mpa._mpc),
                _mpa=op(self._mpa, other))
        
        return MPU[S, dtype](
            _mpp=op(self._mpp, other),
            _mpa=MPA[S, dtype](self._mpp._mpc))
    
    def _matmul_public(self, other: ndarray[S, dtype]) -> MPU[S, dtype]:
        mpc = self.mpc
        if self.is_mpa():
            return MPU[S, dtype](
                _mpp=MPP[S, dtype](self._mpa._mpc),
                _mpa=self._mpa @ other)
        
        if self._mpp._transposed:
            start, stop = self._mpp.get_relative_indices()
            return MPU[S, dtype](
                _mpp=MPP[S, dtype](self._mpp._mpc),
                _mpa=MPU[S, dtype]._matmul_mpp_t_mpp(mpc, self._mpp, MPP[S, dtype](mpc, other[start:stop], self._mpp._ratios)))

        return MPU[S, dtype](
            _mpp=self._mpp @ other,
            _mpa=MPA[S, dtype](self._mpp._mpc))
    
    def _add_mpa_mpp(mpa: MPA[S, dtype], mpp: MPP[S, dtype]) -> MPA[S, dtype]:
        mpc = mpa._mpc
        if mpc.pid == 0:
            return mpa.copy()
        
        transposed = mpp._transposed
        new_mpa = mpa.T if transposed else mpa.copy()
        
        # Case where MPP partition is present only at a single party
        if mpc.council(mpa.shape == mpp.shape).any():
            is_non_empty = mpa.shape == mpp.shape
            if mpp.has_local_data():
                new_mpa += MPA[S, dtype](mpc, mpp._local_data if is_non_empty else zeros(mpa.shape, dtype=dtype))
            else:
                mpp.unify_encryption()
                new_mpa += MPA[S, dtype](mpc, mpp._encryption_unified if is_non_empty else Ciphertensor.zeros(mpc, list(mpa.shape)))
            return new_mpa

        if mpa.ndim == 1 and not transposed:
            assert mpp.ndim == 1, f"MPA: cannot add MPs of different dimensionality: MPA dim {mpa.ndim} != MPP dim {mpp.ndim}"
            if mpp.has_local_data():
                new_mpa += MPA[S, dtype](mpc, mpp._local_data)
            elif not mpp.is_empty():
                mpp.unify_encryption()
                new_mpa += MPA[S, dtype](mpc, mpp._encryption_unified)
            return new_mpa

        start, end = mpp.get_relative_indices()        
        if end - start:
            if mpp.has_local_data():
                new_mpa[start:end] += MPA[S, dtype](mpc, mpp._local_data)
            else:
                mpp.unify_encryption()
                new_mpa[start:end] += MPA[S, dtype](mpc, mpp._encryption_unified)
        
        return new_mpa.T if transposed else new_mpa
    
    def _sub_mpa_mpp(mpa: MPA[S, dtype], mpp: MPP[S, dtype]) -> MPA[S, dtype]:
        mpc = mpa._mpc
        if mpc.pid == 0:
            return mpa.copy()
        
        transposed = mpp._transposed
        new_mpa = mpa.T if transposed else mpa.copy()

        if mpa.ndim == 1 and not transposed:
            assert mpp.ndim == 1, f"MPA: cannot add MPs of different dimensionality: MPA dim {mpa.ndim} != MPP dim {mpp.ndim}"
            if mpp.has_local_data():
                new_mpa -= MPA[S, dtype](mpc, mpp._local_data)
            elif not mpp.is_empty():
                mpp.unify_encryption()
                new_mpa -= MPA[S, dtype](mpc, mpp._encryption_unified)
            return new_mpa

        start, end = mpp.get_relative_indices()
        if end - start:
            if mpp.has_local_data():
                new_mpa[start:end] -= MPA[S, dtype](mpc, mpp._local_data)
            else:
                mpp.unify_encryption()
                new_mpa[start:end] -= MPA[S, dtype](mpc, mpp._encryption_unified)
        
        return new_mpa.T if transposed else new_mpa
    
    def _mul_mpa_mpp(mpa: MPA[S, dtype], mpp: MPP[S, dtype]) -> MPA[S, dtype]:
        if mpa._mpc.pid == 0:
            return mpa.copy()
        
        # TODO: dtype of Ciphertensor is hardcoded to float at the moment.
        # Thus mpa_float will be autocasted to MPA[S, float] below since mpp.join() returns a Ciphertensor.
        # Manual casting to dtype is required afterwards until a generic dtype is added to Ciphertensor.
        if mpp._transposed:
            mpa_float = (mpa.T * mpp.join()).T
        else:
            mpa_float = mpa * mpp.join()
        
        return MPA[S, dtype](
            _mpc=mpa._mpc,
            _plain=mpa_float._plain.astype(dtype),
            _encryption=mpa_float._encryption,
            _aggregate=mpa_float._aggregate)
    
    def _matmul_mpp_mpa(mpc, mpp: MPP[S, dtype], mpa: MPA[S, dtype]) -> MPU[S, dtype]:
        if mpp._transposed:
            return MPU[S, dtype](
                _mpp=MPP[S, dtype](mpc),
                _mpa=MPU[S, dtype]._matmul_mpp_t_mpa(mpc, mpp, mpa))
        
        return MPU[S, dtype](
            _mpp=MPU[S, dtype]._matmul_mpp_nt_mpa(mpc, mpp, mpa),
            _mpa=MPA[S, dtype](mpc))
    
    def _matmul_mpa_mpp(mpc, mpa: MPA[S, dtype], mpp: MPP[S, dtype]) -> MPU[S, dtype]:
        if mpp._transposed:
            return MPU[S, dtype](
                _mpp=MPU[S, dtype]._matmul_mpa_mpp_t(mpc, mpa, mpp),
                _mpa=MPA[S, dtype](mpc))
        
        return MPU[S, dtype]._matmul_mpp_mpa(mpc, mpp.T, mpa.T).T
    
    @sequre
    def _matmul_mpa_mpp_t(mpc, mpa: MPA[S, dtype], mpp: MPP[S, dtype]) -> MPP[S, dtype]:
        assert mpp._transposed, "MPU: MPP expected to be tranposed in transposed x MPA matmul case"
        
        if mpc.pid == 0:
            return MPP[S, dtype](
                _mpc=mpp._mpc,
                _ratios=mpp._ratios.copy(),
                _local_data=zeros((mpp._local_data.shape[0], mpa.shape[0]), dtype=dtype),
                _transposed=True)
        
        mpa.aggregate()
        _encryption = Ciphertensor[Ciphertext]()
        with mpp.partition_aligner() as aligner:
            if not mpp.is_empty():
                if mpp.is_local():
                    _encryption = mpa._aggregate @ mpp._local_data.T
                else:
                    mpp.unify_encryption()
                    _encryption = mpa._aggregate @ mpp._encryption_unified.T
            
            _encryption._transposed ^= True
            return aligner.new_mpp(
                _encryption_unified=_encryption)
    
    @sequre
    def _matmul_mpp_t_mpa(mpc, mpp: MPP[S, dtype], mpa: MPA[S, dtype]) -> MPA[S, dtype]:
        assert mpp._transposed, "MPU: MPP expected to be tranposed in transposed x MPA matmul case"
        
        if mpc.pid == 0:
            return MPA[S, dtype](mpc, zeros((mpp._local_data.shape[1], mpa._plain.shape[1]), dtype=dtype))
        
        mpa.aggregate()
        start, stop = mpp.get_relative_indices()
        other = MPP[S, dtype](mpc, mpa._aggregate[start:stop], mpp._ratios)
        
        _encryption = Ciphertensor[Ciphertext]()
        with mpp.partition_aligner(), other.partition_aligner():
            if not mpp.is_empty():
                if mpp.is_local():
                    _encryption = mpp._local_data.T @ other._encryption_unified
                else:
                    mpp.unify_encryption()
                    _encryption = mpp._encryption_unified.T @ other._encryption_unified
            
            return MPA[S, dtype](
                _mpc=mpc,
                _encryption=_encryption)
    
    @sequre
    def _matmul_mpp_nt_mpa(mpc, mpp: MPP[S, dtype], mpa: MPA[S, dtype]) -> MPP[S, dtype]:
        assert not mpp._transposed, "MPU: MPP expected not to be tranposed in transposed x MPA matmul case"
        
        if mpc.pid == 0:
            return MPP[S, dtype](
                _mpc=mpp._mpc,
                _ratios=mpp._ratios.copy(),
                _local_data=zeros((mpp._local_data.shape[0], mpa.shape[1]), dtype=dtype))
        
        mpa.aggregate()

        _encryption = Ciphertensor[Ciphertext]()
        with mpp.partition_aligner() as aligner:
            if not mpp.is_empty():
                if mpp.is_local():
                    _encryption = mpp._local_data @ mpa._aggregate
                else:
                    mpp.unify_encryption()
                    _encryption = mpp._encryption_unified @ mpa._aggregate
            
            return aligner.new_mpp(
                _encryption_unified=_encryption)

    @sequre
    def _matmul_mpp_t_mpp(mpc, first: MPP[S, dtype], other: MPP[S, dtype]) -> MPA[S, dtype]:
        if mpc.pid == 0:
            return MPA[S, dtype](
                _mpc=first._mpc,
                _plain=zeros((first._local_data.shape[1], other._local_data.shape[1]), dtype=dtype))

        assert not (first.is_empty() ^ other.is_empty()), "MPU: all or none of the partitions should be empty"

        plain = ndarray[S, dtype]()
        enc = Ciphertensor[Ciphertext]()
        
        with first.partition_aligner(), other.partition_aligner():
            assert first.shape[1] == other.shape[0], f"MPU: Invalid shapes for per-partition matmul. {first.shape} x {other.shape}"

            if not first.is_empty():
                if not first.is_local():
                    first.unify_encryption()
                if not other.is_local():
                    other.unify_encryption()
                
                if first.is_local() and other.is_local():
                    plain = first._local_data.T @ other._local_data
                elif first.is_local():
                    enc = first._local_data.T @ other._encryption_unified
                elif other.is_local():
                    enc = first._encryption_unified.T @ other._local_data
                else:
                    enc = first._encryption_unified.T @ other._encryption_unified

        return MPA[S, dtype](
            _mpc=first._mpc,
            _plain=plain,
            _encryption=enc)
    
    @staticmethod
    def _broadcast_mpa_to_mpp(mpa: MPA[S, dtype], mpp: MPP[S, dtype]) -> MPP[S, dtype]:
        assert not mpp._transposed, "Not implemented yet: broadcasting to a transposed MPP"
        assert mpa.ndim == mpp.ndim == 2, "Not implemented yet: broadcasting non-2-dimensional MPAs to MPPs"
        assert mpa.shape[1] == mpp.shape[1], f"MPU: cannot broadcast mpa of shape {mpa.shape} to MPP of shape {mpp.shape}"
        assert mpa.shape[0] == 1, "MPU: can broadcast only a single-row MPA to MPU"

        mpc = mpa._mpc
        if mpc.pid == 0:
            return mpp
        
        mpa.aggregate()
        
        _encryption_unified = Ciphertensor[Ciphertext](
            shape=[0, mpa._aggregate.shape[1]],
            slots=mpa._aggregate.slots)

        for _ in range(mpp._ratios[mpc.pid - 1]):
            _encryption_unified.extend(mpc, mpa._aggregate.copy())
        
        return MPP[S, dtype](
            _mpc=mpc,
            _ratios=mpp._ratios.copy(),
            _encryption_unified=_encryption_unified,
            _transposed=mpp._transposed)
    
    # Internal typechecker hack
    @property
    def _internal_type(self) -> dtype:
        return dtype()
    
    # Temporaty helpers
    def slice_local(self, i: int, *args) -> MPU[S, dtype]:
        self.check_valid()
        if self.is_mpa():
            raise NotImplementedError()
        
        return MPU[S, dtype](
            _mpp=self._mpp.slice_local(i, *args),
            _mpa=MPA[S, dtype](self._mpp._mpc))
    
    def rotate_local(self, i: int, *args) -> MPU[S, dtype]:
        self.check_valid()
        if self.is_mpa():
            raise NotImplementedError()
        
        return MPU[S, dtype](
            _mpp=self._mpp.rotate_local(i, *args),
            _mpa=MPA[S, dtype](self._mpp._mpc))


@extend
class ndarray:
    def __add__(self, other: MPU[S, T]) -> MPU[S, T]:
        return other + self
    
    def __sub__(self, other: MPU[S, T]) -> MPU[S, T]:
        return -other + self
    
    def __mul__(self, other: MPU[S, T]) -> MPU[S, T]:
        return other * self
    
    def __matmul__(self, other: MPU[S, T]) -> MPU[S, T]:
        return (other.T @ self.T).T


@extend
class int:
    def __add__[S, T](self, other: MPU[S, T]) -> MPU[S, T]:
        return other + self
    
    def __sub__[S, T](self, other: MPU[S, T]) -> MPU[S, T]:
        return -other + self
    
    def __mul__[S, T](self, other: MPU[S, T]) -> MPU[S, T]:
        return other * self
