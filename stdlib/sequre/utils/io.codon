from numpy.ndarray import ndarray
from internal.gc import sizeof
from param import int_t

from ..types.shared_tensor import SharedTensor


def write_vector[TP](f: File, vector: list, binary: bool):
    if binary:
        _C.fwrite(vector.arr.ptr.as_byte(), sizeof(TP), vector.size(), f.fp)
        f._errcheck("error in write")
    else:
        s = ''
        for i in range(len(vector) - 1):
            s += f"{vector[i]} "
        s += f"{vector[-1]}\n"
        f.write(s)


def write_matrix[TP](f: File, mat: list[list], binary: bool):
    for row in mat: write_vector(f, row, binary, TP=TP)


def write_ndarray(f: File, arr: ndarray, binary: bool):
    if binary:
        _C.fwrite(arr.data.as_byte(), arr.itemsize, arr.size(), f.fp)
        f._errcheck("error in write")
    else:
        if not arr.ndim:
            return
        
        if staticlen(arr.S) == 1:
            write_vector(f, arr.tolist(), False, TP=arr.T)
        else:
            for sub_tensor in arr:
                write_ndarray(f, sub_tensor, False)
                f.write('\n')


def read_vector[TP](f: File, length: int, binary: bool) -> list[TP]:
    if binary:
        arr = Array[TP](length)
        for i in range(length): arr[i] = ptr[TP](f.read(sizeof(TP)).ptr.as_byte())[0]
        return list[TP](arr, length)
    return [TP(e.strip()) for e in next(f).split()[:length]]


def read_matrix[TP](f: File, rows: int, cols: int, binary: bool) -> list[list[TP]]:
    return [read_vector(f, cols, binary, TP=TP) for _ in range(rows)]


def read_ndarray[S, dtype](f: File, shape: S, binary: bool) -> ndarray[S, dtype]:
    if binary:
        return ndarray[S, dtype]._new_contig(
            shape, ptr[dtype](f.read(ndarray._count(shape) * sizeof(dtype)).ptr.as_byte()))
    
    raise NotImplementedError("Reading a ndarray from non-binary file is not implemented yet")


def read_matrix_start[TP](f: File, rows: int, cols: int, start: int) -> list[list[TP]]:
    matrix = []
    for i in range(start + rows):
        if i >= start:
            matrix.append(read_vector(f, cols, False, TP=TP))
    return matrix


def read_beaver_vector(mpc, f, f_mask, length, modulus, binary: bool):
    x_r = read_vector(f, length, binary, TP=type(modulus))
    r = read_vector(f_mask, length, binary, TP=type(modulus))
    share = x_r.add_mod(r, modulus) if mpc.pid == 2 else r

    return SharedTensor(
        share, x_r, r, modulus, x_r.zeros(),
        x_r.zeros(), False, False, False)


def read_beaver_matrix(mpc, f, f_mask, rows, cols, modulus, binary: bool):
    x_r = read_matrix(f, rows, cols, binary, TP=type(modulus))
    r = read_matrix(f_mask, rows, cols, binary, TP=type(modulus))
    share = x_r.add_mod(r, modulus) if mpc.pid == 2 else r

    return SharedTensor(
        share, x_r, r, modulus, x_r.zeros(),
        x_r.zeros(), False, False, False)


def read_filtered_matrix(mpc, f, f_mask, imask, jmask, rows, cols, offset, multiplicity, modulus, binary):
    share_mats = [list[list[int_t]](rows * multiplicity) for _ in range(multiplicity)]
    x_r_mats = [list[list[int_t]](rows * multiplicity) for _ in range(multiplicity)]
    r_mats = [list[list[int_t]](rows * multiplicity) for _ in range(multiplicity)]
    
    batch_size = 0
    i = offset
    while batch_size < rows:
        if imask[i]: batch_size += 1
        for m in range(multiplicity):
            x_r = read_vector(f, cols, binary, TP=int_t)
            r = read_vector(f_mask, cols, binary, TP=int_t)
            if imask[i]:
                x_r = [x_r[j] for j in range(len(jmask)) if jmask[j]]
                r = [r[j] for j in range(len(jmask)) if jmask[j]]
                r_mats[m].append(r)
                x_r_mats[m].append(x_r)
                share_mats[m].append(x_r.add_mod(r, modulus) if mpc.pid == 2 else r)
        i += 1
    
    svs = [SharedTensor(
        share_mats[i], x_r_mats[i], r_mats[i], modulus, share_mats[i].zeros(),
        share_mats[i].zeros(), False, False, False) for i in range(multiplicity)]

    return svs


def reset_files(*files):
    for file in files:
        file.seek(0, 0)


def log(name, data, path='log.txt', mode='a+', separator='\t'):
    with open(path, mode) as f:
        f.write(f'{name}\n')
        if isinstance(data, list[list]):
            for row in data:
                f.write(separator.join([str(e) for e in row]))
                f.write('\n')
        elif isinstance(data, list):
            f.write(separator.join([str(e) for e in data]))
            f.write('\n')
        else:
            f.write(f'{data}\n')
