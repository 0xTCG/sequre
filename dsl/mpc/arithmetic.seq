from ..utils.param import *
from ..utils.utils import shapeof

from prg import MPCPRG
from comms import MPCComms


class MPCArithmetic[TP]:
    pid: int
    partitions: int
    reconstructs: int
    prg: MPCPRG
    comms: MPCComms[TP]
    
    def __init__(self, pid: int, prg: MPCPRG, comms: MPCComms[TP]):
        self.pid = pid
        self.prg = prg
        self.comms = comms

        self.reset_stats()
    
    def print_stats(self, file_stream = None):
        partitioning_message = f"Partitions: {self.partitions} at CP{self.pid}.\nReconstructs: {self.reconstructs} at CP{self.pid}."
        if file_stream is not None: file_stream.write(f'{partitioning_message}\n')
        print(partitioning_message)
    
    def reset_stats(self):
        self.partitions = 0
        self.reconstructs = 0
    
    def ring_to_field(self, target):
        return self.__switch_modulus(target, RING_SIZE, FIELD_SIZE)
    
    def field_to_ring(self, target):
        return self.__switch_modulus(target, FIELD_SIZE, RING_SIZE)

    def add_public(self, x, a, modulus):
        if self.pid == 1:
            return x.add_mod(a, modulus)
        return x
    
    def multiply(self, a, b, modulus):
        x_1_r, r_1 = self.__beaver_partition(a, modulus)
        x_2_r, r_2 = self.__beaver_partition(b, modulus)
        
        c = self.__beaver_mult(x_1_r, r_1, x_2_r, r_2, modulus)
        c = self.__beaver_reconstruct(c, modulus)
        
        return c

    def multiply_matmul(self, a, b, modulus):
        x_1_r, r_1 = self.__beaver_partition(a, modulus)
        x_2_r, r_2 = self.__beaver_partition(b, modulus)

        c = self.__beaver_matmul(x_1_r, r_1, x_2_r, r_2, modulus)
        c = self.__beaver_reconstruct(c, modulus)
        
        return c
    
    def beaver_inner_prod(self, ar, am, modulus):
        # TODO: #55 This method might be redundant with __beaver_dot_prod and not needed
        ab = am.mul_mod(am, modulus) if self.pid == 0 else ar.mul_mod(am, modulus).lsh_mod(1, modulus)
        if self.pid == 1:
            ab = ab.add_mod(ar.mul_mod(ar))

        cum_sum = type(modulus)(0)
        for e in ab: cum_sum = cum_sum.add_mod(e, modulus)

        return cum_sum
    
    def inner_prod(self, a, modulus):
        ar, am = self.__beaver_partition(a, modulus)
        n = len(a)

        c = list[type(modulus)](n)
        for i in range(n): c.append(self.beaver_inner_prod(ar[i], am[i], modulus))

        return self.__beaver_reconstruct(c, modulus)
    
    def __beaver_partition_mat_bulk(self, x, modulus):
        # TODO: #55 Deprecate this method
        # TODO: #55 Do this in parallel
        partitions = [self.__beaver_partition(e, modulus) for e in x]
        x_r = [p[0] for p in partitions]
        r = [p[1] for p in partitions]
        return x_r, r
    
    def __beaver_reconstruct_mat_bulk(self, x, modulus):
        # TODO: #55 Deprecate this method
        # TODO: #55 Do this in parallel
        return [self.__beaver_reconstruct(e, modulus) for e in x]

    def multiply_bulk(self, a, b, modulus):
        # TODO: #55 Deprecate this method
        # TODO: #16 Vectorize this method. Make it parallel by having a and b as ndarrays.
        nmat = len(a)

        ar, am = self.__beaver_partition_mat_bulk(a, modulus)
        br, bm = self.__beaver_partition_mat_bulk(b, modulus)

        c = [self.__beaver_mult(ar[k], am[k], br[k], bm[k], modulus)
             for k in range(nmat)]
        
        return self.__beaver_reconstruct_mat_bulk(c, modulus)
    
    def multiply_mat_bulk(self, a, b, modulus):
        # TODO: #55 Deprecate this method
        # TODO: #16 Vectorize/parallelize this method. Make it parallel by having a and b as ndarrays.
        nmat = len(a)

        ar, am = self.__beaver_partition_mat_bulk(a, modulus)
        br, bm = self.__beaver_partition_mat_bulk(b, modulus)

        c = [self.__beaver_matmul(ar[k], am[k], br[k], bm[k], modulus)
             for k in range(nmat)]
            
        return self.__beaver_reconstruct_mat_bulk(c, modulus)
    
    def beaver_inner_prod_pair(self, ar, am, br, bm, modulus):
        # TODO: #55 Deprecate this method
        ab = type(modulus)(0)
        
        for i in range(len(ar)):
            if self.pid == 0:
                ab = am[i].mul_mod(bm[i], modulus).mul_add(ab, modulus)
            else:
                ab = ar[i].mul_mod(bm[i], modulus).mul_add(ab, modulus)
                ab = br[i].mul_mod(am[i], modulus).mul_add(ab, modulus)
                if self.pid == 1: ab = ar[i].mul_mod(br[i], modulus).mul_add(ab, modulus)

        return ab
    
    def validate_partitions(self, x, x_r, r, modulus, message = ""):
        if self.pid and DEBUG:
            partition_share = (x_r if self.pid == 1 else x_r.zeros()).add_mod(r, modulus)
            partition_reveal = self.comms.reveal(partition_share, modulus)
            share_reveal = self.comms.reveal(x, modulus)
            assert partition_reveal == share_reveal, f"{message}:\n\tPartition reveal: {partition_reveal}\n\tShare reveal: {share_reveal}"
    
    def __beaver_partition(self, value, modulus):
        self.partitions += 1

        if self.pid == 0:
            self.prg.switch_seed(1)
            r_1 = value.rand(modulus)
            self.prg.restore_seed(1)

            self.prg.switch_seed(2)
            r_2 = value.rand(modulus)
            self.prg.restore_seed(2)

            r = r_1.add_mod(r_2, modulus)
            return value.zeros(), r
        else:
            self.prg.switch_seed(0)
            r = value.rand(modulus)
            self.prg.restore_seed(0)
            
            x_r = value.sub_mod(r, modulus)
            x_r = self.comms.reveal(x_r, modulus)

            return x_r, r

    def __beaver_reconstruct(self, value, modulus):
        self.reconstructs += 1

        if self.pid == 0:
            self.prg.switch_seed(1)
            mask = value.rand(modulus)
            self.prg.restore_seed(1)

            mm = value.sub_mod(mask, modulus)
            self.comms.send(mm, 2)
            
            return mm
        elif self.pid == 1:
            self.prg.switch_seed(0)
            rr = value.rand(modulus)
            self.prg.restore_seed(0)
            
            return value.add_mod(rr, modulus)
        else:
            rr = self.comms.receive(0, shapeof(value), T=type(modulus))
                
            return value.add_mod(rr, modulus)
    
    def __beaver_mult(self, x_r, r_1, y_r, r_2, modulus):
        if self.pid == 0:
            return r_1.mul_mod(r_2, modulus)

        xy = x_r.mul_mod(r_2, modulus)
        xy = r_1.mul_mod(y_r, modulus).add_mod(xy, modulus)
        
        if self.pid == 1: xy = x_r.mul_mod(y_r, modulus).add_mod(xy, modulus)

        return xy
    
    def __beaver_matmul(
            self, x_r, r_1, y_r, r_2, modulus):
        if self.pid == 0:
            return r_1.matmul_mod(r_2, modulus)

        xy = x_r.matmul_mod(r_2, modulus)
        xy = xy.add_mod(r_1.matmul_mod(y_r, modulus), modulus)
        if self.pid == 1:
            xy = xy.add_mod(x_r.matmul_mod(y_r, modulus), modulus)

        return xy
    
    def __beaver_dot_prod(self, x_r, r_1, y_r, r_2, modulus):
        xy = self.__beaver_mult(x_r, r_1, y_r, r_2, modulus)
        
        cum_sum = xy[0].zeros()
        for e in xy: cum_sum = cum_sum.add_mod(e, modulus)

        return cum_sum
    
    def __switch_modulus(self, target, from_mod, to_mod):
        return self.add_public(target, to_mod.sub_mod(from_mod, to_mod), to_mod)
