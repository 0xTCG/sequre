from ..utils.param import *
from ..utils.utils import zeros, zeros_vec
from utils import double_to_fp


class SharedTensor[TP]:
    share: TP
    x_r: TP
    r: TP
    modulus: int_t
    sqrt: TP
    sqrt_inv: TP
    pows: list[TP]
    fp: bool
    public: bool
    diagonal: bool

    def __init__(self: SharedTensor[TP], other: int, modulus: int_t):
        self.share = TP(other)
        self.x_r = TP(0)
        self.r = TP(0)
        self.modulus = modulus
        self.sqrt = TP(0)
        self.sqrt_inv = TP(0)
        self.pows = []
        self.fp = False
        self.public = False
        self.diagonal = False
    
    def __init__(self: SharedTensor[TP], share: TP, x_r: TP, r: TP, modulus: int_t):
        self.share = share
        self.x_r = x_r
        self.r = r
        self.modulus = modulus
        self.sqrt = TP(0)
        self.sqrt_inv = TP(0)
        self.pows = []
        self.fp = False
        self.public = False
        self.diagonal = False
    
    def __init__(self: SharedTensor[TP],
                 share: TP, x_r: TP, r: TP,
                 modulus: int_t, sqrt: TP,
                 sqrt_inv: TP, pows: list[TP],
                 fp: bool, public: bool, diagonal: bool):
        self.share = share
        self.x_r = x_r
        self.r = r
        self.modulus = modulus
        self.sqrt = sqrt
        self.sqrt_inv = sqrt_inv
        self.pows = pows
        self.fp = fp
        self.public = public
        self.diagonal = diagonal

    def __init__(self: SharedTensor[TP], other: TP, modulus: int_t):
        self.share = other
        self.x_r = TP(0)
        self.r = TP(0)
        self.modulus = modulus
        self.sqrt = TP(0)
        self.sqrt_inv = TP(0)
        self.pows = []
        self.fp = False
        self.public = False
        self.diagonal = False

    def __init__(self: SharedTensor[TP], other: SharedTensor[TP]):
        self.share = other.share.copy()
        self.x_r = other.x_r.copy()
        self.r = other.r.copy()
        self.modulus = other.modulus
        self.sqrt = other.sqrt.copy()
        self.sqrt_inv = other.sqrt_inv.copy()
        self.pows = other.pows.copy()
        self.fp = other.fp
        self.public = other.public
        self.diagonal = other.diagonal
    
    def __iter__(self: SharedTensor[TP]):
        for i in range(len(self)): yield self[i]
    
    def __getitem__(self: SharedTensor[TP], index):
        item_share = self.share[index]
        return SharedTensor[typeof(item_share)](
            item_share,
            self.x_r[index] if self.x_r else typeof(item_share)(0),
            self.r[index] if self.r else typeof(item_share)(0),
            self.modulus,
            self.sqrt[index] if self.sqrt else typeof(item_share)(0),
            self.sqrt_inv[index] if self.sqrt_inv else typeof(item_share)(0),
            [p[index] for p in self.pows],
            self.fp,
            self.public,
            False)
    
    def __setitem__(self: SharedTensor[TP], index, other):
        assert self.public == other.public
        if isinstance(index, tuple[int, int]):
            ri, ci = index
            self.diagonal = self.diagonal and (ri == ci)
        else:
            self.diagonal = False
        self.fp = other.fp
        self.modulus = other.modulus
        
        self.share[index] = other.share
        if not other.x_r: self.x_r = TP(0)
        elif self.x_r: self.x_r[index] = other.x_r
        
        if not other.r: self.r = TP(0)
        elif self.r:self.r[index] = other.r

        if not other.sqrt: self.sqrt = TP(0)
        elif self.sqrt: self.sqrt[index] = other.sqrt
        
        if not other.sqrt_inv: self.sqrt_inv = TP(0)
        elif self.sqrt_inv: self.sqrt_inv[index] = other.sqrt_inv
        
        if not other.pows: self.pows = list[TP](0)
        elif self.pows:
            for i in range(min(len(self.pows), len(other.pows))):
                self.pows[i][index] = other.pows[i]
    
    def __bool__(self: SharedTensor[TP]) -> bool:
        return bool(self.share)
    
    def __int__(self: SharedTensor[TP]) -> int:
        return int(self.share)
    
    def __neg__(self: SharedTensor[TP]) -> SharedTensor[TP]:
        return SharedTensor[TP](
            share = -self.share,
            x_r = -self.x_r,
            r = -self.r,
            modulus = self.modulus,
            sqrt = -self.sqrt,
            sqrt_inv = -self.sqrt_inv,
            pows = -self.pows,
            fp = self.fp,
            public = self.public,
            diagonal = self.diagonal)

    def __iadd__(self: SharedTensor[TP], other: SharedTensor[TP]) -> SharedTensor[TP]:
        if isinstance(TP, list[list[int_t]]):
            if other.diagonal and not NO_OPT:
                for i in range(len(self)):
                    self.share[i][i] += other.share[i][i]
            else: self.share += other.share
        else: self.share += other.share

        if self.x_r and other.x_r:
            if isinstance(TP, list[list[int_t]]):
                if other.diagonal and not NO_OPT:
                    for i in range(len(self)):
                        self.x_r[i][i] += other.x_r[i][i]
                else: self.x_r += other.x_r
            else: self.x_r += other.x_r
        else: self.x_r = TP(0)

        if self.r and other.r:
            if isinstance(TP, list[list[int_t]]):
                if other.diagonal and not NO_OPT:
                    for i in range(len(self)):
                        self.r[i][i] += other.r[i][i]
                else: self.r += other.r
            else: self.r += other.r
        else: self.r = TP(0)

        self.pows = []
        self.sqrt = TP(0)
        self.sqrt_inv = TP(0)
        
        self.share %= self.modulus
        self.x_r %= self.modulus
        self.r %= self.modulus

        return self
    
    def __iadd__(self: SharedTensor[TP], other: TP) -> SharedTensor[TP]:
        self.share += other
        if self.is_partitioned():
            self.x_r += other
            self.r += other
        self.pows = []
        self.sqrt = TP(0)
        self.sqrt_inv = TP(0)
        
        self.share %= self.modulus
        self.x_r %= self.modulus
        self.r %= self.modulus
        
        return self
    
    def __iadd__(self: SharedTensor[TP], other: int) -> SharedTensor[TP]:
        raise NotImplementedError("SharedTensor[TP]s cannot be publicly added to without IR pass enabled.")
    
    def __add__(self: SharedTensor[TP], other) -> SharedTensor[TP]:
        if isinstance(other, SharedTensor[TP]):
            new_number = SharedTensor[TP](self if not self.diagonal else other)
            new_number += (other if not self.diagonal else self)
            return new_number
        else:
            new_number = SharedTensor[TP](self)
            new_number += other
            return new_number
    
    def __isub__(self: SharedTensor[TP], other: SharedTensor[TP]) -> SharedTensor[TP]:
        if isinstance(self, list[list[int_t]]):
            if other.diagonal and not NO_OPT:
                for i in range(len(self)):
                    self.share[i][i] -= other.share[i][i]
            else: self.share -= other.share
        else: self.share -= other.share
        
        if self.x_r and other.x_r:
            if isinstance(self, list[list[int_t]]):
                if other.diagonal and not NO_OPT:
                    for i in range(len(self)):
                        self.x_r[i][i] -= other.x_r[i][i]
                else: self.x_r -= other.x_r
            else: self.x_r -= other.x_r
        else: self.x_r = TP(0)
        
        if self.r and other.r:
            if isinstance(self, list[list[int_t]]):
                if other.diagonal and not NO_OPT:
                    for i in range(len(self)):
                        self.r[i][i] -= other.r[i][i]
                else: self.r -= other.r    
            else: self.r -= other.r
        else: self.r = TP(0)
        
        self.pows = []
        self.sqrt = TP(0)
        self.sqrt_inv = TP(0)
        
        self.share %= self.modulus
        self.x_r %= self.modulus
        self.r %= self.modulus

        return self
    
    def __isub__(self: SharedTensor[TP], other: TP) -> SharedTensor[TP]:
        self.share -= other
        if self.is_partitioned():
            self.x_r -= other
            self.r -= other
        self.pows = []
        self.sqrt = TP(0)
        self.sqrt_inv = TP(0)
        
        self.share %= self.modulus
        self.x_r %= self.modulus
        self.r %= self.modulus
        
        return self
    
    def __isub__(self: SharedTensor[TP], other: int) -> SharedTensor[TP]:
        raise NotImplementedError("SharedTensor[TP]s cannot be publicly subtracted to without IR pass enabled.")
    
    def __sub__(self: SharedTensor[TP], other) -> SharedTensor[TP]:
        if isinstance(other, SharedTensor[TP]):
            new_number = SharedTensor[TP](self if not self.diagonal else other)
            new_number -= other if not self.diagonal else self
            return new_number
        else:
            new_number = SharedTensor[TP](self)
            new_number -= other
            return new_number
    
    def __imul__(self: SharedTensor[TP], other) -> SharedTensor[TP]:        
        if isinstance(other, SharedTensor):
            raise NotImplementedError("SharedTensor[TP]s cannot be multiplied without IR pass enabled.")
        else:
            self.share *= other
            if self.is_partitioned():
                self.x_r *= other
                self.r *= other
            
            for i in range(1, len(self.pows)):
                self.pows[i] *= other ** i
                self.pows[i] %= self.modulus
            
            self.share %= self.modulus
            self.x_r %= self.modulus
            self.r %= self.modulus

            # TODO: Fix sqrt
            # other_sqrt = math.sqrt(other)
            # self.sqrt *= other_sqrt
            # self.sqrt_inv /= other_sqrt
            
            return self
    
    def __mul__(self: SharedTensor[TP], other) -> SharedTensor[TP]:
        new_number = SharedTensor[TP](self)
        new_number *= other
        return new_number
    
    def __truediv__(self: SharedTensor[TP], other) -> SharedTensor[TP]:
        raise NotImplementedError("SharedTensor[TP]s cannot be divided without IR pass enabled.")
    
    def __ipow__(self: SharedTensor[TP], other) -> SharedTensor[TP]:
        raise NotImplementedError(
            "You tried to power a SharedTensor[TP], which is impossible.\n"
            "If the IR pass is enabled though, SharedTensor[TP] can be powered by an int.")
    
    def __pow__(self: SharedTensor[TP], other) -> SharedTensor[TP]:
        new_number = SharedTensor[TP](self)
        new_number **= other
        return new_number
    
    def __gt__(self: SharedTensor[TP], other) -> SharedTensor[TP]:
        raise NotImplementedError(
            "SharedTensor[TP]s cannot be compared without IR pass enabled")
    
    def __lt__(self: SharedTensor[TP], other) -> SharedTensor[TP]:
        raise NotImplementedError(
            "SharedTensor[TP]s cannot be compared without IR pass enabled")
    
    def __len__(self: SharedTensor[TP]) -> int:
        return len(self.share)
    
    def diag(self: SharedTensor[TP], other: SharedTensor[int_t]) -> SharedTensor[TP]:
        diagonal_sv = self.zeros()

        for i in range(len(diagonal_sv)):
            diagonal_sv[i, i] = other
        
        diagonal_sv.modulus = other.modulus
        diagonal_sv.fp = other.fp
        diagonal_sv.public = other.public
        diagonal_sv.diagonal = True

        return diagonal_sv

    @property
    def I(self: SharedTensor[TP]) -> SharedTensor[TP]:
        identity = self.share.get_identity()
        if self.fp:
            one = double_to_fp(1.0, self.modulus)
            for i in range(len(identity)):
                identity[i][i] = one
        
        sv = SharedTensor[TP](identity, self.modulus)
        sv.modulus = self.modulus
        sv.fp = self.fp
        sv.public = True
        sv.diagonal = True

        return sv
    
    @property
    def T(self: SharedTensor[TP]) -> SharedTensor[TP]:
        # TODO: Consider storing T within self after fist calculation.
        pows = [p.transpose() for p in self.pows]
        
        return SharedTensor[TP](
            share = self.share.transpose(),
            x_r = self.x_r.transpose() if self.x_r else self.x_r,
            r = self.r.transpose() if self.r else self.r,
            modulus = self.modulus,
            sqrt = self.sqrt.transpose() if self.sqrt else self.sqrt,
            sqrt_inv = self.sqrt_inv.transpose() if self.sqrt_inv else self.sqrt_inv,
            pows = pows,
            fp = self.fp,
            public = self.public,
            diagonal = self.diagonal)

    def is_partitioned(self: SharedTensor[TP]) -> bool:
        return bool(self.r)
    
    def set_partitions(self: SharedTensor[TP], partitions):
        self.x_r, self.r = partitions
    
    def get_partitions(self: SharedTensor[TP]):
        return self.x_r, self.r
    
    def get_partitions(self: SharedTensor[TP], mpc, force = False):
        if not self.is_partitioned() or force:
            self.set_partitions(mpc.arithmetic.__beaver_partition(self.share, self.modulus))
        return self.get_partitions()
    
    def is_fp(self):
        return self.fp
    
    def is_public(self):
        return self.public
    
    def copy(self):
        share = self.share.copy()
        x_r = self.x_r.copy()
        r = self.r.copy()
        modulus = self.modulus
        sqrt = self.sqrt.copy()
        sqrt_inv = self.sqrt_inv.copy()
        pows = self.pows.copy()
        fp = self.fp
        public = self.public
        diagonal = self.diagonal
        
        return SharedTensor[TP](
            share, x_r, r, modulus, sqrt,
            sqrt_inv, pows, fp, public, diagonal)
    
    def expand_dims(self):
        pows = [[p] for p in self.pows]
        
        return SharedTensor[list[TP]](
            share = [self.share],
            x_r = [self.x_r] if self.x_r else list[TP](),
            r = [self.r] if self.r else list[TP](),
            modulus = self.modulus,
            sqrt = [self.sqrt] if self.sqrt else list[TP](),
            sqrt_inv = [self.sqrt_inv] if self.sqrt_inv else list[TP](),
            pows = pows,
            fp = self.fp,
            public = self.public,
            diagonal = self.diagonal)
    
    def zeros(self):
        return SharedTensor[TP](
            share = self.share.zeros(),
            x_r = self.share.zeros(),
            r = self.share.zeros(),
            modulus = self.modulus,
            sqrt = self.share.zeros(),
            sqrt_inv = self.share.zeros(),
            pows = list[TP](),
            fp = self.fp,
            public = self.public,
            diagonal = self.diagonal)

    def zeros(rows: int, cols: int, modulus: int_t) -> SharedTensor[list[list[int_t]]]:
        return SharedTensor[list[list[int_t]]](
            share = zeros[int_t](rows, cols),
            x_r = zeros[int_t](rows, cols),
            r = zeros[int_t](rows, cols),
            modulus = modulus,
            sqrt = zeros[int_t](rows, cols),
            sqrt_inv = zeros[int_t](rows, cols),
            pows = list[list[list[int_t]]](),
            fp = False,
            public = False,
            diagonal = True)
    
    def zeros(size: int, modulus: int_t) -> SharedTensor[list[int_t]]:
        return SharedTensor[list[int_t]](
            share = zeros_vec[int_t](size),
            x_r = zeros_vec[int_t](size),
            r = zeros_vec[int_t](size),
            modulus = modulus,
            sqrt = zeros_vec[int_t](size),
            sqrt_inv = zeros_vec[int_t](size),
            pows = list[list[int_t]](),
            fp = False,
            public = False,
            diagonal = True)

    def ones(self):
        return SharedTensor[TP](
            share = self.share.ones(),
            x_r = TP(0),
            r = TP(0),
            modulus = self.modulus,
            sqrt = self.share.ones(),
            sqrt_inv = self.share.ones(),
            pows = list[TP](),
            fp = self.fp,
            public = self.public,
            diagonal = self.diagonal)
    
    def raw_zeros(self):
        return self.share.zeros()

    def raw_ones(self):
        return self.share.ones()
    
    def shape(self) -> list[int]:
        return self.share.shape()
    
    def extend(self, other):
        share = self.share.__copy__()
        x_r = self.x_r.__copy__()
        r = self.r.__copy__()
        modulus = self.modulus
        sqrt = self.sqrt.__copy__()
        sqrt_inv = self.sqrt_inv.__copy__()
        pows = [p.__copy__() for p in self.pows]
        fp = self.fp
        public = self.public
        diagonal = False

        share.extend(other.share)
        if x_r: x_r.extend(other.x_r)
        if r: r.extend(other.r)
        for i in range(len(pows)): pows[i].extend(other.pows[i])
        if sqrt: sqrt.extend(other.sqrt)
        if sqrt_inv: sqrt_inv.extend(other.sqrt_inv)

        return SharedTensor[TP](
            share, x_r, r, modulus, sqrt,
            sqrt_inv, pows, fp, public, diagonal)
    
    def pad(self, rows: int, cols: int):
        pows = [p.pad(rows, cols) for p in self.pows]
        
        return SharedTensor[TP](
            self.share.pad(rows, cols),
            self.x_r.pad(rows, cols),
            self.r.pad(rows, cols),
            self.modulus,
            self.sqrt.pad(rows, cols),
            self.sqrt_inv.pad(rows, cols),
            pows, self.fp, self.public,
            self.diagonal and rows == cols)

    def pad_right(self, size: int):
        assert isinstance(TP, list[int_t]), 'Cannot pad anything other than vector'
        extension = [int_t(0) for _ in range(size)]
        
        share = self.share.__copy__()
        x_r = self.x_r.__copy__()
        r = self.r.__copy__()
        modulus = self.modulus
        sqrt = self.sqrt.__copy__()
        sqrt_inv = self.sqrt_inv.__copy__()
        pows = [p.__copy__() for p in self.pows]
        fp = self.fp
        public = self.public
        diagonal = False

        share.extend(extension)
        if x_r: x_r.extend(extension)
        if r: r.extend(extension)
        for i in range(len(pows)): pows[i].extend(extension)
        if sqrt: sqrt.extend(extension)
        if sqrt_inv: sqrt_inv.extend(extension)

        return SharedTensor[TP](
            share, x_r, r, modulus, sqrt,
            sqrt_inv, pows, fp, public, diagonal)
    
    def pad_left(self, size: int):
        assert isinstance(TP, list[int_t]), 'Cannot pad anything other than vector'
        extension = [int_t(0) for _ in range(size)]
        
        share = extension.__copy__()
        x_r = extension.__copy__() if self.x_r else self.x_r
        r = extension.__copy__() if self.r else self.r
        modulus = self.modulus
        sqrt = extension.__copy__() if self.sqrt else self.sqrt
        sqrt_inv = extension.__copy__() if self.sqrt_inv else self.sqrt_inv
        pows = [extension.__copy__() for _ in range(len(self.pows))]
        fp = self.fp
        public = self.public
        diagonal = False

        share.extend(self.share)
        if self.x_r: x_r.extend(self.x_r)
        if self.r: r.extend(self.r)
        for i in range(len(self.pows)): pows[i].extend(self.pows[i])
        if self.sqrt: sqrt.extend(self.sqrt)
        if self.sqrt_inv: sqrt_inv.extend(self.sqrt_inv)
        
        return SharedTensor[TP](
            share, x_r, r, modulus, sqrt, sqrt_inv,
            pows, fp, public, diagonal)
    
    def filter(self, mask):
        mask_len = len(mask)
        pows = [[p[i] for i in range(mask_len) if mask[i]] for p in self.pows]

        return SharedTensor[TP](
            share = [self.share[i] for i in range(mask_len) if mask[i]],
            x_r = [self.x_r[i] for i in range(mask_len) if mask[i]] if self.x_r else TP(),
            r = [self.r[i] for i in range(mask_len) if mask[i]] if self.r else TP(),
            modulus = self.modulus,
            sqrt = [self.sqrt[i] for i in range(mask_len) if mask[i]] if self.sqrt else TP(),
            sqrt_inv = [self.sqrt_inv[i] for i in range(mask_len) if mask[i]] if self.sqrt_inv else TP(),
            pows = pows,
            fp = self.fp,
            public = self.public,
            diagonal = self.diagonal)
    
    def sum(self):
        cumsum = self[0].zeros()
        for e in self: cumsum += e
        return cumsum
    
    def to_fp(self):
        if self.fp: return self
        modulus = self.modulus
        
        fp_one = double_to_fp(1.0, modulus)
        pows = [p * fp_one for p in self.pows]
        
        return SharedTensor[TP](
            share = (self.share * fp_one) % modulus,
            x_r = (self.x_r * fp_one) % modulus,
            r = (self.r * fp_one) % modulus,
            modulus = modulus,
            sqrt = self.sqrt,
            sqrt_inv = self.sqrt_inv,
            pows = pows,
            fp = True,
            public = self.public,
            diagonal = self.diagonal)
    
    def trunc(self, fp):
        pows = [fp.trunc(p, self.modulus) for p in self.pows]
        return SharedTensor[TP](
            share = fp.trunc(self.share, self.modulus),
            x_r = TP(0),  # TODO: #61 Resolve #61 and calculate x_r here
            r = TP(0),  # TODO: #61 Resolve #61 and calculate x_r here
            modulus = self.modulus,
            sqrt = fp.trunc(self.sqrt, self.modulus) if self.sqrt else self.sqrt,
            sqrt_inv = fp.trunc(self.sqrt_inv, self.modulus) if self.sqrt_inv else self.sqrt_inv,
            pows = pows,
            fp = self.fp,
            public = self.public,
            diagonal = self.diagonal)
    
    def beaver_reveal(self, mpc):
        revealed_float = self.share.zeros_float()
        if mpc.pid:
            revealed_float = mpc.comms.print_fp((self.x_r if mpc.pid == 1 else self.x_r.zeros()) + self.r)
        
        return revealed_float
    
    def validate_partitions(self, mpc, message = ""):
        if mpc.pid:
            partition_share = (self.x_r if mpc.pid == 1 else self.x_r.zeros()) + self.r
            partition_reveal = mpc.comms.reveal(partition_share)
            share_reveal = mpc.comms.reveal(self.share)
            assert partition_reveal == share_reveal, f"{message}:\n\tPartition reveal: {partition_reveal}\n\tShare reveal: {share_reveal}"

    def print(self, mpc, debug = False):
        revealed_value = mpc.comms.reveal(self.share, self.modulus)
        if debug and mpc.pid == 2: print(f'{mpc.pid}: Revealed value - {revealed_value}')
        if self.fp: return mpc.comms.print_fp(self.share, self.modulus)
        if mpc.pid == 0: return self.share.zeros_float()
        if mpc.pid == 2: print(f'{mpc.pid}: Revealed int value - {revealed_value}')
        return revealed_value.to_float()
    
    def flatten(self):
        pows = [p.flatten() for p in self.pows]
        share = self.share.flatten()
        return SharedTensor[typeof(share)](
            share = share,
            x_r = self.x_r.flatten(),
            r = self.r.flatten(),
            modulus = self.modulus,
            sqrt = self.sqrt.flatten(),
            sqrt_inv = self.sqrt_inv.flatten(),
            pows = pows,
            fp = self.fp,
            public = self.public,
            diagonal = False)
    
    def reshape(self, shape):
        pows = [p.reshape(shape) for p in self.pows]
        share = self.share.reshape(shape)
        return SharedTensor[typeof(share)](
            share = share,
            x_r = self.x_r.reshape(shape) if self.x_r else self.x_r,
            r = self.r.reshape(shape) if self.r else self.r,
            modulus = self.modulus,
            sqrt = self.sqrt.reshape(shape) if self.sqrt else self.sqrt,
            sqrt_inv = self.sqrt_inv.reshape(shape) if self.sqrt_inv else self.sqrt_inv,
            pows = pows,
            fp = self.fp,
            public = self.public,
            diagonal = False)
    
    def parallel_add(self: SharedTensor[list[list[int_t]]], other: SharedTensor[list[int_t]]):
        return SharedTensor[list[list[int_t]]](
            share = self.share.parallel_add(other.share),
            x_r = (self.x_r.parallel_add(other.x_r)) if self.x_r else self.x_r,
            r = (self.r.parallel_add(other.r)) if self.r else self.r,
            modulus = self.modulus,
            sqrt = TP(0),
            sqrt_inv = TP(0),
            pows = list[TP](),
            fp = self.fp,
            public = self.public,
            diagonal = False)
    
    def broadcast_add(self: SharedTensor[list[list[int_t]]], other: SharedTensor[list[int_t]]):
        return SharedTensor[list[list[int_t]]](
            share = self.share.broadcast_add(other.share),
            x_r = (self.x_r.broadcast_add(other.x_r)) if self.x_r else self.x_r,
            r = (self.r.broadcast_add(other.r)) if self.r else self.r,
            modulus = self.modulus,
            sqrt = TP(0),
            sqrt_inv = TP(0),
            pows = list[TP](),
            fp = self.fp,
            public = self.public,
            diagonal = False)
    
    def to_ring(self, mpc):
        assert self.modulus.popcnt() != 1, "Shared tensor already on ring."
        pows = [mpc.arithmetic.field_to_ring(e) for e in self.pows]

        self.share = mpc.arithmetic.field_to_ring(self.share)
        # TODO: #128 Calculate partitions instead of deleting them on modulus switch
        self.x_r = TP(0)
        self.r = TP(0)
        self.modulus = RING_SIZE
        self.sqrt = mpc.arithmetic.field_to_ring(self.sqrt) if self.sqrt else self.sqrt
        self.sqrt_inv = mpc.arithmetic.field_to_ring(self.sqrt_inv) if self.sqrt_inv else self.sqrt_inv
        self.pows = pows

    def to_field(self, mpc):
        assert self.modulus.popcnt() == 1, "Shared tensor already on field."
        pows = [mpc.arithmetic.ring_to_field(e) for e in self.pows]

        self.share = mpc.arithmetic.ring_to_field(self.share)
        # TODO: #128 Calculate partitions instead of deleting them on modulus switch
        self.x_r = TP(0)
        self.r = TP(0)
        self.modulus = FIELD_SIZE
        self.sqrt = mpc.arithmetic.ring_to_field(self.sqrt) if self.sqrt else self.sqrt
        self.sqrt_inv = mpc.arithmetic.ring_to_field(self.sqrt_inv) if self.sqrt_inv else self.sqrt_inv
        self.pows = pows
