from ..utils.param import *
from ..utils.utils import zeros, zeros_vec
from utils import double_to_fp
from ..types.utils import num_to_bits


class SharedTensor[TP]:
    share: TP
    x_r: TP
    r: TP
    modulus: int_t  # TODO: #145 Add support for generic modulus type after migrating to new Codon
    sqrt: TP
    sqrt_inv: TP
    fp: bool
    public: bool
    diagonal: bool

    def __init__(self: SharedTensor[TP], other: int, modulus: int_t):
        self.share = TP(other)
        self.x_r = TP(0)
        self.r = TP(0)
        self.modulus = modulus
        self.sqrt = TP(0)
        self.sqrt_inv = TP(0)
        self.fp = False
        self.public = False
        self.diagonal = False
    
    def __init__(self: SharedTensor[TP], share: TP, x_r: TP, r: TP, modulus: int_t):
        self.share = share
        self.x_r = x_r
        self.r = r
        self.modulus = modulus
        self.sqrt = TP(0)
        self.sqrt_inv = TP(0)
        self.fp = False
        self.public = False
        self.diagonal = False
    
    def __init__(self: SharedTensor[TP],
                 share: TP, x_r: TP, r: TP,
                 modulus: int_t, sqrt: TP,
                 sqrt_inv: TP, fp: bool,
                 public: bool, diagonal: bool):
        self.share = share
        self.x_r = x_r
        self.r = r
        self.modulus = modulus
        self.sqrt = sqrt
        self.sqrt_inv = sqrt_inv
        self.fp = fp
        self.public = public
        self.diagonal = diagonal

    def __init__(self: SharedTensor[TP], other: TP, modulus: int_t):
        self.share = other
        self.x_r = TP(0)
        self.r = TP(0)
        self.modulus = modulus
        self.sqrt = TP(0)
        self.sqrt_inv = TP(0)
        self.fp = False
        self.public = False
        self.diagonal = False

    def __init__(self: SharedTensor[TP], other: SharedTensor[TP]):
        self.share = other.share.copy()
        self.x_r = other.x_r.copy()
        self.r = other.r.copy()
        self.modulus = other.modulus
        self.sqrt = other.sqrt.copy()
        self.sqrt_inv = other.sqrt_inv.copy()
        self.fp = other.fp
        self.public = other.public
        self.diagonal = other.diagonal
    
    def __iter__(self: SharedTensor[TP]):
        for i in range(len(self)): yield self[i]
    
    def __getitem__(self: SharedTensor[TP], index):
        item_share = self.share[index]
        return SharedTensor[typeof(item_share)](
            item_share,
            self.x_r[index] if self.x_r else typeof(item_share)(0),
            self.r[index] if self.r else typeof(item_share)(0),
            self.modulus,
            self.sqrt[index] if self.sqrt else typeof(item_share)(0),
            self.sqrt_inv[index] if self.sqrt_inv else typeof(item_share)(0),
            self.fp,
            self.public,
            False)
    
    def __setitem__(self: SharedTensor[TP], index, other):
        if DEBUG: assert self.public == other.public
        if isinstance(index, tuple[int, int]):
            ri, ci = index
            self.diagonal = self.diagonal and (ri == ci)
        else:
            self.diagonal = False
        self.fp = other.fp
        self.modulus = other.modulus
        
        self.share[index] = other.share
        if not other.x_r: self.x_r = TP(0)
        elif self.x_r: self.x_r[index] = other.x_r
        
        if not other.r: self.r = TP(0)
        elif self.r:self.r[index] = other.r

        if not other.sqrt: self.sqrt = TP(0)
        elif self.sqrt: self.sqrt[index] = other.sqrt
        
        if not other.sqrt_inv: self.sqrt_inv = TP(0)
        elif self.sqrt_inv: self.sqrt_inv[index] = other.sqrt_inv
        
    def __bool__(self: SharedTensor[TP]) -> bool:
        return bool(self.share)
    
    def __int__(self: SharedTensor[TP]) -> int:
        return int(self.share)
    
    def __neg__(self: SharedTensor[TP]) -> SharedTensor[TP]:
        return SharedTensor[TP](
            share = self.share.neg_mod(self.modulus),
            x_r = self.x_r.neg_mod(self.modulus) if self.x_r else self.x_r,
            r = self.r.neg_mod(self.modulus) if self.r else self.r,
            modulus = self.modulus,
            sqrt = TP(0),
            sqrt_inv = TP(0),
            fp = self.fp,
            public = self.public,
            diagonal = self.diagonal)

    def __iadd__(self: SharedTensor[TP], other: SharedTensor[TP]) -> SharedTensor[TP]:
        if isinstance(TP, list[list[int_t]]):
            if other.diagonal and MATRIX_OPT:
                for i in range(len(self)):
                    self.share[i][i] = self.share[i][i].add_mod(other.share[i][i], self.modulus)
            else: self.share = self.share.add_mod(other.share, self.modulus)
        else: self.share = self.share.add_mod(other.share, self.modulus)

        if self.x_r and other.x_r:
            if isinstance(TP, list[list[int_t]]):
                if other.diagonal and MATRIX_OPT:
                    for i in range(len(self)):
                        self.x_r[i][i] = self.x_r[i][i].add_mod(other.x_r[i][i], self.modulus)
                else: self.x_r = self.x_r.add_mod(other.x_r, self.modulus)
            else: self.x_r = self.x_r.add_mod(other.x_r, self.modulus)
        else: self.x_r = TP(0)

        if self.r and other.r:
            if isinstance(TP, list[list[int_t]]):
                if other.diagonal and MATRIX_OPT:
                    for i in range(len(self)):
                        self.r[i][i] = self.r[i][i].add_mod(other.r[i][i], self.modulus)
                else: self.r = self.r.add_mod(other.r, self.modulus)
            else: self.r = self.r.add_mod(other.r, self.modulus)
        else: self.r = TP(0)

        self.sqrt = TP(0)
        self.sqrt_inv = TP(0)
        
        return self
    
    def __iadd__(self: SharedTensor[TP], other: TP) -> SharedTensor[TP]:
        self.share = self.share.add_mod(other, self.modulus)
        if self.is_partitioned():
            self.x_r = self.x_r.add_mod(other, self.modulus)
            self.r = self.r.add_mod(other, self.modulus)
        self.sqrt = TP(0)
        self.sqrt_inv = TP(0)
        
        return self
    
    def __iadd__(self: SharedTensor[TP], other: int) -> SharedTensor[TP]:
        raise NotImplementedError("SharedTensor[TP]s cannot be publicly added to without IR pass enabled.")
    
    def __add__(self: SharedTensor[TP], other) -> SharedTensor[TP]:
        if isinstance(other, SharedTensor[TP]):
            new_number = SharedTensor[TP](self if not self.diagonal else other)
            new_number += (other if not self.diagonal else self)
            return new_number
        else:
            new_number = SharedTensor[TP](self)
            new_number += other
            return new_number
    
    def __isub__(self: SharedTensor[TP], other: SharedTensor[TP]) -> SharedTensor[TP]:
        if isinstance(self, list[list[int_t]]):
            if other.diagonal and MATRIX_OPT:
                for i in range(len(self)):
                    self.share[i][i] = self.share[i][i].sub_mod(other.share[i][i], self.modulus)
            else: self.share = self.share.sub_mod(other.share, self.modulus)
        else: self.share = self.share.sub_mod(other.share, self.modulus)
        
        if self.x_r and other.x_r:
            if isinstance(self, list[list[int_t]]):
                if other.diagonal and MATRIX_OPT:
                    for i in range(len(self)):
                        self.x_r[i][i] = self.x_r[i][i].sub_mod(other.x_r[i][i], self.modulus)
                else: self.x_r = self.x_r.sub_mod(other.x_r, self.modulus)
            else: self.x_r = self.x_r.sub_mod(other.x_r, self.modulus)
        else: self.x_r = TP(0)
        
        if self.r and other.r:
            if isinstance(self, list[list[int_t]]):
                if other.diagonal and MATRIX_OPT:
                    for i in range(len(self)):
                        self.r[i][i] = self.r[i][i].sub_mod(other.r[i][i], self.modulus)
                else: self.r = self.r.sub_mod(other.r, self.modulus)
            else: self.r = self.r.sub_mod(other.r, self.modulus)
        else: self.r = TP(0)
        
        self.sqrt = TP(0)
        self.sqrt_inv = TP(0)
        
        return self
    
    def __isub__(self: SharedTensor[TP], other: TP) -> SharedTensor[TP]:
        self.share = self.share.sub_mod(other, self.modulus)
        if self.is_partitioned():
            self.x_r = self.x_r.sub_mod(other, self.modulus)
            self.r = self.r.sub_mod(other, self.modulus)
        self.sqrt = TP(0)
        self.sqrt_inv = TP(0)
        
        return self
    
    def __isub__(self: SharedTensor[TP], other: int) -> SharedTensor[TP]:
        raise NotImplementedError("SharedTensor[TP]s cannot be publicly subtracted to without IR pass enabled.")
    
    def __sub__(self: SharedTensor[TP], other) -> SharedTensor[TP]:
        if isinstance(other, SharedTensor[TP]):
            new_number = SharedTensor[TP](self if not self.diagonal else other)
            new_number = new_number - (other if not self.diagonal else self)
            return new_number
        else:
            new_number = SharedTensor[TP](self)
            new_number = new_number - other
            return new_number
    
    def __imul__(self: SharedTensor[TP], other) -> SharedTensor[TP]:        
        if isinstance(other, SharedTensor):
            raise NotImplementedError("SharedTensor[TP]s cannot be multiplied without IR pass enabled.")
        elif isinstance(other, float):
            raise NotImplementedError("SharedTensor[TP]s cannot be multiplied by float without IR pass enabled.")
        else:
            self.share = self.share.mul_mod(other, self.modulus)
            if self.is_partitioned():
                self.x_r = self.x_r.mul_mod(other, self.modulus)
                self.r = self.r.mul_mod(other, self.modulus)
            
            # TODO: Fix sqrt
            # other_sqrt = math.sqrt(other)
            # self.sqrt *= other_sqrt
            # self.sqrt_inv /= other_sqrt
            
            return self
    
    def __mul__(self: SharedTensor[TP], other) -> SharedTensor[TP]:
        new_number = SharedTensor[TP](self)
        new_number *= other
        return new_number
    
    def __truediv__(self: SharedTensor[TP], other) -> SharedTensor[TP]:
        raise NotImplementedError("SharedTensor[TP]s cannot be divided without IR pass enabled.")
    
    def __ipow__(self: SharedTensor[TP], other) -> SharedTensor[TP]:
        raise NotImplementedError(
            "You tried to power a SharedTensor[TP], which is impossible.\n"
            "If the IR pass is enabled though, SharedTensor[TP] can be powered by an int.")
    
    def __pow__(self: SharedTensor[TP], other) -> SharedTensor[TP]:
        new_number = SharedTensor[TP](self)
        new_number **= other
        return new_number
    
    def __gt__(self: SharedTensor[TP], other) -> SharedTensor[TP]:
        raise NotImplementedError(
            "SharedTensor[TP]s cannot be compared without IR pass enabled")
    
    def __lt__(self: SharedTensor[TP], other) -> SharedTensor[TP]:
        raise NotImplementedError(
            "SharedTensor[TP]s cannot be compared without IR pass enabled")
    
    def __eq__(self: SharedTensor[TP], other) -> SharedTensor[TP]:
        raise NotImplementedError(
            "SharedTensor[TP]s cannot be compared without IR pass enabled")
    
    def __len__(self: SharedTensor[TP]) -> int:
        return len(self.share)
    
    def diag(self: SharedTensor[TP], other: SharedTensor[int_t]) -> SharedTensor[TP]:
        diagonal_sv = self.zeros()

        for i in range(len(diagonal_sv)):
            diagonal_sv[i, i] = other
        
        diagonal_sv.modulus = other.modulus
        diagonal_sv.fp = other.fp
        diagonal_sv.public = other.public
        diagonal_sv.diagonal = True

        return diagonal_sv

    @property
    def I(self: SharedTensor[TP]) -> SharedTensor[TP]:
        identity = self.share.get_identity()
        if self.fp:
            one = double_to_fp(1.0, self.modulus)
            for i in range(len(identity)):
                identity[i][i] = one
        
        sv = SharedTensor[TP](identity, self.modulus)
        sv.modulus = self.modulus
        sv.fp = self.fp
        sv.public = True
        sv.diagonal = True

        return sv
    
    @property
    def T(self: SharedTensor[TP]) -> SharedTensor[TP]:
        # TODO: Consider storing T within self after fist calculation.
        return SharedTensor[TP](
            share = self.share.transpose(),
            x_r = self.x_r.transpose() if self.x_r else self.x_r,
            r = self.r.transpose() if self.r else self.r,
            modulus = self.modulus,
            sqrt = self.sqrt.transpose() if self.sqrt else self.sqrt,
            sqrt_inv = self.sqrt_inv.transpose() if self.sqrt_inv else self.sqrt_inv,
            fp = self.fp,
            public = self.public,
            diagonal = self.diagonal)

    def is_partitioned(self: SharedTensor[TP]) -> bool:
        return bool(self.r)
    
    def set_partitions(self: SharedTensor[TP], partitions):
        self.x_r, self.r = partitions
    
    def get_partitions(self: SharedTensor[TP]):
        return self.x_r, self.r
    
    def get_partitions(self: SharedTensor[TP], mpc, force = False):
        if not self.is_partitioned() or force:
            self.set_partitions(mpc.arithmetic.__beaver_partition(self.share, self.modulus))
        return self.get_partitions()
    
    def is_fp(self):
        return self.fp
    
    def is_public(self):
        return self.public
    
    def copy(self):
        share = self.share.copy()
        x_r = self.x_r.copy()
        r = self.r.copy()
        modulus = self.modulus
        sqrt = self.sqrt.copy()
        sqrt_inv = self.sqrt_inv.copy()
        fp = self.fp
        public = self.public
        diagonal = self.diagonal
        
        return SharedTensor[TP](
            share, x_r, r, modulus, sqrt,
            sqrt_inv, fp, public, diagonal)
    
    def expand_dims(self):
        return SharedTensor[list[TP]](
            share = [self.share],
            x_r = [self.x_r] if self.x_r else list[TP](),
            r = [self.r] if self.r else list[TP](),
            modulus = self.modulus,
            sqrt = [self.sqrt] if self.sqrt else list[TP](),
            sqrt_inv = [self.sqrt_inv] if self.sqrt_inv else list[TP](),
            fp = self.fp,
            public = self.public,
            diagonal = self.diagonal)
    
    def broadcast(self, count: int):
        return SharedTensor[list[TP]](
            share = [self.share.copy() for _ in range(count)],
            x_r = [self.x_r.copy() for _ in range(count)] if self.x_r else list[TP](),
            r = [self.r.copy() for _ in range(count)] if self.r else list[TP](),
            modulus = self.modulus,
            sqrt = [self.sqrt.copy() for _ in range(count)] if self.sqrt else list[TP](),
            sqrt_inv = [self.sqrt_inv.copy() for _ in range(count)] if self.sqrt_inv else list[TP](),
            fp = self.fp,
            public = self.public,
            diagonal = self.diagonal)
    
    def range(l, h, modulus):
        return SharedTensor[list[int_t]](
            share = [int_t(l + i) for i in range(h - l)],
            x_r = [int_t(l + i) for i in range(h - l)],
            r = zeros_vec[int_t](h - l),
            modulus = modulus,
            sqrt = list[int_t](0),
            sqrt_inv = list[int_t](0),
            fp = False,
            public = True,
            diagonal = False)
    
    def zeros(self):
        return SharedTensor[TP](
            share = self.share.zeros(),
            x_r = self.share.zeros(),
            r = self.share.zeros(),
            modulus = self.modulus,
            sqrt = self.share.zeros(),
            sqrt_inv = self.share.zeros(),
            fp = self.fp,
            public = self.public,
            diagonal = self.diagonal)

    def zeros(rows: int, cols: int, modulus: int_t) -> SharedTensor[list[list[int_t]]]:
        return SharedTensor[list[list[int_t]]](
            share = zeros[int_t](rows, cols),
            x_r = zeros[int_t](rows, cols),
            r = zeros[int_t](rows, cols),
            modulus = modulus,
            sqrt = zeros[int_t](rows, cols),
            sqrt_inv = zeros[int_t](rows, cols),
            fp = False,
            public = False,
            diagonal = True)
    
    def zeros(size: int, modulus: int_t) -> SharedTensor[list[int_t]]:
        return SharedTensor[list[int_t]](
            share = zeros_vec[int_t](size),
            x_r = zeros_vec[int_t](size),
            r = zeros_vec[int_t](size),
            modulus = modulus,
            sqrt = zeros_vec[int_t](size),
            sqrt_inv = zeros_vec[int_t](size),
            fp = False,
            public = False,
            diagonal = True)

    def raw_zeros(self):
        return self.share.zeros()

    def raw_ones(self):
        return self.share.ones()
    
    def shape(self) -> list[int]:
        return self.share.shape()
    
    def append(self, other):
        self.share.append(other.share)
        if self.x_r: self.x_r.append(other.x_r)
        if self.r: self.r.append(other.r)
        if self.sqrt: self.sqrt.append(other.sqrt)
        if self.sqrt_inv: self.sqrt_inv.append(other.sqrt_inv)
    
    def extend(self, other):
        share = self.share.__copy__()
        x_r = self.x_r.__copy__()
        r = self.r.__copy__()
        modulus = self.modulus
        sqrt = self.sqrt.__copy__()
        sqrt_inv = self.sqrt_inv.__copy__()
        fp = self.fp
        public = self.public
        diagonal = False

        share.extend(other.share)
        if x_r: x_r.extend(other.x_r)
        if r: r.extend(other.r)
        if sqrt: sqrt.extend(other.sqrt)
        if sqrt_inv: sqrt_inv.extend(other.sqrt_inv)

        return SharedTensor[TP](
            share, x_r, r, modulus, sqrt,
            sqrt_inv, fp, public, diagonal)
    
    def pad(self, rows: int, cols: int):
        return SharedTensor[TP](
            self.share.pad(rows, cols),
            self.x_r.pad(rows, cols),
            self.r.pad(rows, cols),
            self.modulus,
            self.sqrt.pad(rows, cols),
            self.sqrt_inv.pad(rows, cols),
            self.fp,
            self.public,
            self.diagonal and rows == cols)

    def pad_right(self, size: int):
        if DEBUG: assert isinstance(TP, list[int_t]), 'Cannot pad anything other than vector'
        extension = [int_t(0) for _ in range(size)]
        
        share = self.share.__copy__()
        x_r = self.x_r.__copy__()
        r = self.r.__copy__()
        modulus = self.modulus
        sqrt = self.sqrt.__copy__()
        sqrt_inv = self.sqrt_inv.__copy__()
        fp = self.fp
        public = self.public
        diagonal = False

        share.extend(extension)
        if x_r: x_r.extend(extension)
        if r: r.extend(extension)
        if sqrt: sqrt.extend(extension)
        if sqrt_inv: sqrt_inv.extend(extension)

        return SharedTensor[TP](
            share, x_r, r, modulus, sqrt,
            sqrt_inv, fp, public, diagonal)
    
    def pad_left(self, size: int):
        if DEBUG: assert isinstance(TP, list[int_t]), 'Cannot pad anything other than vector'
        extension = [int_t(0) for _ in range(size)]
        
        share = extension.__copy__()
        x_r = extension.__copy__() if self.x_r else self.x_r
        r = extension.__copy__() if self.r else self.r
        modulus = self.modulus
        sqrt = extension.__copy__() if self.sqrt else self.sqrt
        sqrt_inv = extension.__copy__() if self.sqrt_inv else self.sqrt_inv
        fp = self.fp
        public = self.public
        diagonal = False

        share.extend(self.share)
        if self.x_r: x_r.extend(self.x_r)
        if self.r: r.extend(self.r)
        if self.sqrt: sqrt.extend(self.sqrt)
        if self.sqrt_inv: sqrt_inv.extend(self.sqrt_inv)
        
        return SharedTensor[TP](
            share, x_r, r, modulus, sqrt, sqrt_inv,
            fp, public, diagonal)
    
    def filter(self, mask):
        mask_len = len(mask)

        return SharedTensor[TP](
            share = [self.share[i] for i in range(mask_len) if mask[i]],
            x_r = [self.x_r[i] for i in range(mask_len) if mask[i]] if self.x_r else TP(),
            r = [self.r[i] for i in range(mask_len) if mask[i]] if self.r else TP(),
            modulus = self.modulus,
            sqrt = [self.sqrt[i] for i in range(mask_len) if mask[i]] if self.sqrt else TP(),
            sqrt_inv = [self.sqrt_inv[i] for i in range(mask_len) if mask[i]] if self.sqrt_inv else TP(),
            fp = self.fp,
            public = self.public,
            diagonal = self.diagonal)
    
    def sum(self):
        cumsum = self[0].zeros()
        for e in self: cumsum += e
        return cumsum
    
    def to_fp(self):
        if self.fp: return self
        modulus = self.modulus
        
        fp_one = double_to_fp(1.0, modulus)
        
        return SharedTensor[TP](
            share = self.share.mul_mod(fp_one, modulus),
            x_r = self.x_r.mul_mod(fp_one, modulus),
            r = self.r.mul_mod(fp_one, modulus),
            modulus = modulus,
            sqrt = self.sqrt,
            sqrt_inv = self.sqrt_inv,
            fp = True,
            public = self.public,
            diagonal = self.diagonal)
    
    def trunc(self, fp):
        return SharedTensor[TP](
            share = fp.trunc(self.share, self.modulus),
            x_r = TP(0),  # TODO: #61 Resolve #61 and calculate x_r here
            r = TP(0),  # TODO: #61 Resolve #61 and calculate x_r here
            modulus = self.modulus,
            sqrt = fp.trunc(self.sqrt, self.modulus) if self.sqrt else self.sqrt,
            sqrt_inv = fp.trunc(self.sqrt_inv, self.modulus) if self.sqrt_inv else self.sqrt_inv,
            fp = self.fp,
            public = self.public,
            diagonal = self.diagonal)
    
    def beaver_reveal(self, mpc):
        revealed_float = self.share.zeros_float()
        if mpc.pid:
            revealed_float = mpc.comms.print_fp((self.x_r if mpc.pid == 1 else self.x_r.zeros()).add_mod(self.r, self.modulus))
        
        return revealed_float
    
    def validate_partitions(self, mpc, message = ""):
        if mpc.pid and DEBUG:
            partition_share = (self.x_r if mpc.pid == 1 else self.x_r.zeros()).add_mod(self.r, self.modulus)
            partition_reveal = mpc.comms.reveal(partition_share, self.modulus)
            share_reveal = mpc.comms.reveal(self.share, self.modulus)
            assert partition_reveal == share_reveal, f"{message}:\n\tPartition reveal: {partition_reveal}\n\tShare reveal: {share_reveal}"

    def print(self, mpc, debug = False):
        if mpc.pid == 0: return self.share.zeros_float()
        revealed_value = mpc.comms.reveal(self.share, self.modulus)
        if debug and mpc.pid == 2: print(f'{mpc.pid}: Revealed value - {revealed_value}')
        if self.fp: return mpc.comms.print_fp(self.share, self.modulus)
        if debug and mpc.pid == 2: print(f'{mpc.pid}: Revealed int value - {revealed_value}')
        return revealed_value.to_float()
    
    def reveal(self, mpc):
        self.share = mpc.comms.reveal(self.share, self.modulus)
        self.public = True
        return self
    
    def flatten(self):
        share = self.share.flatten()
        return SharedTensor[typeof(share)](
            share = share,
            x_r = self.x_r.flatten(),
            r = self.r.flatten(),
            modulus = self.modulus,
            sqrt = self.sqrt.flatten(),
            sqrt_inv = self.sqrt_inv.flatten(),
            fp = self.fp,
            public = self.public,
            diagonal = False)
    
    def reshape(self, shape):
        share = self.share.reshape(shape)
        return SharedTensor[typeof(share)](
            share = share,
            x_r = self.x_r.reshape(shape) if self.x_r else self.x_r,
            r = self.r.reshape(shape) if self.r else self.r,
            modulus = self.modulus,
            sqrt = self.sqrt.reshape(shape) if self.sqrt else self.sqrt,
            sqrt_inv = self.sqrt_inv.reshape(shape) if self.sqrt_inv else self.sqrt_inv,
            fp = self.fp,
            public = self.public,
            diagonal = False)
    
    def parallel_add(self: SharedTensor[list[list[int_t]]], other: SharedTensor[list[int_t]]):
        return SharedTensor[list[list[int_t]]](
            share = self.share.parallel_add(other.share),
            x_r = (self.x_r.parallel_add(other.x_r)) if self.x_r else self.x_r,
            r = (self.r.parallel_add(other.r)) if self.r else self.r,
            modulus = self.modulus,
            sqrt = TP(0),
            sqrt_inv = TP(0),
            fp = self.fp,
            public = self.public,
            diagonal = False)
    
    def broadcast_add(self: SharedTensor[list[list[int_t]]], other: SharedTensor[list[int_t]]):
        return SharedTensor[list[list[int_t]]](
            share = self.share.broadcast_add_mod(other.share, self.modulus),
            x_r = self.x_r.broadcast_add_mod(other.x_r, self.modulus) if self.x_r else self.x_r,
            r = self.r.broadcast_add_mod(other.r, self.modulus) if self.r else self.r,
            modulus = self.modulus,
            sqrt = TP(0),
            sqrt_inv = TP(0),
            fp = self.fp,
            public = self.public,
            diagonal = False)
    
    def to_ring(self, mpc):
        if DEBUG: assert self.modulus.popcnt() != 1, "Shared tensor already on ring."

        self.share = mpc.arithmetic.field_to_ring(self.share)
        # TODO: #128 Calculate partitions instead of deleting them on modulus switch
        self.x_r = TP(0)
        self.r = TP(0)
        self.modulus = RING_SIZE
        self.sqrt = mpc.arithmetic.field_to_ring(self.sqrt) if self.sqrt else self.sqrt
        self.sqrt_inv = mpc.arithmetic.field_to_ring(self.sqrt_inv) if self.sqrt_inv else self.sqrt_inv

    def to_field(self, mpc):
        if DEBUG: assert self.modulus.popcnt() == 1, "Shared tensor already on field."

        self.share = mpc.arithmetic.ring_to_field(self.share)
        # TODO: #128 Calculate partitions instead of deleting them on modulus switch
        self.x_r = TP(0)
        self.r = TP(0)
        self.modulus = FIELD_SIZE
        self.sqrt = mpc.arithmetic.ring_to_field(self.sqrt) if self.sqrt else self.sqrt
        self.sqrt_inv = mpc.arithmetic.ring_to_field(self.sqrt_inv) if self.sqrt_inv else self.sqrt_inv
    

    def to_bits(self, mpc, bitlen, delimiter_prime):
        if self.public:
            bits = num_to_bits(self.share, bitlen, True).to_int_t()
            st = SharedTensor[typeof(bits)](bits, int_t(delimiter_prime))
            st.public = True
            return st
        
        bit_decomposition = mpc.boolean.bit_decomposition(
            self.share, bitlen, delimiter_prime, self.modulus).to_int_t()
        return SharedTensor[typeof(bit_decomposition)](bit_decomposition, int_t(delimiter_prime))
    

    def generate_random_bits(self, mpc, k: int, padding: int, n: int, little_endian: bool, small_modulus: int):
        r, rbits = mpc.boolean.__share_random_bits(k, padding, n, little_endian, small_modulus, self.modulus)
        rbits_extended = rbits.to_int_t()

        return SharedTensor[typeof(r)](r, self.modulus), SharedTensor[typeof(rbits_extended)](rbits_extended, int_t(small_modulus))
    
    def __no_trunc_mult(self, other: float):
        """ Temporary method. Until #117 is fixed."""
        other_fp = double_to_fp(other, self.modulus)
        sv = self * other_fp
        sv.fp = True
        return sv
