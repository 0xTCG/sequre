import random

from sequre.utils.param import \
    FIELD_PRIME, N_HIDDEN, FEATURE_RANK, N_CLASSES, N_NEURONS, \
    LOSS, REG, MOMENTUM, LEARN_RATE, NN_BATCH_SIZE, MAX_EPOCHS, \
    N_FILE_BATCH, int_t
from sequre.utils.utils import zeros, zeros_vec
from sequre.utils.io import read_matrix
from sequre.types.utils import double_to_fp, fp_to_double

from utils.data_sharing import shares_paths


def open_input_files(mpc, test_run):
    _, features_path, _ = shares_paths(mpc, 'neural_net', 'features', test_run)
    _, labels_path, _ = shares_paths(mpc, 'neural_net', 'labels', test_run)
    
    return open(features_path), open(labels_path)


def load_X_y(mpc, X_shape, y_shape, test_run):
    if mpc.pid == 0:
        return zeros(X_shape[0], X_shape[1]), zeros(y_shape[0], y_shape[1])
    
    fx, fy = open_input_files(mpc, test_run)
    X = read_matrix[int_t](fx, X_shape[0], X_shape[1])
    y = read_matrix[int_t](fy, y_shape[0], y_shape[1])
    fx.close()
    fy.close()
      
    return X, y


def initialize_model(mpc, modulus):
    W = []
    dW = []
    vW = []
    b = []
    db = []
    vb = []

    for l in range(N_HIDDEN + 1):
        W_layer = []
        b_layer = []

        if (N_HIDDEN == 0 and l >= 1):
            break
        elif (N_HIDDEN == 0 and l == 0):
            W_layer = zeros(FEATURE_RANK, N_CLASSES - 1)
            b_layer = zeros_vec(N_CLASSES - 1)
        elif (l == 0):
            W_layer = zeros(FEATURE_RANK, N_NEURONS)
            b_layer = zeros_vec(N_NEURONS)
        elif (l == N_HIDDEN):
            W_layer = zeros(N_NEURONS, N_CLASSES - 1)
            b_layer = zeros_vec(N_CLASSES - 1)
        else:
            W_layer = zeros(N_NEURONS, N_NEURONS)
            b_layer = zeros_vec(N_NEURONS)
    
        dW_layer = W_layer.zeros()
        vW_layer = W_layer.zeros()
    
        db_layer = b_layer.zeros()
        vb_layer = b_layer.zeros()

        rows, cols = W_layer.shape()
        if mpc.pid == 2:
            for i in range(rows):
                for j in range(cols):
                    noise = random.gauss(0.0, 0.01)
                    W_layer[i][j] = double_to_fp(noise, modulus)
            
            mpc.prg.switch_seed(1)
            W_r = W_layer.rand(modulus)
            b_r = b_layer.rand(modulus)
            mpc.prg.restore_seed(1)
            W_layer -= W_r
            b_layer -= b_r
            W_layer %= modulus
            b_layer %= modulus
      
        elif mpc.pid == 1:
            mpc.prg.switch_seed(2)
            W_r = W_layer.rand(modulus)
            b_r = b_layer.rand(modulus)
            mpc.prg.restore_seed(2)
            W_layer = W_r
            b_layer = b_r
    
        W.append(W_layer)
        dW.append(dW_layer)
        vW.append(vW_layer)
        b.append(b_layer)
        db.append(db_layer)
        vb.append(vb_layer)
     
    return W, dW, vW, b, db, vb


def gradient_descent(mpc, X, y, W, b, dW, db, vW, vb, modulus):
    act = []
    relus = []
    
    # Forward pass
    for l in range(N_HIDDEN):
        activation = mpc.arithmetic.multiply_matmul(X if l == 0 else act[l-1], W[l], modulus)
        activation = mpc.fp.trunc(activation, modulus)

        # Add bias term
        activation += b[l]
        activation %= modulus

        # Apply ReLU non-linearity
        relu = mpc.boolean.is_positive(activation, modulus)
        after_relu = mpc.arithmetic.multiply(activation, relu, modulus)

        # TODO: #110 Implement dropout.

        act.append(after_relu)
        relus.append(relu)
  
    # Calculate scores
    scores = mpc.arithmetic.multiply_matmul(X if N_HIDDEN == 0 else act[-1], W[-1], modulus)
    scores = mpc.fp.trunc(scores, modulus)

    # Add bias term
    for i in range(len(scores)):
        scores[i] += b[-1]
        scores[i] %= modulus

    dscores = y.zeros()
    mod_scores = y.zeros()
    if (LOSS == "hinge"):
        # Scale y to be -1 or 1
        y *= 2
        y %= modulus

        rows, cols = y.shape()
        if mpc.pid == 2:
            for i in range(rows):
                for j in range(cols):
                    y[i][j] -= double_to_fp(1.0, modulus)
                    y[i][j] %= modulus

        # Compute 1 - y * scores
        y *= -1
        y %= modulus
        mod_scores = mpc.arithmetic.multiply(y, scores, modulus)
        mod_scores = mpc.fp.trunc(mod_scores, modulus)

        rows, cols = mod_scores.shape()
        if mpc.pid == 2:
            for i in range(rows):
                for j in range(cols):
                    mod_scores[i][j] += double_to_fp(1.0, modulus)
                    mod_scores[i][j] %= modulus

        # Compute hinge loss and derivative
        hinge = mpc.boolean.is_positive(mod_scores, modulus)
        dscores = mpc.arithmetic.multiply(y, hinge, modulus)
    else:
        dscores = scores - y
        dscores %= modulus
    
    norm_examples = double_to_fp(1.0 / len(X), modulus)
    dscores = (dscores * norm_examples) % modulus
    dscores = mpc.fp.trunc(dscores, modulus)

    # Back propagation
    dhidden = dscores
    for l in range(N_HIDDEN, -1, -1):
        # Compute derivative of weights
        X_T = X.transpose() if l == 0 else act.pop().transpose()
        dW[l] = mpc.arithmetic.multiply_matmul(X_T, dhidden, modulus)
        dW[l] = mpc.fp.trunc(dW[l], modulus)
      
        # Add regularization term to weights
        if REG != 0.0:
            reg = (W[l] * double_to_fp(REG, modulus)) % modulus
            reg = mpc.fp.trunc(reg, modulus)
            dW[l] += reg
            dW[l] %= modulus

        # Compute derivative of biases
        db[l] = b[l].zeros()
        for i in range(len(dhidden)):
            db[l] += dhidden[i]
            db[l] %= modulus

        if l > 0:
            # Compute backpropagated activations
            W_T = W[l].transpose()
            dhidden_new = mpc.arithmetic.multiply_matmul(dhidden, W_T, modulus)
            dhidden_new = mpc.fp.trunc(dhidden_new, modulus)

            # Apply derivative of ReLU
            dhidden = mpc.arithmetic.multiply(dhidden_new, relus.pop(), modulus)

    assert len(act) == 0
    assert len(relus) == 0

    # Update the model using Nesterov momentum
    # Compute constants that update various parameters
    momentum = double_to_fp(MOMENTUM, modulus)
    increased_momentum = double_to_fp(MOMENTUM + 1, modulus)
    learn_rate = double_to_fp(LEARN_RATE, modulus)

    for l in range(N_HIDDEN + 1):
        # Update the weights
        vW_prev = vW[l].copy()
        vW[l] = (((vW[l] * momentum) % modulus) - ((dW[l] * learn_rate) % modulus)) % modulus
        vW[l] = mpc.fp.trunc(vW[l], modulus)
        W_update = (((vW_prev * (-momentum)) % modulus) + ((vW[l] * increased_momentum) % modulus)) % modulus
        W_update = mpc.fp.trunc(W_update, modulus)
        W[l] += W_update
        W[l] %= modulus

        # Update the biases
        vb_prev = vb[l].copy()
        vb[l] = (((vb[l] * momentum) % modulus) - ((db[l] * learn_rate) % modulus)) % modulus
        vb[l] = mpc.fp.trunc(vb[l], modulus)
        b_update = (((vb_prev * (-momentum)) % modulus) + ((vb[l] * increased_momentum) % modulus)) % modulus
        b_update = mpc.fp.trunc(b_update, modulus)
        b[l] += b_update
        b[l] %= modulus


def train_model(mpc, X, y, W, b, dW, db, vW, vb, modulus):    
    # Round down number of batches in file
    batches_in_file = len(X) // NN_BATCH_SIZE
    X_batch = zeros(NN_BATCH_SIZE, len(X[0]))
    y_batch = zeros(NN_BATCH_SIZE, len(y[0]))
    random_idx = list(range(len(X)))
    epoch = 0

    while True:
        # TODO: #113 Fix the shuffle bug
        # random.shuffle(random_idx)
        for i in range(batches_in_file):
            # Scan matrix (pre-shuffled) to get batch
            base_j = i * NN_BATCH_SIZE
            for j in range(base_j, min(base_j + NN_BATCH_SIZE, len(X))):
                X_batch[j - base_j] = X[random_idx[j]]
                y_batch[j - base_j] = y[random_idx[j]]
            
            # Do one round of mini-batch gradient descent
            if mpc.pid == 2: print(f"Epoch: {epoch}/{MAX_EPOCHS}")
            gradient_descent(
                mpc, X_batch, y_batch,
                W, b, dW, db, vW, vb, modulus)

            # Update reference to training epoch
            epoch += 1
            if epoch >= MAX_EPOCHS: return
    

def neural_net_protocol(mpc, test_run, modulus = FIELD_PRIME):
    if test_run: random.seed(0)
    # Initialize model and data structures
    print(f"Initializing model at CP{mpc.pid}...")
    W, dW, vW, b, db, vb = initialize_model(mpc, modulus)

    # Initialize data matrices
    X, y = load_X_y(mpc, [N_FILE_BATCH, FEATURE_RANK], [N_FILE_BATCH, N_CLASSES - 1], test_run)
    # Do gradient descent over multiple training epochs
    train_model(mpc, X, y, W, b, dW, db, vW, vb, modulus)

    if mpc.pid > 0 and not test_run:
        for l in range(N_HIDDEN + 1):
            with open(f'data/nn_models/__W{l}_P{mpc.pid}.txt', 'w') as fw, \
                 open(f'data/nn_models/__b{l}_P{mpc.pid}.txt', 'w') as fb:
                W_out_revealed = mpc.comms.reveal(W[l], modulus)
                W_out = fp_to_double(W_out_revealed, modulus)
                fw.write(f'{W_out}\n')
                b_out_revealed = mpc.comms.reveal(b[l], modulus)
                b_out = fp_to_double(b_out_revealed, modulus)
                fb.write(f'{b_out}\n')
    
    return W[-1]
