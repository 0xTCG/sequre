import math

from bio.fastq import FASTQ
from bio.kmer import Kmer


RANK = {
    'A': 0,
    'C': 1,
    'G': 2,
    'N': 3,
    'T': 4
}

SEQAN_HASH_SEEDS = [
    13572355802537770549u,  # 2**64 / (e/2)
    13043817825332782213u,  # 2**64 / sqrt(2)
    10650232656628343401u,  # 2**64 / sqrt(3)
    16499269484942379435u,  # 2**64 / (sqrt(5)/2)
    4893150838803335377u    # 2**64 / (3*pi/5)
]


def bit_decompose(value, number_of_bits):
    return [int(bool(value & (1 << i))) for i in range(number_of_bits)]


class FilterConfig:
    ibf: list[int]
    ibf_decomposed: list[list[int]]
    map: dict[int, int]
    rel_cutoff: float
    abs_cutoff: float
    bin_count: int
    technical_bin_count: int
    hash_count: int
    bin_size: u64

    def __init__(self, ibf_path, map_path, rel_cutoff, abs_cutoff, bin_count, hash_count):
        self.technical_bin_count = ((bin_count + 63) // 64) * 64
        self.ibf, self.ibf_decomposed = self.load_ibf(ibf_path, bin_count)
        self.map = FilterConfig.load_map(map_path)
        self.rel_cutoff = rel_cutoff
        self.abs_cutoff = abs_cutoff
        self.bin_count = bin_count
        self.hash_count = hash_count
        self.bin_size = u64(len(self.ibf))
    
    def load_ibf(self, ibf_path, bin_count):
        bytes_per_bin = (bin_count + 7) // 8
        bytes_count = 0
        next_value = 0
        ibf = []
        ibf_decomposed = []
        
        with open(ibf_path, 'rb') as f:
            while True:
                next_byte = f.read(1)
                
                if not next_byte:
                    break
                
                next_value <<= 8
                next_value += ord(next_byte)
                bytes_count += 1
                
                if bytes_count == bytes_per_bin:
                    ibf.append(next_value)
                    ibf_decomposed.append(bit_decompose(next_value, self.technical_bin_count))
                    bytes_count = 0
                    next_value = 0

        return ibf, ibf_decomposed

    def load_map(map_path):
        map_dict = {}
        
        with open(map_path) as f:
            count = 0
            for line in f:
                target = line.strip()
                map_dict[count] = int(target)
                count += 1
        
        return map_dict

class ReadOut:
    read_name: str
    matches: list[tuple[int, int]]

    def __init__(self, read_name):
        self.read_name = read_name
        self.matches = list[tuple[int, int]]()

def threshold_abs(kmers, e, k, o):
    return int(math.ceil(kmers - ( e * k ) / o)) if (kmers * o > e * k) else 0

def threshold_rel(kmers, p):
    return int(math.ceil(kmers * p))

def get_abs_error(kmers, k, o, count):
    return int(math.ceil(o * (-count + kmers) / k))

def select_matches(matches, counts_f, counts_r, filter_map, threshold_cutoff, max_kmer_count_read):
    # reset low threshold_cutoff to just one kmer (0 would match everywhere)
    if threshold_cutoff == 0:
        threshold_cutoff = 1

    # for each bin
    # for ( uint32_t bin_n = 0; bin_n < filter.ibf.noOfBins; ++bin_n )
    # loop in map structure to avoid extra validations when map.size() < filter.ibf.noOfBins when ibf is updated and
    # sequences removed also avoid the error of spurius results from empty bins (bug reported)
    for bin_n, target in filter_map.items():
        # if kmer count is higher than threshold_cutoff
        if counts_f[bin_n] >= threshold_cutoff or counts_r[bin_n] >= threshold_cutoff:
            # get best matching strand
            max_kmer_count_bin = max(counts_f[bin_n], counts_r[bin_n])
            # keep only the best match target/read when same targets are split in several bins
            if target not in matches or max_kmer_count_bin > matches[target]:
                # store match to target
                matches[target] = max_kmer_count_bin;
                if max_kmer_count_bin > max_kmer_count_read:
                    max_kmer_count_read = max_kmer_count_bin
    
    return max_kmer_count_read

def _seqan_kmer_hash(kmer, sigma):
    if sigma == 4:
        return u64(int(kmer.as_int()))
    
    hash_value = 0

    for nucleotide in str(kmer):
        hash_value *= sigma
        hash_value += RANK[nucleotide]
    
    return u64(hash_value)

def seqan_ungapped_kmer_hash(seq, sigma):
    for kmer in seq.kmers[Kmer[19]](1):
        yield _seqan_kmer_hash(kmer, sigma)

def seqan_ungapped_minimizer_hash(seq, sigma):
    window_size = 0
    seed=0x8F3F73B5CF1C9ADE  # Seed taken from official SeqAn documentation for minimiser_hash
    
    for window in seq.split(window_size, 1):
        kmers = list(window.kmers(1, k=19))
        kmers.extend(list((~window).kmers(1, k=19)))

        hashes = [_seqan_kmer_hash(kmer, sigma) for kmer in kmers]
        hashes_pertubed = [hash ^ seed for hash in hashes]

        min_hash_idx = 0
        min_hash_pertubed = hashes_pertubed[0]
        for i, hash_pertubed in enumerate(hashes_pertubed):
            if hash_pertubed < min_hash_pertubed:
                min_hash_idx = i
                min_hash_pertubed = hashes_pertubed[i]
        
        yield hashes[min_hash_idx]

def get_hashes(seq, window_size, offset, sigma):
    if window_size > 0:
        # minimizers
        # hashes_f = seqan_ungapped_minimizer_hash(seq, sigma)
        # return hashes_f, hashes_f
        raise NotImplementedError('Offset not implemented')

    if offset > 1:
        # offset
        raise NotImplementedError('Offset not implemented')
    
    # kmer
    return list(seqan_ungapped_kmer_hash(seq, sigma)), list(seqan_ungapped_kmer_hash(~seq, sigma))

def get_threshold_filter(hierarchy_config, kmers, max_kmer_count_read, kmer_size, offset):
    threshold_filter = 0
    if hierarchy_config["rel_filter"] >= 0:
        threshold_filter = max_kmer_count_read - threshold_rel(max_kmer_count_read, hierarchy_config["rel_filter"])
    elif hierarchy_config["abs_filter"] >= 0:
        # get maximum possible number of errors of best match + abs_filter
        max_error_threshold = get_abs_error( kmers, kmer_size, offset, max_kmer_count_read ) + hierarchy_config["abs_filter"]
        # get min kmer count necesary to achieve the calculated number of errors
        threshold_filter = threshold_abs( kmers, max_error_threshold, kmer_size, offset)

    return threshold_filter

def filter_matches(read_name, matches, threshold_filter):
    read_out = ReadOut(read_name)
    
    for target, kmer_count in matches.items():
        if kmer_count >= threshold_filter:
            read_out.matches.append((target, kmer_count))

    return read_out

def transpose(mat):
    rows, cols = len(mat), len(mat[0])
    new_mat = list[list[int]](cols)

    for i in range(cols):
        row = list[int](rows)
        for j in range(rows):
            row.append(mat[j][i])
        new_mat.append(row)

    return new_mat

def seqan_ibf_hash(value, seed, bin_size):
    hash_shift = 47u  # Hardcoded for now: the number of leading zeros in bin_size
    value *= seed
    assert hash_shift < 64u
    value ^= value >> hash_shift  # XOR and shift higher bits into lower bits
    value *= 11400714819323198485u  # = 2^64 / golden_ration, to expand h to 64 bit range
    value %= bin_size
    return int(value)

def query_ibf(ibf, hash_f, ibf_hash_count, ibf_bin_size):
    query_result = -1

    for i in range(ibf_hash_count):
        query_result &= ibf[seqan_ibf_hash(hash_f, SEQAN_HASH_SEEDS[i], ibf_bin_size)]
    
    return query_result

def bulk_count(ibf, hashes_f, bin_count, ibf_hash_count, ibf_bin_size):
    bin_queries = transpose([bit_decompose(query_ibf(ibf, hash_f, ibf_hash_count, ibf_bin_size), bin_count) for hash_f in hashes_f])
    return [sum(row) for row in bin_queries]

def classify(
        reads_generator,
        filters_configs,
        hierarchy_config):
    # get max from kmer/window size
    wk_size = hierarchy_config["window_size"] if hierarchy_config["window_size"] > 0 else hierarchy_config["kmer_size"]

    classified_reads = []
    unclassified_reads = []

    for read in reads_generator:
        # read lenghts
        read_len = len(read.read)
        matches: dict[int, int] = {}

        # hash count
        kmers = 0
        # Best scoring kmer count
        max_kmer_count_read = 0
        if read_len >= wk_size:
            # Count hashes from first pair
            hashes_f, hashes_r = get_hashes(
                read.read,
                hierarchy_config["window_size"],
                hierarchy_config["offset"],
                hierarchy_config["alphabet_size"])
            kmers = len(read.read) - hierarchy_config["kmer_size"] + 1

            # For each filter in the hierarchy
            for filter_config in filters_configs:
                # count matches
                counts_f = bulk_count(
                    filter_config.ibf, hashes_f,
                    filter_config.bin_count, filter_config.hash_count,
                    filter_config.bin_size)
                counts_r = bulk_count(
                    filter_config.ibf, hashes_r,
                    filter_config.bin_count, filter_config.hash_count,
                    filter_config.bin_size)

                # Calculate threshold for cutoff (keep matches above)
                threshold_cutoff = 0
                if filter_config.rel_cutoff >= 0:
                    threshold_cutoff = threshold_rel(kmers, filter_config.rel_cutoff)
                elif filter_config.abs_cutoff >= 0:
                    threshold_cutoff = threshold_abs(
                        kmers,
                        filter_config.abs_cutoff,
                        hierarchy_config["kmer_size"],
                        hierarchy_config["offset"],)
                            
                # select matches based on threshold cutoff
                max_kmer_count_read = select_matches(
                    matches, counts_f, counts_r, filter_config.map, threshold_cutoff, max_kmer_count_read)

        # if read got valid matches (above cutoff)
        if max_kmer_count_read > 0:
            # Calculate threshold for filtering (keep matches above)
            threshold_filter = get_threshold_filter(
                hierarchy_config,
                kmers,
                max_kmer_count_read,
                hierarchy_config["kmer_size"],
                hierarchy_config["offset"])

            # Filter matches
            read_out = filter_matches(
                read.name,
                matches,
                threshold_filter)

            if read_out.matches:
                classified_reads.append(read_out)

            # read classified, continue to the next
            continue;

        unclassified_reads.append(read)

    return classified_reads, unclassified_reads

def output_report(classified_reads, hierarchy_label):
    print(f'Level {hierarchy_label}:')
    for read_out in classified_reads:
        print(f'\tMatches for read {read_out.read_name}:')
        for match_tuple in read_out.matches:
            print(f'\t\t{match_tuple}')

def __fastq_record_generator(input_fasta):
    for record in FASTQ(input_fasta):
        yield record

def run(parsed_hierarchy, filters_configs, input_fasta):
    # Classify reads iteractively for each hierarchy level
    hierarchy_id = 0
    unclassified_reads = []

    for hierarchy_label, hierarchy_config in parsed_hierarchy.items():
        hierarchy_id += 1

        reads_generator = iter(unclassified_reads) if unclassified_reads else __fastq_record_generator(input_fasta)

        # Parallelize
        classified_reads, unclassified_reads = classify(
            reads_generator,
            filters_configs,
            hierarchy_config)
            
        # write reports
        output_report(classified_reads, hierarchy_label)


parsed_hierarchy = {
    "level_1": {
            "window_size": 0,
            "kmer_size": 19,
            "offset": 1,
            "rel_filter": 1,
            "abs_filter": 0,
            "alphabet_size": 5  # the size of the nucleotide alphabeth -- 5 in case of SeqAn's IBF (A,C,G,N,T)
        }
    }

filter_config = FilterConfig(
    ibf_path="data/ganon/ibf.bin",
    map_path="data/ganon/ibf_map.txt",
    rel_cutoff=0.25,
    abs_cutoff=0.0,
    bin_count=63,
    hash_count=3)

# run(parsed_hierarchy, [filter_config], 'data/ganon/bac.sim.1.fq')
