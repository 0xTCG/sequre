import math

from bio.fastq import FASTQ
from bio.fasta import FASTA

from utils.seqan import load_ibf, seqan_ungapped_kmer_encode, bulk_count


class FilterConfig:
    ibf: list[int]
    ibf_decomposed: list[list[int]]
    map: dict[int, int]
    rel_cutoff: float
    abs_cutoff: float
    bin_count: int
    technical_bin_count: int
    hash_count: int
    bin_size: u64

    def __init__(self, ibf_path, map_path, rel_cutoff, abs_cutoff, bin_count, hash_count):
        self.ibf, self.ibf_decomposed, self.technical_bin_count = load_ibf(ibf_path, bin_count)
        self.map = FilterConfig.load_map(map_path)
        self.rel_cutoff = rel_cutoff
        self.abs_cutoff = abs_cutoff
        self.bin_count = bin_count
        self.hash_count = hash_count
        self.bin_size = u64(len(self.ibf))

    def load_map(map_path):
        map_dict = {}
        
        with open(map_path) as f:
            count = 0
            for line in f:
                target = line.strip()
                map_dict[count] = int(target)
                count += 1
        
        return map_dict

class ReadOut:
    read_name: str
    matches: list[tuple[int, int]]

    def __init__(self, read_name):
        self.read_name = read_name
        self.matches = list[tuple[int, int]]()

def threshold_abs(kmers, e, k, o):
    return int(math.ceil(kmers - ( e * k ) / o)) if (kmers * o > e * k) else 0

def threshold_rel(kmers, p):
    return int(math.ceil(kmers * p))

def get_abs_error(kmers, k, o, count):
    return int(math.ceil(o * (-count + kmers) / k))

def select_matches(matches, counts_f, counts_r, filter_map, threshold_cutoff, max_kmer_count_read):
    # reset low threshold_cutoff to just one kmer (0 would match everywhere)
    if threshold_cutoff == 0:
        threshold_cutoff = 1

    # for each bin
    # for ( uint32_t bin_n = 0; bin_n < filter.ibf.noOfBins; ++bin_n )
    # loop in map structure to avoid extra validations when map.size() < filter.ibf.noOfBins when ibf is updated and
    # sequences removed also avoid the error of spurius results from empty bins (bug reported)
    for bin_n, target in filter_map.items():
        # if kmer count is higher than threshold_cutoff
        if counts_f[bin_n] >= threshold_cutoff or counts_r[bin_n] >= threshold_cutoff:
            # get best matching strand
            max_kmer_count_bin = max(counts_f[bin_n], counts_r[bin_n])
            # keep only the best match target/read when same targets are split in several bins
            if target not in matches or max_kmer_count_bin > matches[target]:
                # store match to target
                matches[target] = max_kmer_count_bin;
                if max_kmer_count_bin > max_kmer_count_read:
                    max_kmer_count_read = max_kmer_count_bin
    
    return max_kmer_count_read

def get_kmer_encodings(seq, window_size, offset, sigma):
    if window_size > 0:
        # minimizers
        # hashes_f = seqan_ungapped_minimizer_hash(seq, sigma)
        # return hashes_f, hashes_f
        raise NotImplementedError('Offset not implemented')

    if offset > 1:
        # offset
        raise NotImplementedError('Offset not implemented')
    
    # kmer
    return list(seqan_ungapped_kmer_encode(seq, sigma)), list(seqan_ungapped_kmer_encode(~seq, sigma))

def get_threshold_filter(hierarchy_config, kmers, max_kmer_count_read, kmer_size, offset):
    threshold_filter = 0
    if hierarchy_config["rel_filter"] >= 0:
        threshold_filter = max_kmer_count_read - threshold_rel(max_kmer_count_read, hierarchy_config["rel_filter"])
    elif hierarchy_config["abs_filter"] >= 0:
        # get maximum possible number of errors of best match + abs_filter
        max_error_threshold = get_abs_error( kmers, kmer_size, offset, max_kmer_count_read ) + hierarchy_config["abs_filter"]
        # get min kmer count necesary to achieve the calculated number of errors
        threshold_filter = threshold_abs( kmers, max_error_threshold, kmer_size, offset)

    return threshold_filter

def filter_matches(read_name, matches, threshold_filter):
    read_out = ReadOut(read_name)
    
    for target, kmer_count in matches.items():
        if kmer_count >= threshold_filter:
            read_out.matches.append((target, kmer_count))

    return read_out

def classify(
        reads_generator,
        filters_configs,
        hierarchy_config):
    # get max from kmer/window size
    wk_size = hierarchy_config["window_size"] if hierarchy_config["window_size"] > 0 else hierarchy_config["kmer_size"]

    classified_reads = []
    unclassified_reads = []

    for read in reads_generator:
        sequence = read.read  # Switch for FASTA/Q
        # read lenghts
        read_len = len(sequence)
        matches: dict[int, int] = {}

        # hash count
        kmers = 0
        # Best scoring kmer count
        max_kmer_count_read = 0
        if read_len >= wk_size:
            # Count hashes from first pair
            kmer_enc, kmer_enc_rev_comp = get_kmer_encodings(
                sequence,
                hierarchy_config["window_size"],
                hierarchy_config["offset"],
                hierarchy_config["alphabet_size"])
            kmers = len(sequence) - hierarchy_config["kmer_size"] + 1

            # For each filter in the hierarchy
            for filter_config in filters_configs:
                # count matches
                counts_f = bulk_count(
                    filter_config.ibf, kmer_enc,
                    filter_config.bin_count, filter_config.hash_count,
                    filter_config.bin_size)
                counts_r = bulk_count(
                    filter_config.ibf, kmer_enc_rev_comp,
                    filter_config.bin_count, filter_config.hash_count,
                    filter_config.bin_size)

                # Calculate threshold for cutoff (keep matches above)
                threshold_cutoff = 0
                if filter_config.rel_cutoff >= 0:
                    threshold_cutoff = threshold_rel(kmers, filter_config.rel_cutoff)
                elif filter_config.abs_cutoff >= 0:
                    threshold_cutoff = threshold_abs(
                        kmers,
                        filter_config.abs_cutoff,
                        hierarchy_config["kmer_size"],
                        hierarchy_config["offset"],)
                            
                # select matches based on threshold cutoff
                max_kmer_count_read = select_matches(
                    matches, counts_f, counts_r, filter_config.map, threshold_cutoff, max_kmer_count_read)

        # if read got valid matches (above cutoff)
        if max_kmer_count_read > 0:
            # Calculate threshold for filtering (keep matches above)
            threshold_filter = get_threshold_filter(
                hierarchy_config,
                kmers,
                max_kmer_count_read,
                hierarchy_config["kmer_size"],
                hierarchy_config["offset"])

            # Filter matches
            read_out = filter_matches(
                read.name,
                matches,
                threshold_filter)

            if read_out.matches:
                classified_reads.append(read_out)

            # read classified, continue to the next
            continue;

        unclassified_reads.append(read)

    return classified_reads, unclassified_reads

def output_report(classified_reads, hierarchy_label):
    print(f'Level {hierarchy_label}:')
    for read_out in classified_reads:
        print(f'\tMatches for read {read_out.read_name}:')
        for match_tuple in read_out.matches:
            print(f'\t\t{match_tuple}')

def __fastq_record_generator(input_fasta):
    for record in FASTQ(input_fasta):  # FASTA(input_fasta, fai=False):
        yield record

def run(parsed_hierarchy, filters_configs, input_fasta):
    # Classify reads iteractively for each hierarchy level
    hierarchy_id = 0
    unclassified_reads = []

    for hierarchy_label, hierarchy_config in parsed_hierarchy.items():
        hierarchy_id += 1

        reads_generator = iter(unclassified_reads) if unclassified_reads else __fastq_record_generator(input_fasta)

        # Parallelize
        classified_reads, unclassified_reads = classify(
            reads_generator,
            filters_configs,
            hierarchy_config)
            
        # write reports
        output_report(classified_reads, hierarchy_label)


parsed_hierarchy = {
    "level_1": {
            "window_size": 0,
            "kmer_size": 19,
            "offset": 1,
            "rel_filter": 1,
            "abs_filter": 0,
            "alphabet_size": 5  # the size of the nucleotide alphabeth -- 5 in case of SeqAn's IBF (A,C,G,N,T)
        }
    }

filter_config = FilterConfig(
    ibf_path="data/ganon/ibf.bin",
    map_path="data/ganon/ibf_map.txt",
    rel_cutoff=0.25,
    abs_cutoff=0.0,
    bin_count=63,
    hash_count=3)

# run(parsed_hierarchy, [filter_config], 'data/ganon/bac.sim.1.fq')
