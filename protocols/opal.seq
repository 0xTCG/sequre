from bio.fasta import FASTA
from bio.kmer import Kmer
from sequre.stdlib.logireg import logireg, predict
from sequre.types.shared_tensor import SharedTensor as Stensor
from sequre.stdlib.internal import Internal as sq
from sequre.stdlib.builtin import argmax
from sequre.utils.utils import zeros, zeros_vec
from sequre.types.utils import double_to_fp
from sequre.types.builtin import *
from sequre.attributes import *
import random
import time


ACGT = {c: i for i, c in enumerate('ACGT')}


def hmap(feature):
    value = 0
    for c in feature: value = value * 4 + ACGT[c]
    return value


def drawfrag(input_fasta, taxid_path, target_coverage, kmer_size=64):
    taxid_infile = open(taxid_path, 'r')

    for record in FASTA(input_fasta, fai=False):
        tlabel = int(next(taxid_infile).strip())
        coverage = 0
        desired_coverage = target_coverage * len(record.seq)
        if len(record.seq) >= kmer_size:
            try_num = 0
            while coverage < desired_coverage:
                try_num = try_num + 1
                pos = random.randint(0, len(record.seq) - kmer_size)
                sample = record.seq[pos:pos + kmer_size]
                if not sample.N() and len(sample) == kmer_size:
                    coverage = coverage + kmer_size
                    yield sample, tlabel
                if try_num > 10 * len(record.seq): break


def gen_features(pattern_getters, sequence):
    '''Generates features from a pattern list and a sequence'''
    features = []
    for kmer in sequence.kmers[Kmer[64]](1):
        for pat in pattern_getters:
            features.append(str(kmer)[pat])
    return features


def skms(frag_generator, patterns, dico):
    for sequence, label in frag_generator:
        feature_list = gen_features(patterns, sequence)
        feature_list.extend(gen_features(patterns, ~sequence))
        yield dico[label], feature_list


def get_patterns_and_dico(patterns_path, taxid_path):
    patterns = []
    with open(patterns_path, 'r') as pattern_file:
        for pattern in pattern_file:
            patterns.append([int(e) for e in pattern.split()])
    
    labels = list(int(label.strip()) for label in open(taxid_path, 'r'))
    labels_set = set(labels)
    dico = {v: i for i, v in enumerate(labels_set)}

    return patterns, dico, labels_set


def centroid_skms(frag_generator, taxid_path, patterns_path):
    # Read in the pattern file
    patterns, dico, labels_set = get_patterns_and_dico(patterns_path, taxid_path)

    all_features = zeros[int](len(labels_set), patterns.shape()[0] * 2)
    features_count = zeros_vec[int](len(labels_set))
    
    for label, feature_list in skms(frag_generator, patterns, dico):
        all_features[label] += [hmap(feature) for feature in feature_list]
        features_count[label] += 1
    
    return all_features // features_count, {v: k for k, v in dico.items()}


def share_centroids(mpc, frag_generator, taxid_path, patterns_path, modulus):
    centroid_skm, dico = centroid_skms(
        frag_generator=frag_generator,
        taxid_path=taxid_path,
        patterns_path=patterns_path)
    
    return Stensor(mpc.comms.share(centroid_skm.to_int_t(), modulus), modulus), dico


def share_data(mpc, input_path, taxid_path, patterns_path, target_coverage, modulus):
    '''
    Can be extended for multiple clients.
    New mean will be sum_i(M_i \cdot c_i) / sum_i(c_i)
    where for i-th client, M_i is the mean and c_i is the count of features per label.
    '''
    s = time.time()
    patterns, dico, _ = get_patterns_and_dico(patterns_path, taxid_path)
    frag_generator = drawfrag(input_path, taxid_path, target_coverage)

    X_positive = []
    y_positive = []
    X_negative = []
    y_negative = []
    count_positives = 0
    count_negatives = 0

    for label, features in skms(frag_generator, patterns, dico):
        if int(label) == 1:
            count_positives += 1
            X_positive.append(features)
            y_positive.append(1)
        else:
            count_negatives += 1
            X_negative.append(features)
            y_negative.append(-1)

    assert count_positives > 0
    assert count_negatives > 0
    assert count_negatives > count_positives
    
    neg_data = list(zip(X_negative, y_negative))
    test_element = neg_data[0]
    random.shuffle(neg_data)
    assert neg_data[0] != test_element
    neg_data = neg_data[:count_positives]

    data = list(zip(X_positive, y_positive))
    for e in neg_data: data.append(e)
    test_element = data[0]
    random.shuffle(data)
    assert data[0] != test_element

    X = []
    y = []
    for features, label in data:
        hashed_features = [hmap(feature) for feature in features]
        hashed_features.append(1)
        assert hashed_features[-1] == 1
        X.append(hashed_features)
        y.append(label)
    
    e = time.time()
    print(f'Preprocessing time: {e - s}')

    sv_x = Stensor(mpc.comms.share(X.to_int_t(), modulus), modulus)
    sv_x.fp = True
    sv_x.get_partitions(mpc)
    sv_y = Stensor(mpc.comms.share(y.to_int_t(), modulus), modulus)
    sv_y.fp = True
    sv_y.get_partitions(mpc)

    return sv_x, sv_y, dico


def load_test_features(mpc, input_path, patterns_path, modulus):
    # Read in the pattern file
    patterns = []
    with open(patterns_path, 'r') as pattern_file:
        for pattern in pattern_file:
            patterns.append([int(e) for e in pattern.split()])
    
    all_features = []

    for record in FASTA(input_path, fai=False):
        feature_list = gen_features(patterns, record.seq)
        feature_list.extend(gen_features(patterns, ~record.seq))
        all_features.append([hmap(feature) for feature in feature_list])
    
    sv = Stensor(mpc.comms.share(all_features.to_int_t(), modulus), modulus)
    sv.fp = True

    return sv


def opal_via_lsh(mpc, input_path, taxid_path, patterns_path, target_coverage, test_input_path, modulus):    
    # Data sharing should be done apriori in a real-world scenario
    if mpc.pid == 2: print('Sharing data ...')
    bins_centroids, dico = share_data(
        mpc=mpc,
        input_path=input_path,
        taxid_path=taxid_path,
        patterns_path=patterns_path,
        target_coverage=target_coverage,
        modulus=modulus)
    
    # Test features should be shared in apriori in a real-world scenario
    if mpc.pid == 2: print('Loading test features ...')
    test_features = load_test_features(
        mpc=mpc,
        input_path=test_input_path,
        patterns_path=patterns_path,
        modulus=modulus)

    if mpc.pid == 2: print('Calculating predictions ...')
    predictions = Stensor.zeros(len(test_features), modulus=modulus)
    
    for i, test_features_vec in enumerate(test_features):
        if i % 500 == 0: print(f'{i}/{len(test_features)}')
        target_bin_centroids = bins_centroids.zeros()
        for i in range(len(target_bin_centroids)):
            target_bin_centroids[i] = bins_centroids[i] - test_features_vec

        predictions[i] = argmax(mpc, sq.dot(mpc, bins_centroids))
    
    return predictions, dico

def opal_via_logireg(mpc, input_path, taxid_path, patterns_path, target_coverage, test_input_path, modulus):
    # Data sharing should be done apriori in a real-world scenario
    if mpc.pid == 2: print('Sharing data ...')
    X, y, dico = share_data(
        mpc=mpc,
        input_path=input_path,
        taxid_path=taxid_path,
        patterns_path=patterns_path,
        target_coverage=target_coverage,
        modulus=modulus)
    relu_alpha = 0.1  # Stensor(mpc.comms.share(double_to_fp(0.1, modulus), modulus), modulus)
    relu_beta = 0.1  # Stensor(mpc.comms.share(double_to_fp(0.1, modulus), modulus), modulus)
    step_size = 0.01  # Stensor(mpc.comms.share(double_to_fp(0.01, modulus), modulus), modulus)
    # relu_alpha.fp = True
    # relu_beta.fp = True
    # step_size.fp = True
    
    if mpc.pid == 2: print(f'Training model ... Input size: {X.shape()}')
    
    s = time.time()
    logireg_weights = logireg(
        mpc=mpc,
        X=X,
        y=y,
        eta=step_size,
        epochs=10,
        relu_alpha=relu_alpha,
        relu_beta=relu_beta)
    e = time.time()
    if mpc.pid == 2: print(f'Training done in {e - s}s')

    # Test features should be shared in apriori in a real-world scenario
    if mpc.pid == 2: print('Loading test features ...')
    test_features = load_test_features(
        mpc=mpc,
        input_path=test_input_path,
        patterns_path=patterns_path,
        modulus=modulus)

    if mpc.pid == 2: print(f'Calculating predictions ... Input size: {test_features.shape()}. Weights size: {len(logireg_weights)}')
    return predict(mpc, test_features, logireg_weights, relu_alpha, relu_beta), dico


def opal_protocol(mpc, test_run, modulus):
    # TODO: Make the params configurable
    input_path = 'data/opal/A1.train.fasta'
    taxid_path = 'data/opal/A1.train.taxid'
    patterns_path = 'data/opal/patterns.txt'
    test_input_path = 'data/opal/test.fragments.fasta'
    target_coverage = 0.1

    return opal_via_logireg(mpc, input_path, taxid_path, patterns_path, target_coverage, test_input_path, modulus)
