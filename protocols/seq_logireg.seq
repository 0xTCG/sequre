from sequre.utils.param import *
from sequre.utils.io import read_vector, read_matrix
from sequre.utils.utils import switch_pair
from sequre.types.utils import double_to_fp

from sequre.mpc.env import MPCEnv


def neg_log_sigmoid[TP](mpc: MPCEnv[TP], a: list[TP], modulus: TP) -> list[TP]:
    n: int = len(a)
    depth: int = 6
    step: float = 4.0
    a_ind = [TP(0) for _ in range(n)]

    for i in range(depth):
        a_sign = mpc.boolean.is_positive(a)
        index_step = TP(1 << (depth - 1 - i)) % modulus

        a_ind += (a_sign * index_step)
        a_ind %= modulus

        a_sign *= 2
        if mpc.pid == 1:
            a_sign -= 1
        a_sign %= modulus

        step_fp = double_to_fp(
            step, NBIT_K, NBIT_F, modulus)

        a -= a_sign * step_fp
        a %= modulus

        step /= 2

    if mpc.pid == 1:
        a_ind += 1
    a_ind %= modulus

    params = mpc.polynomial.table_lookup(a_ind, 2)

    # b = mpc.arithmetic.multiply(params[1], a, modulus)
    # b = mpc.fp.trunc(b)

    # if mpc.pid > 0:
    #     b += params[0]
    #     b %= modulus

    return params[1]


def parallel_logistic_regression[TP](
    mpc: MPCEnv[TP], xr: list[list[TP]], xm: list[list[TP]], vr: list[list[TP]],
    vm: list[list[TP]], yr: list[TP], ym: list[TP], max_iter: int, modulus: TP) -> tuple[list[TP], list[list[TP]], list[TP]]:
    n: int = vr.shape()[1]
    p: int = vr.shape()[0]
    c: int = xr.shape()[0]
    
    assert vm.shape()[0] == p
    assert vm.shape()[1] == n
    assert xm.shape()[0] == c
    assert xm.shape()[1] == n
    assert xr.shape()[1] == n
    assert len(yr) == n
    assert len(ym) == n

    b0 = [TP(0) for _ in range(c)]
    bv = [[TP(0) for _ in range(p)] for _ in range(c)]  # Matrix(c, p)
    bx = [TP(0) for _ in range(c)]

    yneg_r = -yr
    yneg_m = -ym
    if mpc.pid > 0:
        yneg_r += 1
        yneg_r %= modulus

    yneg = yneg_m + (yneg_r if mpc.pid == 1 else yneg_r.zeros())

    fp_memory = double_to_fp(0.5, NBIT_K, NBIT_F)
    fp_one = double_to_fp(1.0, NBIT_K, NBIT_F)
    eta: float = 0.3

    step0 = [TP(0) for _ in range(c)]
    stepv = [[TP(0) for _ in range(p)] for _ in range(c)]  # Matrix(c, p)
    stepx = [TP(0) for _ in range(c)]

    nbatch: int = 10
    batch_size: int = (n + nbatch - 1) // nbatch

    for it in range(max_iter):
        print(f'Logistic regression iteration {it} initialized at CP{mpc.pid}')
        batch_index: int = it % nbatch
        start_ind: int = batch_size * batch_index
        end_ind: int = start_ind + batch_size
        if end_ind > n:
            end_ind = n
        cur_bsize: int = end_ind - start_ind

        xr_batch = [[TP(0) for _ in range(cur_bsize)] for _ in range(c)]  # Matrix(c, cur_bsize)
        xm_batch = [[TP(0) for _ in range(cur_bsize)] for _ in range(c)]  # Matrix(c, cur_bsize)
        vr_batch = [[TP(0) for _ in range(cur_bsize)] for _ in range(p)]  # Matrix(p, cur_bsize)
        vm_batch = [[TP(0) for _ in range(cur_bsize)] for _ in range(p)]  # Matrix(p, cur_bsize)
        yn_batch = [TP(0) for _ in range(cur_bsize)]
        ynr_batch = [TP(0) for _ in range(cur_bsize)]
        ynm_batch = [TP(0) for _ in range(cur_bsize)]

        for j in range(c):
            for i in range(cur_bsize):
                xr_batch[j][i] = xr[j][start_ind + i]
                xm_batch[j][i] = xm[j][start_ind + i]

        for j in range(p):
            for i in range(cur_bsize):
                vr_batch[j][i] = vr[j][start_ind + i]
                vm_batch[j][i] = vm[j][start_ind + i]

        for i in range(cur_bsize):
            yn_batch[i] = yneg[start_ind + i]
            ynr_batch[i] = yneg_r[start_ind + i]
            ynm_batch[i] = yneg_m[start_ind + i]

        fp_bsize_inv = double_to_fp(eta * (1.0 / cur_bsize), NBIT_K, NBIT_F)

        bvr, bvm = mpc.arithmetic.__beaver_partition(bv)
        bxr, bxm = mpc.arithmetic.__beaver_partition(bx)

        h = mpc.arithmetic.__beaver_matmul(bvr, bvm, vr_batch, vm_batch)
        for j in range(c):
            xrvec = (xr_batch[j] * fp_one) % modulus
            xmvec = (xm_batch[j] * fp_one) % modulus
            h[j] += mpc.arithmetic.__beaver_mult(xrvec, xmvec, bxr[j], bxm[j])
        h = mpc.arithmetic.__beaver_reconstruct(h)
        h = mpc.fp.trunc(h, NBIT_K + NBIT_F, NBIT_F)

        for j in range(c):
            h[j] += b0[j]
        h %= modulus

        s_grad_vec = neg_log_sigmoid[TP](mpc, h.flatten(), modulus)
        s_grad = [s_grad_vec].reshape([c, cur_bsize])

        d0 = [TP(0) for _ in range(c)]
        dv = [[TP(0) for _ in range(p)] for _ in range(c)]  # Matrix(c, p)
        dx = [TP(0) for _ in range(c)]

        for j in range(c):
            s_grad[j] += (yn_batch * fp_one) % modulus
            d0[j] = sum(s_grad[j]) % modulus
        s_grad %= modulus
        d0 %= modulus

        s_grad_r, s_grad_m = mpc.arithmetic.__beaver_partition(s_grad)

        for j in range(c):
            dx[j] = mpc.arithmetic.beaver_inner_prod_pair(
                xr_batch[j], xm_batch[j], s_grad_r[j], s_grad_m[j])
        dx = mpc.arithmetic.__beaver_reconstruct(dx)

        vr_batch = vr_batch.transpose()
        vm_batch = vm_batch.transpose()
        dv = mpc.arithmetic.__beaver_matmul(s_grad_r, s_grad_m, vr_batch, vm_batch)
        dv = mpc.arithmetic.__beaver_reconstruct(dv)
        dv = mpc.fp.trunc(dv, NBIT_K + NBIT_F, NBIT_F)

        step0 = ((step0 * fp_memory) % modulus - (d0 * fp_bsize_inv) % modulus) % modulus
        stepv = ((stepv * fp_memory) % modulus - (dv * fp_bsize_inv) % modulus) % modulus
        stepx = ((stepx * fp_memory) % modulus - (dx * fp_bsize_inv) % modulus) % modulus
        step0 = mpc.fp.trunc(step0, NBIT_K + NBIT_F, NBIT_F)
        stepv = mpc.fp.trunc(stepv, NBIT_K + NBIT_F, NBIT_F)
        stepx = mpc.fp.trunc(stepx, NBIT_K + NBIT_F, NBIT_F)

        b0 += step0
        bv += stepv
        bx += stepx

        bv %= modulus
        bv %= modulus
        bx %= modulus

    return b0, bv, bx


def logireg_protocol[TP](mpc: MPCEnv[TP], test_run: bool = True):
    pheno_path = (f"tests/data/input/gwas_pheno_shares_{mpc.pid}.txt" if test_run else
                  f"{GWAS_DATA_PATH}/__pheno_shares_{mpc.pid}.txt")
    cov_path = (f"tests/data/input/gwas_cov_shares_{mpc.pid}.txt" if test_run else
                f"{GWAS_DATA_PATH}/__cov_shares_{mpc.pid}.txt")
    geno_path = (f"tests/data/input/gwas_geno_x_r.txt" if test_run else
                 f"{GWAS_DATA_PATH}/__geno_x_r.txt")
    geno_mask_path = (f"tests/data/input/gwas_geno_r_{mpc.pid}.txt" if test_run else
                      f"{GWAS_DATA_PATH}/__geno_r_{mpc.pid}.txt")
    eigen_path = (f"tests/data/input/logireg_eigen_shares_{mpc.pid}.txt" if test_run else
                  f"{LOGIREG_DATA_PATH}/__eigen_shares_{mpc.pid}.txt")
    gkeep1_path = (f"tests/data/input/logireg_gkeep1.txt" if test_run else
                   f"{LOGIREG_DATA_PATH}/__gkeep1.txt")
    gkeep2_path = (f"tests/data/input/logireg_gkeep2.txt" if test_run else
                   f"{LOGIREG_DATA_PATH}/__gkeep2.txt")
    ikeep_path = (f"tests/data/input/logireg_ikeep.txt" if test_run else
                  f"{LOGIREG_DATA_PATH}/__ikeep.txt")
    assoc_path = (f"tests/data/input/logireg_assoc.txt" if test_run else
                  f"{LOGIREG_DATA_PATH}/__assoc.txt")
    
    # TODO: #16 Implemend threading. (See original implementation of this protocol)
    ntop: int = 100

    n0 = NUM_INDS
    m0 = NUM_SNPS
    k = NUM_DIM_TO_REDUCE_TO
    modulus: TP = FIELD_PRIME

    # Shared variables
    ind = 0
    fp_one = double_to_fp(1.0, NBIT_K, NBIT_F)

    print('Initial data sharing results found')

    f_pheno = open(pheno_path)
    f_cov = open(cov_path)
    f_geno = open(geno_path)
    f_geno_mask = open(geno_mask_path)
    f_eigen = open(eigen_path)
    f_gkeep1 = open(gkeep1_path)
    f_gkeep2 = open(gkeep2_path)
    f_ikeep = open(ikeep_path)
    
    pheno = read_vector[TP](f_pheno, n0)
    cov = read_matrix[TP](f_cov, n0, NUM_COVS)

    print('Phenotypes and covariates loaded')
    print('Using locus missing rate filter from a previous run')

    gkeep1 = read_vector[int](f_gkeep1, m0)
    m1 = sum(gkeep1)

    print('Using individual missing rate/het rate filters from a previous run')

    ikeep = read_vector[int](f_ikeep, n0)
    n1 = sum(ikeep)

    print(f'n1: {n1}, m1: {m1}')
    print('Filtering phenotypes and covariates')
    
    pheno = [e for i, e in enumerate(pheno) if ikeep[i]]
    cov = [row for i, row in enumerate(cov) if ikeep[i]]

    print('Using MAF/HWE filters from a previous run')

    gkeep2 = read_vector[int](f_gkeep2, m0)
    m2 = sum(gkeep2)
    print(f'n1: {n1}, m2: {m2}')

    print('Using CA statistics from a previous run')

    cavec = list[tuple[int, float]](m2)
    with open(assoc_path) as f:
        for i, line in zip(range(m2), f):
            cavec.append((i, float(line) ** 2))

    cavec.sort(key=switch_pair[int, float], reverse=True)

    print(f'Selected top {ntop} candidates')
    print(f'Top 5 CA stats: {cavec[:5]}')

    gkeep3 = [0 for _ in range(m2)]
    for i in range(ntop):
        gkeep3[cavec[i][0]] = 1

    V = read_matrix[TP](f_eigen, k, n1)

    # Concatenate covariate matrix and jointly orthogonalize
    cov = cov.transpose()
    V = V.pad(k + NUM_COVS, n1)
    if mpc.pid > 0:
        for i in range(NUM_COVS):
            V[k + i] = cov[i] * fp_one
    V %= modulus

    print('Finding orthonormal basis for ', NUM_COVS, n1)
    V = mpc.lin_alg.orthonormal_basis(V)

    V_mean = [TP(0) for _ in range(len(V))]
    fp_denom = double_to_fp(1.0 / len(V[0]), NBIT_K, NBIT_F)

    for i in range(len(V_mean)):
        V_mean[i] = (sum(V[i]) % modulus) * fp_denom
    V_mean %= modulus
    V_mean = mpc.fp.trunc(V_mean)
    for i in range(len(V)):
        V[i] -= V_mean[i]
    V %= modulus

    V_var = mpc.arithmetic.inner_prod(V)
    V_var = mpc.fp.trunc(V_var)
    V_var *= fp_denom
    V_var %= modulus
    V_var = mpc.fp.trunc(V_var)

    print('Calculating fp_sqrt')
    _, V_stdinv = mpc.fp.fp_sqrt(V_var)

    for i in range(len(V_mean)):
        V[i] = mpc.arithmetic.multiply(V[i], V_stdinv[i])
    V = mpc.fp.trunc(V, k=NBIT_K + NBIT_F, m=NBIT_F)

    gkeep = [(gkeep1[j] == 1) for j in range(m0)]

    X = []
    X_mask = []
    ind: int = 0
    for j in range(m0):
        if gkeep[j]:
            gkeep[j] = (gkeep2[ind] == 1)
            ind += 1

    ind: int = 0
    for j in range(m0):
        if gkeep[j]:
            gkeep[j] = (gkeep3[ind] == 1)
            ind += 1
    
    ind = -1
    for i in range(n1):
        ind += 1

        g0 = [[TP(0) for _ in range(m0)] for _ in range(3)]  # Matrix()
        g0_mask = [[TP(0) for _ in range(m0)] for _ in range(3)]  # Matrix()
        miss0 = [TP(0) for _ in range(m0)]  # Vector()
        miss0_mask = [TP(0) for _ in range(m0)]  # Vector()

        while ikeep[ind] != 1:
            raise NotImplementedError()

        if mpc.pid > 0:
            g0 = read_matrix[TP](f_geno, 3, m0)
        g0_mask = read_matrix[TP](f_geno_mask, 3, m0)

        g = [[TP(0) for _ in range(ntop)] for _ in range(3)]
        g_mask = [[TP(0) for _ in range(ntop)] for _ in range(3)]
        ind2: int = 0
        for j in range(m0):
            if gkeep[j]:
                for k in range(3):
                    g[k][ind2] = g0[k][j]
                    g_mask[k][ind2] = g0_mask[k][j]
                ind2 += 1

        dosage = (g[1] + (g[2] * 2) % modulus) % modulus
        dosage_mask = (g_mask[1] + (g_mask[2] * 2) % modulus) % modulus
        X.append(dosage)
        X_mask.append(dosage_mask)

    X = X.transpose()
    X_mask = X_mask.transpose()
    V, V_mask = mpc.arithmetic.__beaver_partition(V)
    pheno, pheno_mask = mpc.arithmetic.__beaver_partition(pheno)

    b = parallel_logistic_regression(mpc, X, X_mask, V, V_mask, pheno, pheno_mask, LOGIREG_ITER, modulus)

    f_pheno.close()
    f_cov.close()
    f_geno.close()
    f_geno_mask.close()
    f_eigen.close()
    f_gkeep1.close()
    f_gkeep2.close()
    f_ikeep.close()

    return b[2]
