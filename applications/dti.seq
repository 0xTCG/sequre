import random

from sequre.utils.param import int_t, DEBUG
from sequre.utils.io import read_matrix
from sequre.utils.utils import zeros
from sequre.types.utils import fp_to_double
from sequre.types.shared_tensor import SharedTensor as Stensor
from sequre.attributes import *
from sequre.stdlib.internal import Internal as sq

from utils.param import *
from utils.data_sharing import shares_paths


def get_random_idx_generator(mpc, size):
    l = list(range(size))
    mpc.prg.switch_seed(-1)
    random.shuffle(l)
    mpc.prg.restore_seed(-1)
    for e in l: yield e


def open_input_files(mpc, test_run):
    _, features_path, _ = shares_paths(mpc, 'neural_net', 'features', test_run)
    _, labels_path, _ = shares_paths(mpc, 'neural_net', 'labels', test_run)
    
    return open(features_path), open(labels_path)


def load_X_y(mpc, X_shape, y_shape, test_run, modulus):
    X = None
    y = None

    if mpc.pid == 0:
        X = Stensor(zeros(X_shape[0], X_shape[1]), modulus)
        y = Stensor(zeros(y_shape[0], y_shape[1]), modulus)
    else:
        fx, fy = open_input_files(mpc, test_run)
        X = Stensor(read_matrix[int_t](fx, X_shape[0], X_shape[1]), modulus)
        y = Stensor(read_matrix[int_t](fy, y_shape[0], y_shape[1]), modulus)
        fx.close()
        fy.close()

    X.fp = True
    y.fp = True
      
    return X, y


def initialize_model(mpc, modulus):
    W = []
    dW = []
    vW = []
    b = []
    db = []
    vb = []

    for l in range(N_HIDDEN + 1):
        if (N_HIDDEN == 0 and l >= 1):
            break
        
        W_layer_shape = (N_NEURONS, N_NEURONS)
        b_layer_len = N_NEURONS

        if (N_HIDDEN == 0 and l == 0):
            W_layer_shape = (FEATURE_RANK, N_CLASSES - 1)
            b_layer_len = N_CLASSES - 1
        elif (l == 0):
            W_layer_shape = (FEATURE_RANK, N_NEURONS)
        elif (l == N_HIDDEN):
            W_layer_shape = (N_NEURONS, N_CLASSES - 1)
            b_layer_len = N_CLASSES - 1
            
        W_layer = sq.dist(mpc, W_layer_shape, 'normal', modulus, (0.0, 0.01))
        b_layer = Stensor.zeros(b_layer_len, modulus)
        b_layer.fp = True
        
        dW_layer = W_layer.zeros()
        vW_layer = W_layer.zeros()
    
        db_layer = b_layer.zeros()
        vb_layer = b_layer.zeros()

        W.append(W_layer)
        dW.append(dW_layer)
        vW.append(vW_layer)
        b.append(b_layer)
        db.append(db_layer)
        vb.append(vb_layer)
     
    return W, dW, vW, b, db, vb


@sequre_beaver
def gradient_descent(mpc, X, y, W, b, dW, db, vW, vb):
    act = []
    relus = []
    
    # Forward pass
    for l in range(N_HIDDEN):
        activation = sq.matmul(mpc, (X if l == 0 else act[l - 1]), W[l]) + b[l]
        # Apply ReLU non-linearity
        relu = activation > 0
        after_relu = activation * relu

        # TODO: #110 Implement dropout.

        act.append(after_relu)
        relus.append(relu)
    
    # Calculate scores and add bias term
    scores = sq.matmul(mpc, (X if N_HIDDEN == 0 else act[-1]), W[-1]) + b[-1]

    dscores = y.zeros()
    if (LOSS == "hinge"):
        y = y * 2 - 1
        dscores = y * ((1 - y * scores) > 0)
    else:
        dscores = scores - y
    dscores = dscores / len(X)

    # Back propagation
    dhidden = dscores
    for l in range(N_HIDDEN, -1, -1):
        # Compute derivative of weights
        dW[l] = sq.matmul(mpc, X.T if l == 0 else act.pop().T, dhidden)
        # Add regularization term to weights
        if REG != 0.0: dW[l] = dW[l] + W[l] * REG
        # Compute derivative of biases
        db[l] = dhidden.sum()
        # Compute backpropagated activations and apply derivative of ReLU
        if l > 0: dhidden = sq.matmul(mpc, dhidden, W[l].T) * relus.pop()

    if DEBUG:
        assert len(act) == 0
        assert len(relus) == 0

    # Update the model using Nesterov momentum
    # Compute constants that update various parameters
    for l in range(N_HIDDEN + 1):
        # Update the weights
        vW_prev = vW[l].copy()
        vW[l] = vW[l] * MOMENTUM - dW[l] * LEARN_RATE
        W[l] = W[l] + vW[l] * (MOMENTUM + 1) - vW_prev * MOMENTUM
        # TODO: #117 Implement clever joint truncations pattern matcher
        # vW[l] = vW[l] * MOMENTUM - dW[l] * LEARN_RATE
        # vW[l] = vW[l].trunc(mpc.fp)
        # temp = vW[l] * (MOMENTUM + 1) - vW_prev * MOMENTUM
        # temp = temp.trunc(mpc.fp)
        # W[l] = W[l] + temp
        
        # Update the biases
        vb_prev = vb[l].copy()
        vb[l] = vb[l] * MOMENTUM - db[l] * LEARN_RATE
        b[l] = b[l] + vb[l] * (MOMENTUM + 1) - vb_prev * MOMENTUM
        # TODO: #117 Implement clever joint truncations pattern matcher
        # vb[l] = vb[l] * MOMENTUM - db[l] * LEARN_RATE
        # vb[l] = vb[l].trunc(mpc.fp)
        # temp_v = vb[l] * (MOMENTUM + 1) - vb_prev * MOMENTUM
        # temp_v = temp_v.trunc(mpc.fp)
        # b[l] = b[l] + temp_v
    
    return W, b, vW, vb
    

def train_model(mpc, X, y, W, b, dW, db, vW, vb):    
    # Round down number of batches in file
    batches_in_file = len(X) // NN_BATCH_SIZE
    epoch = 0

    while True:
        random_idx = get_random_idx_generator(mpc, len(X))

        for i in range(batches_in_file):
            X_batch = Stensor[typeof(X.share)]([], X.modulus)
            y_batch = Stensor[typeof(y.share)]([], y.modulus)
            X_batch.fp = True
            y_batch.fp = True
            for _ in range(NN_BATCH_SIZE):
                next_idx = next(random_idx)
                X_batch.share.append(X.share[next_idx])
                y_batch.share.append(y.share[next_idx])
            
            # Do one round of mini-batch gradient descent
            if mpc.pid == 2: print(f"Epoch: {epoch}/{MAX_EPOCHS}")
            W, b, vW, vb = gradient_descent(
                mpc, X_batch, y_batch,
                W, b, dW, db, vW, vb)

            # Update reference to training epoch
            epoch += 1
            if epoch >= MAX_EPOCHS:
                return W, b
    

def neural_net_protocol(mpc, test_run, modulus):
    if test_run: random.seed(0)
    # Initialize model and data structures
    print(f"Initializing model at CP{mpc.pid}...")
    W, dW, vW, b, db, vb = initialize_model(mpc, modulus)

    # Initialize data matrices
    X, y = load_X_y(mpc, [N_FILE_BATCH, FEATURE_RANK], [N_FILE_BATCH, N_CLASSES - 1], test_run, modulus)
    # Do gradient descent over multiple training epochs
    W, b = train_model(mpc, X, y, W, b, dW, db, vW, vb)

    if mpc.pid > 0:
        for l in range(N_HIDDEN + 1):
            W_out_revealed = mpc.comms.reveal(W[l].share, modulus)
            W_out = fp_to_double(W_out_revealed, modulus)
            b_out_revealed = mpc.comms.reveal(b[l].share, modulus)
            b_out = fp_to_double(b_out_revealed, modulus)
            
            if mpc.pid == 2:
                with open(f'results/drug_target_interaction_inference_results_weights_layer_{l}.txt', 'w') as fw, \
                        open(f'results/drug_target_interaction_inference_results_bias_layer_{l}.txt', 'w') as fb:
                    for row in W_out: fw.write(f'{" ".join([str(e) for e in row])}\n')
                    fb.write(f'{" ".join([str(e) for e in b_out])}\n')
    
    return W[-1]
