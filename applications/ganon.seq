from sequre.types.builtin import *
from sequre.attributes import sequre_beaver
from sequre.utils.param import *
from sequre.types.shared_tensor import SharedTensor as Stensor
from sequre.stdlib.protocols import bit_decomposition

from offline.ganon import FilterConfig, threshold_abs, \
    threshold_rel, get_kmer_encodings, get_threshold_filter, \
        filter_matches, output_report, __fastq_record_generator, select_matches
from offline.seqan import SEQAN_HASH_SEEDS, seqan_ibf_hash, bulk_count



DELIMITER_PRIME = 127  # MUST BE A MERSENNE PRIME. Hardcoded for now. The smallest prime that is larger than the max kmer count in all reads.


def one_hash_ibf_query(ibf, shared_hashes, modulus):
    query_results = []
    
    for shared_one_hot_hash in shared_hashes:
        dot_sum = typeof(modulus)(0)
        for sh, ibf_val in zip(shared_one_hot_hash, ibf):
            dot_sum = dot_sum.add_mod(sh.mul_mod(ibf_val, modulus), modulus)
        query_results.append(dot_sum)
    
    return query_results


@sequre_beaver
def bulk_count_one_hot(mpc, ibf, kmer_encodings, technical_bin_count, bin_count, ibf_hash_count, hash_shift, modulus):
    queries_per_hash = []
    small_modulus = DELIMITER_PRIME + int(modulus.popcnt() == 1)

    for i in range(ibf_hash_count):
        ibf_hashes = []

        for kmer_int in kmer_encodings:
            ibf_hashes_row = [int_t(0) for _ in range(len(ibf))]
            ibf_hashes_row[seqan_ibf_hash(kmer_int, SEQAN_HASH_SEEDS[i], u64(len(ibf)), hash_shift)] = int_t(1)
            ibf_hashes.append(ibf_hashes_row)
        
        shared_ibf_hashes = mpc.comms.share(ibf_hashes, modulus)
        queries_per_hash.append(Stensor[list[int_t]](one_hash_ibf_query(ibf, shared_ibf_hashes, modulus), modulus))

    decomposed_bits = bit_decomposition(mpc, queries_per_hash[0], technical_bin_count, small_modulus, modulus)

    for query in queries_per_hash[1:]:
        decomposed_bits = decomposed_bits * bit_decomposition(mpc, query, technical_bin_count, small_modulus, modulus)
    
    return decomposed_bits.sum()[technical_bin_count - bin_count:].reverse().print(mpc).to_int()


def classify(
        mpc,
        reads_generator,
        filters_configs,
        hierarchy_config,
        modulus,
        offline):

    # get max from kmer/window size
    wk_size = hierarchy_config["window_size"] if hierarchy_config["window_size"] > 0 else hierarchy_config["kmer_size"]

    classified_reads = []
    unclassified_reads = []
    reads_count = 0

    for read in reads_generator:
        sequence = read.seq  # FASTA/Q toggle
        if not offline:
            assert len(sequence) < DELIMITER_PRIME + int(modulus.popcnt() == 1) + wk_size, f"Sequre Ganon cannot operate on reads longer than {DELIMITER_PRIME} + k-mer size."  # TODO: Enable Sequre Ganon to work on longer reads
        reads_count += 1
        if mpc.pid == 2: print(f'Processing read {reads_count} ...')

        # read lenghts
        read_len = len(sequence)
        matches: dict[int, int] = {}

        # hash count
        kmers = 0
        # Best scoring kmer count
        max_kmer_count_read = 0
        if read_len >= wk_size:
            # Count hashes from first pair
            kmer_enc, kmer_enc_rev_comp = get_kmer_encodings(
                sequence,
                hierarchy_config["window_size"],
                hierarchy_config["offset"],
                hierarchy_config["alphabet_size"])
            kmers = len(sequence) - hierarchy_config["kmer_size"] + 1
            
            # For each filter in the hierarchy
            for filter_config in filters_configs:
                # count matches
                counts_f = bulk_count(
                    filter_config.ibf, kmer_enc,
                    filter_config.bin_count, filter_config.hash_count) if offline else bulk_count_one_hot(
                        mpc, filter_config.ibf, kmer_enc,
                        filter_config.technical_bin_count,
                        filter_config.bin_count, filter_config.hash_count,
                        filter_config.hash_shift, modulus)
                counts_r = bulk_count(
                    filter_config.ibf, kmer_enc_rev_comp,
                    filter_config.bin_count, filter_config.hash_count) if offline else bulk_count_one_hot(
                        mpc, filter_config.ibf, kmer_enc_rev_comp,
                        filter_config.technical_bin_count,
                        filter_config.bin_count, filter_config.hash_count,
                        filter_config.hash_shift, modulus)

                # Calculate threshold for cutoff (keep matches above)
                threshold_cutoff = 0
                if filter_config.rel_cutoff >= 0:
                    threshold_cutoff = threshold_rel(kmers, filter_config.rel_cutoff)
                elif filter_config.abs_cutoff >= 0:
                    threshold_cutoff = threshold_abs(
                        kmers,
                        filter_config.abs_cutoff,
                        hierarchy_config["kmer_size"],
                        hierarchy_config["offset"],)
                if threshold_cutoff == 0:
                    threshold_cutoff = 1
                
                # select matches based on threshold cutoff
                max_kmer_count_read = select_matches(
                    matches, counts_f, counts_r, filter_config.map, threshold_cutoff, max_kmer_count_read)
                
        # if read got valid matches (above cutoff)
        if max_kmer_count_read > 0:
            # Calculate threshold for filtering (keep matches above)
            threshold_filter = get_threshold_filter(
                hierarchy_config,
                kmers,
                max_kmer_count_read,
                hierarchy_config["kmer_size"],
                hierarchy_config["offset"])

            # Filter matches
            read_out = filter_matches(
                read.name,
                matches,
                threshold_filter)

            if read_out.matches:
                classified_reads.append(read_out)

            # read classified, continue to the next
            continue;

        unclassified_reads.append(read)

    return classified_reads, unclassified_reads


def ganon(mpc, parsed_hierarchy, filters_configs, input_fasta, modulus, offline):
    # Classify reads iteractively for each hierarchy level
    hierarchy_id = 0
    unclassified_reads = []
    filtered_matches = []

    for hierarchy_label, hierarchy_config in parsed_hierarchy.items():
        hierarchy_id += 1

        reads_generator = iter(unclassified_reads) if unclassified_reads else __fastq_record_generator(input_fasta)

        # Parallelize
        classified_reads, unclassified_reads = classify(
            mpc,
            reads_generator,
            filters_configs,
            hierarchy_config,
            modulus,
            offline)
            
        # Output reports
        output_report(classified_reads, hierarchy_label)
        for readout in classified_reads:
            target, count = max_by_key(readout.matches, key=lambda x: x[1])
            filtered_matches.append([readout.read_name, str(target), str(count)])
    
    return filtered_matches


def offline_ganon(mpc, parsed_hierarchy, filters_configs, input_fasta):
    return ganon(mpc, parsed_hierarchy, filters_configs, input_fasta, int_t(0), True)


def sequre_ganon(mpc, parsed_hierarchy, filters_configs, input_fasta, modulus):
    return ganon(mpc, parsed_hierarchy, filters_configs, input_fasta, modulus, False)
